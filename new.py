import os, json, pandas as pd, asyncio
from pathlib import Path
from typing import List, Optional
from pyalex import Sources, config
import re
import csv
from pandas.errors import ParserError

# ê° ë‹¨ê³„ë³„ ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ import (í•¨ìˆ˜í™”)
from libs import real1_test_openalex_sequential_optimized_0722 as real1
from libs import real2_test_openalex_to_csv as real2
from libs import real3_ror_mapping_processor_copy as real3
from libs import real4_ror_extract as real4
from libs import real5_name_mapping as real5
from libs import real6_visualize_fast as real6


# Google Drive adapter
import json as _json
import ast as _ast
from collections.abc import Mapping

try:
    import streamlit as st
except Exception:
    st = None # ë¡œì»¬ í™˜ê²½ì—ì„œë„ ë™ì‘í•˜ë„ë¡ ì„ íƒì  import
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

FORCE_JOURNAL_KEY: str | None = None  # âœ… ì‹¤í–‰ ë™ì•ˆ ëŒ€í‘œí‚¤ë¥¼ ê°•ì œë¡œ ê³ ì •


def _parse_keys_years_from_final(final_csv_path: str):
    """
    ìµœì¢… CSV íŒŒì¼ëª…ì—ì„œ keys(ì €ë„ ìŠ¬ëŸ¬ê·¸ë“¤)ì™€ ì—°ë„ ë²”ìœ„ë¥¼ íŒŒì‹±.
    ì˜ˆ) slugA-slugB_2005_2006_ror_extract_name.csv
       -> keys=['slugA','slugB'], (2005, 2006)
    """
    name = Path(final_csv_path).name
    m = re.match(r"^(?P<keys>.+)_(?P<y1>\d{4})_(?P<y2>\d{4})_ror_extract_name\.csv$", name)
    if not m:
        raise ValueError(f"Unexpected final csv filename: {name}")
    keys = m.group("keys").split("-")
    return keys, int(m.group("y1")), int(m.group("y2"))

def _collect_metrics_paths_from_final(final_csv_path: str):
    """
    metrics ìˆ˜ì§‘ ìš°ì„ ìˆœìœ„:
      1) ë¡œì»¬ì— ì´ë¯¸ ìˆìœ¼ë©´ ì‚¬ìš©
         - ì¤‘ì²© í´ë”: <base>/<key>/<year>/<key>_<year>_<year>_metrics.csv
         - í‰ë©´ íŒŒì¼: <base>/<key>_<year>_<year>_metrics.csv
      2) ì—†ìœ¼ë©´ Google Driveì—ì„œ ì¦‰ì‹œ ë‚´ë ¤ë°›ì•„ ë¡œì»¬ì— ì €ì¥ í›„ ì‚¬ìš©
         - ì €ì¥ ìœ„ì¹˜: <base>/<key>/<year>/<key>_<year>_<year>_metrics.csv
    """
    from pathlib import Path as _Path

    base_dir = _Path(final_csv_path).parent
    keys, y1, y2 = _parse_keys_years_from_final(final_csv_path)

    found: list[Path] = []
    seen: set[str] = set()

    for key in keys:
        for y in range(y1, y2 + 1):
            # 1) ë¡œì»¬ í›„ë³´ ê²½ë¡œë“¤ (ì¤‘ì²©/í‰ë©´ ë‘˜ ë‹¤ í™•ì¸)
            nested = base_dir / key / str(y) / f"{key}_{y}_{y}_metrics.csv"
            flat   = base_dir / f"{key}_{y}_{y}_metrics.csv"

            local_candidates = [nested, flat]
            hit = None
            for p in local_candidates:
                if p.exists():
                    hit = p
                    break

            # 2) ë¡œì»¬ì— ì—†ìœ¼ë©´ â†’ Driveì—ì„œ ë‚´ë ¤ë°›ì•„ í‘œì¤€ ìœ„ì¹˜(nested)ì— ì €ì¥
            if hit is None:
                try:
                    nested.parent.mkdir(parents=True, exist_ok=True)
                except Exception:
                    pass
                ok = _gdrive_download_metric_by_key(key, y, nested)
                if ok and nested.exists():
                    hit = nested

            # 3) ìˆ˜ì§‘
            if hit is not None:
                try:
                    rp = str(hit.resolve())
                except Exception:
                    rp = str(hit)
                if rp not in seen:
                    found.append(hit)
                    seen.add(rp)

    return found


def build_summary_from_metrics_for_final(final_csv_path: str, out_csv_path: str):
    """
    per-year '*_metrics.csv'ë§Œ ì •í™•íˆ ëª¨ì•„ í•©ì‚°í•´ summary ìƒì„± (aggregateì™€ 1:1).
    - ëŒ€ìƒ ìˆ˜ì§‘: _collect_metrics_paths_from_final(final_csv_path)
    - editorial: authorships_removed + authorships_removed_empty_list í•©ì‚° (ë‘˜ ì¤‘ ë¬´ì—‡ì´ ì™€ë„ ë™ì¼)
    - ROR ë³´ì •: before - after ë§Œí¼ enrichedë¡œ ì´ë™
    - ë³´ê°•ë¥  ë¶„ëª¨: (missing + enriched)
    - NaN/ê³µë°±/ì‰¼í‘œ/í¼ì„¼íŠ¸ ë¬¸ìì—´ ì•ˆì „ íŒŒì‹±
    """
    import pandas as pd
    from pathlib import Path

    base_dir = Path(final_csv_path).parent
    metric_paths = _collect_metrics_paths_from_final(final_csv_path)  # âœ… rglob ê¸ˆì§€

    def _to_num(x):
        """' 1,234 ' / '12.3%' / '' / 'NaN' â†’ ì•ˆì „ ìˆ«ì ë³€í™˜"""
        if x is None:
            return None
        s = str(x).strip()
        if s == "" or s.lower() == "nan":
            return None
        s = s.replace(",", "")
        if s.endswith("%"):
            s = s[:-1]
        try:
            f = float(s)
            return int(f) if float(int(f)) == f else f
        except Exception:
            return None

    if not metric_paths:
        print(f"[WARN] build_summary: metrics íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {base_dir} (per-year metrics)")
        df_empty = pd.DataFrame([{
            # âœ… ì»¬ëŸ¼ëª… í†µì¼(ë¹„ì–´ë„ downstream ë¹„êµ ê¹¨ì§€ì§€ ì•Šê²Œ)
            "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": 0,
            "Editorial Material ì‚­ì œ ìˆ˜": 0,
            "ìµœì¢… CSV í–‰ ìˆ˜(í•©ê³„)": 0,
            "DOI ê²°ì¸¡ ìˆ˜(í•©ì‚°)": 0,
            "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": 0,
            "DOI ë³´ê°•ë¥ ": "0.00%",
            "ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)": 0,
            "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": 0,
            "ROR ID ë³´ê°•ë¥ ": "0.00%",
        }])
        Path(out_csv_path).parent.mkdir(parents=True, exist_ok=True)
        df_empty.to_csv(out_csv_path, index=False, encoding='utf-8-sig')
        return out_csv_path

    # í•©ì‚° ë²„í‚· (canonical í‚¤)
    totals = {
        "total_collected": 0.0,
        "final_csv_rows": 0.0,
        "authorships_removed": 0.0,  # â† Editorial(ì•„ë˜ ë™ë“± í‚¤ë„ ì—¬ê¸°ë¡œ í•©ì‚°)
        "doi_missing": 0.0, "doi_enriched": 0.0,
        "ror_missing": 0.0, "ror_enriched": 0.0,
        "ror_missing_before_extract": 0.0, "ror_missing_after_extract": 0.0,
    }
    # editorial ë™ë“± í‚¤(ë‘˜ ì¤‘ í•˜ë‚˜ê°€ metricsì— ì¡´ì¬)
    editorial_equivs = {"authorships_removed", "authorships_removed_empty_list"}

    # per-year metricsë§Œ ëˆ„ì 
    for p in metric_paths:
        try:
            m = pd.read_csv(p, encoding='utf-8-sig', dtype=str, on_bad_lines='warn')
            if m is None or m.empty or "key" not in m.columns or "value" not in m.columns:
                continue
        except Exception as e:
            print(f"[WARN] build_summary: metrics íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {p}: {e}")
            continue

        # per-row ì²˜ë¦¬ 
        for _, row in m.iterrows():
            k = (row.get("key") or "").strip()
            v = _to_num(row.get("value"))
            if v is None:
                continue

            if k in editorial_equivs:
                totals["authorships_removed"] += v  # í•©ì‚°
                continue
            
            if k in totals:
                totals[k] += v
        

    # í•©ì‚° â†’ ì •ìˆ˜ ì•ˆì „ ë³€í™˜
    def _ival(x):
        try:
            return int(x) if x is not None else 0
        except Exception:
            return 0

    total_collected   = _ival(totals["total_collected"])
    final_rows        = _ival(totals["final_csv_rows"])
    editorial_removed = _ival(totals["authorships_removed"])
    doi_missing       = _ival(totals["doi_missing"])
    doi_enriched      = _ival(totals["doi_enriched"])
    ror_missing       = _ival(totals["ror_missing"])
    ror_enriched      = _ival(totals["ror_enriched"])

    # ğŸ” ROR before/after ë³´ì • (aggregate ë™ì¼)
    ror_before = _ival(totals["ror_missing_before_extract"])
    ror_after  = _ival(totals["ror_missing_after_extract"])
    if ror_before or ror_after:
        fixed = max(0, ror_before - ror_after)
        ror_enriched += fixed
        ror_missing = max(0, ror_missing - fixed)

    # ğŸ“ ë³´ê°•ë¥ : enriched / (missing + enriched)
    doi_denom = doi_missing + doi_enriched
    ror_denom = ror_missing + ror_enriched
    doi_rate = f"{(100.0 * doi_enriched / doi_denom):.2f}%" if doi_denom else "0.00%"
    ror_rate = f"{(100.0 * ror_enriched / ror_denom):.2f}%" if ror_denom else "0.00%"

    # ğŸ“Š ì¶œë ¥ (aggregateì™€ ë™ì¼ ë ˆì´ë¸”)
    df = pd.DataFrame([{
        "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": total_collected,
        "Editorial Material ì‚­ì œ ìˆ˜": editorial_removed,
        "ìµœì¢… CSV í–‰ ìˆ˜(í•©ê³„)": final_rows,
        "DOI ê²°ì¸¡ ìˆ˜(í•©ì‚°)": doi_missing,
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": doi_enriched,
        "DOI ë³´ê°•ë¥ ": doi_rate,
        "ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)": ror_missing,
        "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": ror_enriched,
        "ROR ID ë³´ê°•ë¥ ": ror_rate,
    }])

    Path(out_csv_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv_path, index=False, encoding='utf-8-sig')
    return out_csv_path





def run_pipeline(issns: List[str], year_start: int, year_end: int,
                 email: str = 's0124kw@gmail.com', include_only_with_abstract: bool = False,
                 make_html: bool = False):

    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
    issns: ì €ë„ ISSN ë¦¬ìŠ¤íŠ¸
    year_start, year_end: ì—°ë„ ë²”ìœ„
    email: OpenAlex APIìš© ì´ë©”ì¼
    """

    """
    íŒŒì¼ëª… prefix ê³„ì‚°
    """
    config.email = email
    journal_prefixes = []
    for issn in issns:
        try:
            src = next(
                Sources().filter(issn=[issn])
                         .select(['display_name'])
                         .paginate(per_page=1)
            )
            display = src[0]['display_name'] if isinstance(src, list) else src['display_name']
        except Exception:
            display = issn  # fallback: ISSN ê·¸ëŒ€ë¡œ ì‚¬ìš©
        safe = re.sub(r'\W+', '_', display).strip('_')
        journal_prefixes.append(safe)
    prefix = "-".join(journal_prefixes)


    # 1ï¸âƒ£ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë° DOI ë³´ê°• (real1)
    json_base = f"{prefix}_{year_start}_{year_end}"
    json_parts = [f"{json_base}_part{i+1}.json" for i in range(3)]
    json_merged = f"{json_base}.json"
    csv_file = f"{json_base}.csv"
    csv_ror = f"{json_base}_ror.csv"
    csv_ror_extract = f"{json_base}_ror_extract.csv"
    csv_ror_extract_name = f"{json_base}_ror_extract_name.csv"
    html_ror_extract_name_network = f"{json_base}_ror_extract_name_network.html"
    cache_file = Path("ror_cache.pkl")

    # 1. real1: ë…¼ë¬¸ ìˆ˜ì§‘ ë° ë³´ê°•
    print("[1/7] ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë° DOI ë³´ê°•...")
    real1.main(issns=issns, year_start=year_start, year_end=year_end, email=email, prefix=prefix, include_only_with_abstract=include_only_with_abstract)

    # 2. real2: JSON â†’ CSV ë³€í™˜
    print("[2/7] JSON â†’ CSV ë³€í™˜...")
    real2.main(input_json=json_merged, output_csv=csv_file, prefix=prefix, year_start=year_start, year_end=year_end)

    # 3. real3: ROR ë§¤í•‘
    print("[3/7] ROR ë§¤í•‘...")
    asyncio.run(real3.process(
        input_csv=Path(csv_file),
        output_csv=Path(csv_ror),
        cache_file=cache_file,
        concurrency=20
    ))

    # 4. real4: ROR ì¶”ì¶œ
    print("[4/7] ROR ì¶”ì¶œ ë° í†µê³„...")
    real4.main(input_csv=csv_ror, output_csv=csv_ror_extract, prefix=prefix, year_start=year_start, year_end=year_end)

    # 5. real5: ROR ID â†’ ê¸°ê´€ëª… ë§¤í•‘
    print("[5/7] ROR ID â†’ ê¸°ê´€ëª… ë§¤í•‘...")
    real5.main(input_csv=csv_ror_extract, output_csv=csv_ror_extract_name, prefix=prefix, year_start=year_start, year_end=year_end)

    # 6. real6: í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (HTML ìƒì„±)
    html_path = None
    if make_html:
        print("[6/7] í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”...")
        html_path = real6.main(
            input_csv=csv_ror_extract_name,
            output_html=html_ror_extract_name_network,
            size_by="eigenvector",
            color_by="eigenvector"
        )
        if html_path:
            print(f"â†’ ì‹œê°í™” íŒŒì¼ ìƒì„±: {html_path}")


    print("[7/7] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

    return csv_ror_extract_name, html_path


def make_html_from_csv(final_csv_path: str) -> str:
    """
    ìµœì¢… CSVë¡œ ì‹œê°í™”ë¥¼ 2ê°€ì§€ ë²„ì „(degree / eigenvector)ìœ¼ë¡œ ìƒì„±í•œ ë’¤,
    í•˜ë‚˜ì˜ HTMLì— ë‘ ë·°ë¥¼ iframeìœ¼ë¡œ ë‚˜ë€íˆ ë‹´ì•„ ë°˜í™˜(í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜/ë¦¬í„´íƒ€ì… ë³€ê²½ ì—†ìŒ).
    """
    if not final_csv_path.endswith("_ror_extract_name.csv"):
        raise ValueError("final_csv_path must end with _ror_extract_name.csv")

    base = final_csv_path[:-len("_ror_extract_name.csv")]

    # 1) ê°œë³„ ì‹œê°í™” HTML ìƒì„±(ê°™ì€ í´ë”ì— ìœ ì§€)
    html_degree = f"{base}_degree_network.html"
    html_eigen  = f"{base}_eigenvector_network.html"

    # ê¶Œì¥ ì¡°í•©: í¬ê¸°=degree / ìƒ‰=eigenvector  &  í¬ê¸°=eigenvector / ìƒ‰=degree
    real6.main(
        input_csv=final_csv_path,
        output_html=html_degree,
        size_by="degree",
        color_by="eigenvector"
    )
    real6.main(
        input_csv=final_csv_path,
        output_html=html_eigen,
        size_by="eigenvector",
        color_by="degree"
    )

    # 2) ê°™ì€ í´ë”ì˜ ë‘ HTMLì„ iframeìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë˜í¼ ìƒì„±
    deg_name = Path(html_degree).name
    eig_name = Path(html_eigen).name

    output_html = final_csv_path.replace(
        "_ror_extract_name.csv",
        "_ror_extract_name_network.html"
    )

    wrapper = f"""<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Collaboration Networks (Degree & Eigenvector)</title>
  <style>
    body {{ background:#111; color:#eaeaea; font-family:system-ui, sans-serif; }}
    .row {{ display:flex; gap:12px; flex-wrap:wrap; padding:12px; }}
    .panel {{ flex:1 1 48%; min-width:380px; border:1px solid #333; border-radius:8px; background:#151515; }}
    .panel h3 {{ margin:8px 12px; font-weight:600; }}
    iframe {{ width:100%; height:820px; border:0; background:#111; border-top:1px solid #333; }}
  </style>
</head>
<body>
  <div class="row">
    <div class="panel">
      <h3>Degree-based view (size=degree, color=eigenvector)</h3>
      <iframe src="{deg_name}"></iframe>
    </div>
    <div class="panel">
      <h3>Eigenvector-based view (size=eigenvector, color=degree)</h3>
      <iframe src="{eig_name}"></iframe>
    </div>
  </div>
</body>
</html>
"""
    Path(output_html).write_text(wrapper, encoding="utf-8")
    return output_html

def make_html_string_from_csv(final_csv_path: str, size_by: str, color_by: str) -> str:
    """
    ìµœì¢… CSVì—ì„œ ë„¤íŠ¸ì›Œí¬ HTML ë¬¸ìì—´ ìƒì„±(íŒŒì¼ ì €ì¥ ì—†ìŒ).
    - size_by: "degree" ë˜ëŠ” "eigenvector"
    - color_by: "eigenvector" ë˜ëŠ” "degree"
    """
    res = real6.main(
        input_csv=final_csv_path,
        output_html=None,      # â† íŒŒì¼ ì €ì¥ ì•ˆ í•¨
        size_by=size_by,
        color_by=color_by
    )

    # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ ê²½ë¡œ ë¬¸ìì—´ì´ ëŒì•„ì˜¤ë©´ íŒŒì¼ì„ ì½ì–´ì„œ HTML ë¬¸ìì—´ë¡œ ì¹˜í™˜
    from pathlib import Path as _Path
    if isinstance(res, str):
        s = res.strip().lower()
        if s.startswith("<!doctype") or s.startswith("<html"):
            return res
        if res.endswith(".html") and _Path(res).exists():
            return _Path(res).read_text(encoding="utf-8")
    raise RuntimeError("HTML ìƒì„± ì‹¤íŒ¨: ë°˜í™˜ê°’ì´ ë¹„ì—ˆê±°ë‚˜ HTMLì´ ì•„ë‹™ë‹ˆë‹¤.")


# ì €ì¥ ì „ëµ ì„¤ì •
USE_JOURNAL_NAME_SLUG: bool = True
LOCAL_WORKDIR = Path("workdir_tmp")
LOCAL_WORKDIR.mkdir(parents=True, exist_ok=True)

_slug_rx = re.compile(r"[^a-zA-Z0-9]+")
def _to_slug(name: str) -> str:
    return _slug_rx.sub("_", name.strip().lower()).strip("_")

_JSLUG_CACHE: dict[str, str] = {}
def _resolve_journal_name_slug(issn: str) -> str:
    """ISSN -> OpenAlex display_name -> íŒŒì¼ì‹œìŠ¤í…œ ì•ˆì „ ìŠ¬ëŸ¬ê·¸"""
    if issn in _JSLUG_CACHE:
        return _JSLUG_CACHE[issn]
    try:
        src = next(
            Sources().filter(issn=[issn])
                    .select(['display_name'])
                    .paginate(per_page=1)
        )
        display = src[0]['display_name'] if isinstance(src, list) else src['display_name']
        base_name = display
    except Exception:
        base_name = issn
    slug = _to_slug(base_name)
    _JSLUG_CACHE[issn] = slug
    return slug

def _key_name_for(issn: str) -> str:
    """íŒŒì¼ëª…/í´ë”ëª…ì— ì“¸ í‚¤(ì €ë„ëª… ìŠ¬ëŸ¬ê·¸ ë˜ëŠ” ISSN)"""
    # âœ… ëŒ€í‘œí‚¤ê°€ ì´ë¯¸ ì •í•´ì¡Œë‹¤ë©´ ë¬´ì¡°ê±´ ê·¸ê²ƒë§Œ ì‚¬ìš©
    if 'FORCE_JOURNAL_KEY' in globals() and FORCE_JOURNAL_KEY:
        return FORCE_JOURNAL_KEY
    return _resolve_journal_name_slug(issn) if USE_JOURNAL_NAME_SLUG else issn

# ----- Secrets/ENVì—ì„œ Drive ì¸ì¦/í´ë” ì •ë³´ ì½ê¸° -----

def _get_gdrive_root_folder_id() -> Optional[str]:
    """
    Streamlit Cloud Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ë£¨íŠ¸ í´ë” ID ì½ê¸°
    - secrets: GDRIVE_FOLDER_ID
    - env    : GDRIVE_FOLDER_ID
    """
    if st is not None:
        try:
            return st.secrets["GDRIVE_FOLDER_ID"]
        except Exception:
            pass
    return os.getenv("GDRIVE_FOLDER_ID")


def _get_service_account_info() -> Optional[dict]:
    """
    âœ… ì˜¤ì§ Streamlit secretsì˜ [gcp_service_account]ë§Œ ì§€ì›
       (í•„ìš” ì‹œ ENV GDRIVE_SA_JSON JSON ë¬¸ìì—´ì€ ë³´ì¡°ìš©)
    - ë¬¸ìì—´/ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ í—ˆìš©
    - private_key ì˜ '\\n' â†’ ì‹¤ì œ ê°œí–‰ìœ¼ë¡œ ì¹˜í™˜
    """
    raw = None
    # 1) Streamlit secrets
    if st is not None:
        try:
            raw = st.secrets["gcp_service_account"]
        except Exception:
            print("[GDRIVE] gcp_service_account not found in st.secrets")
            raw = None
    # 2) (ì„ íƒ) í™˜ê²½ë³€ìˆ˜ JSON ë³´ì¡°
    if raw is None:
        raw = os.getenv("GDRIVE_SA_JSON")
        if raw is None:
            print("[GDRIVE] GDRIVE_SA_JSON not found in env")
    if raw is None:
        return None

    # ë”•ì…”ë„ˆë¦¬ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if isinstance(raw, Mapping):
        sa = dict(raw)
    else:
        s = str(raw).strip()
        # JSON ë¨¼ì € ì‹œë„
        try:
            sa = _json.loads(s)
        except Exception:
            # ì¼ë¶€ í™˜ê²½ì—ì„œ ë‹¨ì¼ë”°ì˜´í‘œ dict ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš°
            try:
                sa = _ast.literal_eval(s)
                if not isinstance(sa, Mapping):
                    raise ValueError("literal_eval did not return a mapping")
                sa = dict(sa)
            except Exception as e:
                print("[GDRIVE] Could not parse gcp_service_account string:", repr(e))
                print("[GDRIVE] First 120 chars:", s[:120])
                return None

    # private_key ê°œí–‰ ì •ê·œí™”
    pk = sa.get("private_key")
    if not pk:
        print("[GDRIVE] private_key missing in gcp_service_account")
        return None
    # private_keyê°€ í™˜ê²½/Secretsì— '\\n' í˜•íƒœë¡œ ë“¤ì–´ì˜¬ ë•Œ ì‹¤ì œ ê°œí–‰ìœ¼ë¡œ ë³€í™˜
    pk = sa.get("private_key")
    if pk:
        try:
            sa["private_key"] = str(pk).replace("\\n", "\n")
        except Exception:
            sa["private_key"] = pk
    return sa


def _gdrive_client():
    # âœ… oauth2client ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
    from oauth2client.service_account import ServiceAccountCredentials
    from pydrive2.auth import GoogleAuth
    from pydrive2.drive import GoogleDrive

    sa_info = _get_service_account_info()
    if not sa_info:
        raise RuntimeError("Google Drive ì„œë¹„ìŠ¤ê³„ì • ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. secrets ë˜ëŠ” ENVë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    scopes = ['https://www.googleapis.com/auth/drive']
    # í•µì‹¬: google-auth ëŒ€ì‹  oauth2clientë¡œ í¬ë¦¬ë´ì…œ ìƒì„±
    creds = ServiceAccountCredentials.from_json_keyfile_dict(sa_info, scopes=scopes)

    gauth = GoogleAuth()
    gauth.credentials = creds
    gauth.Authorize()  # â† oauth2client í¬ë¦¬ë´ì…œì€ authorize() ì§€ì›
    drive = GoogleDrive(gauth)
    return drive


def _find_folder(drive: GoogleDrive, parent_id: str, name: str) -> Optional[str]:
    q = f"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'"
    lst = drive.ListFile({'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True}).GetList()
    return lst[0]['id'] if lst else None

def _ensure_folder(drive: GoogleDrive, parent_id: str, name: str) -> str:
    fid = _find_folder(drive, parent_id, name)
    if fid:
        return fid
    f = drive.CreateFile({'title': name, 'parents':[{'id': parent_id}],
                          'mimeType': 'application/vnd.google-apps.folder'})
    f.Upload()
    return f['id']

def _find_file(drive: GoogleDrive, parent_id: str, name: str) -> Optional[str]:
    q = f"'{parent_id}' in parents and trashed=false and mimeType!='application/vnd.google-apps.folder' and title='{name}'"
    lst = drive.ListFile({'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True}).GetList()
    return lst[0]['id'] if lst else None

def _download_file(drive: GoogleDrive, file_id: str, local_path: Path):
    f = drive.CreateFile({'id': file_id})
    local_path.parent.mkdir(parents=True, exist_ok=True)
    f.GetContentFile(str(local_path))

def _upload_file(drive: GoogleDrive, parent_id: str, local_path: Path, name: Optional[str] = None):
    """ë™ì¼ ì´ë¦„ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°(ì—…ì„œíŠ¸)"""
    name = name or local_path.name
    q = (
        f"'{parent_id}' in parents and trashed=false "
        f"and mimeType!='application/vnd.google-apps.folder' and title='{name}'"
    )
    exist = drive.ListFile({
        'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True
    }).GetList()

    if exist:
        f = drive.CreateFile({'id': exist[0]['id']})
        f.SetContentFile(str(local_path))
        f.Upload(param={'supportsAllDrives': True})
        return f['id']

    f = drive.CreateFile({'title': name, 'parents':[{'id': parent_id}]})
    f.SetContentFile(str(local_path))
    f.Upload(param={'supportsAllDrives': True})
    return f['id']


def _gdrive_locate_metric_by_key(key: str, year: int):
    """
    ROOT / <key> / <year> / <key>_<year>_<year>_metrics.csv ë¥¼ ì°¾ì•„ file_idë¥¼ ë°˜í™˜.
    (ISSN ì—†ì´ key(slug)ë§Œ ê°€ì§€ê³  ì°¾ê¸°)
    """
    root_id = _get_gdrive_root_folder_id()
    if not root_id:
        raise RuntimeError("GDRIVE_FOLDER_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    drive = _gdrive_client()

    # í´ë” ì°¾ì•„ê°€ê¸°
    key_folder = _ensure_folder(drive, root_id, key)
    year_folder = _ensure_folder(drive, key_folder, str(year))

    metrics_name = f"{key}_{year}_{year}_metrics.csv"
    file_id = _find_file(drive, year_folder, metrics_name)
    return drive, year_folder, metrics_name, file_id

def _gdrive_download_metric_by_key(key: str, year: int, local_path: Path) -> bool:
    """
    í•´ë‹¹ key, yearì˜ metrics íŒŒì¼ì„ Driveì—ì„œ ì°¾ì•„ ë¡œì»¬ë¡œ ë‚´ë ¤ë°›ëŠ”ë‹¤.
    ì„±ê³µ ì‹œ True, ì—†ìœ¼ë©´ False.
    """
    try:
        drive, year_folder, metrics_name, file_id = _gdrive_locate_metric_by_key(key, year)
        if not file_id:
            return False
        _download_file(drive, file_id, local_path)
        return True
    except Exception as e:
        print(f"[WARN] _gdrive_download_metric_by_key: {key=}, {year=} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# ----- issn/year ì›ê²© íŒŒì¼ ìœ„ì¹˜ -----
def _gdrive_locate_piece(issn: str, year: int):
    """
    ROOT / <key> / <year> / <key>_<year>_ror_extract_name.csv
    """
    root_id = _get_gdrive_root_folder_id()
    if not root_id:
        raise RuntimeError("GDRIVE_FOLDER_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    key = _key_name_for(issn)
    drive = _gdrive_client()
    key_folder = _ensure_folder(drive, root_id, key)
    year_folder = _ensure_folder(drive, key_folder, str(year))
    fname = f"{key}_{year}_ror_extract_name.csv"
    file_id = _find_file(drive, year_folder, fname)
    return drive, key, key_folder, year_folder, fname, file_id

def _gdrive_piece_exists(issn: str, year: int) -> bool:
    try:
        _, _, _, _, _, file_id = _gdrive_locate_piece(issn, year)
        return file_id is not None
    except Exception:
        return False

def _gdrive_download_piece(issn: str, year: int, local_path: Path) -> bool:
    try:
        # drive, key, ì—°ë„ í´ë”, ìµœì¢… CSV íŒŒì¼ id
        drive, key, _, year_folder, _, file_id = _gdrive_locate_piece(issn, year)
        if not file_id:
            return False

        # 1) ìµœì¢… CSV ë‚´ë ¤ë°›ê¸°
        _download_file(drive, file_id, local_path)

        # 2) metrics.csvê°€ ìˆìœ¼ë©´ ê°™ì´ ë‚´ë ¤ë°›ê¸°
        def _find_file(drive, parent_id, name):
            q = (
                f"'{parent_id}' in parents and trashed=false "
                f"and mimeType!='application/vnd.google-apps.folder' and title='{name}'"
            )
            lst = drive.ListFile({'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True}).GetList()
            return lst[0]['id'] if lst else None

        metrics_name = f"{key}_{year}_{year}_metrics.csv"   # â† per-ì—°ë„ ë©”íŠ¸ë¦­ ê·œì¹™
        m_id = _find_file(drive, year_folder, metrics_name)
        if m_id:
            _download_file(drive, m_id, local_path.parent / metrics_name)

        return True
    except Exception:
        return False

def _gdrive_upload_piece(issn: str, year: int, local_path: Path):
    # drive í•¸ë“¤, key(ì €ë„ í´ë”ëª…), ì—°ë„ í´ë”, ì—…ë¡œë“œë  ìµœì¢… CSV íŒŒì¼ëª…
    drive, key, _, year_folder, fname, _ = _gdrive_locate_piece(issn, year)

    # 1) ìµœì¢… CSV ì—…ë¡œë“œ
    _upload_file(drive, year_folder, local_path, fname)

    # 2) metrics.csvê°€ ë¡œì»¬ ì—°ë„ í´ë”ì— ìˆìœ¼ë©´ í•¨ê»˜ ì—…ë¡œë“œ
    metrics_csv = local_path.parent / f"{key}_{year}_{year}_metrics.csv"
    if metrics_csv.exists():
        _upload_file(drive, year_folder, metrics_csv, metrics_csv.name)


# ======================================================================
# ì—°Â·ì €ë„ ë‹¨ìœ„ ì €ì¥/ì¬ì‚¬ìš© (Drive ì‚¬ìš©)
# ======================================================================

# [# ADDED] ISSN ì •ê·œí™”
_issn_rx = re.compile(r"^\d{4}-\d{3}[\dxX]$")

def _normalize_issn_list(issns: List[str]) -> List[str]:
    norm = []
    for s in issns:
        if not s:
            continue
        s = s.strip()
        if "-" not in s and len(s) == 8:
            s = s[:4] + "-" + s[4:]
        if _issn_rx.match(s):
            norm.append(s.upper())
        else:
            print(f"[WARN] ì˜ëª»ëœ ISSN í˜•ì‹ ê±´ë„ˆëœ€: {s!r}")
    # ì…ë ¥ ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
    seen, out = set(), []
    for t in norm:
        if t not in seen:
            out.append(t); seen.add(t)
    return out

# [# ADDED] ë¡œì»¬ ì„ì‹œ(ì¡°ê° ë‹¤ìš´ë¡œë“œ/ìƒì„± ìœ„ì¹˜)
def _local_piece_path(issn: str, year: int) -> Path:
    key = _key_name_for(issn)
    return LOCAL_WORKDIR / key / str(year) / f"{key}_{year}_ror_extract_name.csv"

# [# ADDED] ì—°Â·ì €ë„ 1ê±´ ì²˜ë¦¬ (ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„± í›„ ì—…ë¡œë“œ)
def _run_one_piece(issn: str, year: int, email: str,
                   include_only_with_abstract: bool = False) -> Path:
    local_out = _local_piece_path(issn, year)
    
    if local_out.exists() and not bool(os.environ.get('OVERWRITE_LOCAL_PIECE')):
        return local_out

    # 1) ì›ê²© ì¡´ì¬ ì‹œ â†’ ë‹¤ìš´ë¡œë“œ
    if _gdrive_piece_exists(issn, year):
        if _gdrive_download_piece(issn, year, local_out):
            return local_out

    # 2) ì—†ìœ¼ë©´ ìƒì„±
    local_out.parent.mkdir(parents=True, exist_ok=True)
    prefix = _key_name_for(issn)

    config.email = email
    real1.main(issns=[issn], year_start=year, year_end=year,
               email=email, prefix=prefix, include_only_with_abstract=include_only_with_abstract,
               anchor_path=str(local_out))

    json_merged   = f"{prefix}_{year}_{year}.json"
    tmp_csv       = f"{prefix}_{year}_{year}.csv"
    tmp_csv_ror   = f"{prefix}_{year}_{year}_ror.csv"
    tmp_csv_ror_ex= f"{prefix}_{year}_{year}_ror_extract.csv"
    tmp_csv_name  = f"{prefix}_{year}_{year}_ror_extract_name.csv"

    real2.main(input_json=json_merged, output_csv=tmp_csv,
               prefix=prefix, year_start=year, year_end=year,
               anchor_path=str(local_out))

    asyncio.run(real3.process(
        input_csv=Path(tmp_csv),
        output_csv=Path(tmp_csv_ror),
        cache_file=Path("ror_cache.pkl"),
        concurrency=20, anchor_path=str(local_out)
    ))

    real4.main(input_csv=tmp_csv_ror, output_csv=tmp_csv_ror_ex,
               prefix=prefix, year_start=year, year_end=year,
               anchor_path=str(local_out))

    real5.main(input_csv=tmp_csv_ror_ex, output_csv=tmp_csv_name,
               prefix=prefix, year_start=year, year_end=year)

    # ìµœì¢… ì¡°ê°: ë¡œì»¬ í‘œì¤€ ìœ„ì¹˜ë¡œ ì´ë™
    Path(tmp_csv_name).parent.mkdir(parents=True, exist_ok=True)
    Path(tmp_csv_name).replace(local_out)

    # 3) ì›ê²© ì—…ë¡œë“œ
    _gdrive_upload_piece(issn, year, local_out)
    #ì—°ë„ ë‹¨ìœ„ ì²˜ë¦¬ í›„ ì ê¹ ì‰¬ê¸°(ë°±ì˜¤í”„)
    # 429 ì—ëŸ¬ ì™„í™”(ROR ì¿¼ë¦¬ ëª°ë¦¬ë©´ ë°œìƒ)
    import time, random
    time.sleep(5 + random.uniform(0, 0.5))

    return local_out

# [# ADDED] ì¡°ê° ë³‘í•© â†’ ìµœì¢… CSV
def _collect_merge(issns: List[str], year_start: int, year_end: int) -> Path:
    issns = _normalize_issn_list(issns)
    piece_paths: List[Path] = []
    keys: List[str] = []

    for issn in issns:
        key = _key_name_for(issn)
        keys.append(key)
        for y in range(year_start, year_end + 1):
            local_piece = _local_piece_path(issn, y)
            if local_piece.exists():
                piece_paths.append(local_piece)
            else:
                if _gdrive_download_piece(issn, y, local_piece):
                    piece_paths.append(local_piece)

    if not piece_paths:
        raise FileNotFoundError("ì„ íƒ ë²”ìœ„ì˜ ì¡°ê° CSVê°€ ì—†ìŠµë‹ˆë‹¤.")

    dfs = []
    for p in piece_paths:
        p = Path(p)
        if not p.exists():
            print(f"[_collect_merge] ê²½ê³ : ì¡°ê° íŒŒì¼ ì—†ìŒ: {p}")
            continue

        # 1) ìš°ì„ : C ì—”ì§„ìœ¼ë¡œ ë¹ ë¥´ê³  ê²¬ê³ í•˜ê²Œ ì‹œë„ (low_memory=False í—ˆìš©)
        try:
            df_piece = pd.read_csv(
                p,
                encoding="utf-8-sig",
                quoting=csv.QUOTE_MINIMAL,
                quotechar='"',
                escapechar='\\',
                low_memory=False
            )
            dfs.append(df_piece)
            continue
        except ParserError as e:
            print(f"[_collect_merge] C engine ParserError for {p}: {e} -> retry with python engine")
        except Exception as e:
            print(f"[_collect_merge] C engine exception for {p}: {e} -> retry with python engine")

        # 2) C ì—”ì§„ ì‹¤íŒ¨ ì‹œ: python ì—”ì§„ìœ¼ë¡œ ì¬ì‹œë„ (low_memory ì˜µì…˜ ì—†ìŒ)
        try:
            df_piece = pd.read_csv(
                p,
                engine="python",
                encoding="utf-8-sig",
                quoting=csv.QUOTE_MINIMAL,
                quotechar='"',
                escapechar='\\',
                on_bad_lines='warn'
            )
            dfs.append(df_piece)
            continue
        except Exception as e:
            print(f"[_collect_merge] python engine read failed for {p}: {e} -> falling back to csv.DictReader")

        # 3) ìµœí›„ì˜ ì•ˆì „ë§: csv.DictReaderë¡œ ë¬´ì¡°ê±´ ì½ì–´ì„œ ëª¨ë“  í–‰ì„ í™•ë³´ (ëˆ„ë½ ë°©ì§€)
        try:
            rows = []
            with open(p, 'r', encoding='utf-8-sig', newline='') as fh:
                rdr = csv.DictReader(fh)
                for row in rdr:
                    rows.append(row)
            df_piece = pd.DataFrame(rows)
            dfs.append(df_piece)
            print(f"[_collect_merge] csv.DictReader fallback succeeded for {p} (rows={len(df_piece)})")
        except Exception as e2:
            print(f"[_collect_merge] csv.DictReader fallback also failed for {p}: {e2}")
            raise
    # ë³‘í•©
    if not dfs:
        merged = pd.DataFrame()
    else:
        merged = pd.concat(dfs, ignore_index=True)



    # [# ADDED] ì¤‘ë³µ ì œê±°(ê°€ëŠ¥í•˜ë©´ DOI ê¸°ì¤€)
    if "doi" in merged.columns:
        merged = merged.drop_duplicates(subset=["doi"])
    else:
        merged = merged.drop_duplicates()

    # ë³´ê¸° ì¢‹ì€ ì—´ ìˆœì„œ(ì„ íƒ)
    preferred = [c for c in ["title", "doi", "published_year", "host_venue_issn_l",
                             "institution_name", "ror_id"] if c in merged.columns]
    merged = merged[[*preferred, *[c for c in merged.columns if c not in preferred]]]

    merged_name = f"{'-'.join(keys)}_{year_start}_{year_end}_ror_extract_name.csv"
    out_path = LOCAL_WORKDIR / merged_name
    merged.to_csv(out_path, index=False, encoding="utf-8-sig")
    return out_path


# ======================================================================
# [ì—”ë“œí¬ì¸íŠ¸] run_pipeline_cached â€” app.pyê°€ í˜¸ì¶œ (Drive ì˜êµ¬ ì €ì¥)
# ======================================================================
def run_pipeline_cached(issns: List[str], year_start: int, year_end: int,
                        email: str = 's0124kw@gmail.com',
                        include_only_with_abstract: bool = False,
                        make_html: bool = False,
                        base_dir: Path = Path("storage")):
    """
    1) ê° ì €ë„Ã—ì—°ë„ ì¡°ê°ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒì„± í›„ Drive ì—…ë¡œë“œ
    2) ì¡°ê°ë“¤ì„ ëª¨ì•„ ë¡œì»¬ì—ì„œ ìµœì¢… CSV ë³‘í•©
    3) (ì˜µì…˜) HTML ìƒì„±
    ì£¼ì˜: base_dir ì¸ìëŠ” í˜¸í™˜ì„±ë§Œ ìœ ì§€(Drive ì‚¬ìš©ìœ¼ë¡œ ë¬´ì‹œ)
    """
    issns = _normalize_issn_list(issns)

    # ì´ì „ ì‹¤í–‰ì˜ ì”ì¬ ì œê±° (ì „ì—­ ê°•ì œ ê³ ì • ì‚¬ìš© ì•ˆí•¨)
    global FORCE_JOURNAL_KEY
    FORCE_JOURNAL_KEY = None

    for issn in issns:
        for y in range(int(year_start), int(year_end) + 1):
            try:
                _run_one_piece(issn, y, email, include_only_with_abstract)
            except Exception as e:
                import traceback, sys
                # ì—ëŸ¬ ë¡œê¹…: ì–´ë–¤ ISSN/ì—°ë„ì—ì„œ ì‹¤íŒ¨í–ˆëŠ”ì§€ ëª…í™•íˆ ë‚¨ê¹€
                print(f"[run_pipeline_cached] ERROR processing ISSN={issn}, year={y}: {e}", file=sys.stderr)
                traceback.print_exc()
                # (ì„ íƒ) ì‹¤íŒ¨ì‹œ placeholder *_metrics.csvë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆìŒ(ìš”ì•½ ì§‘ê³„ ì™„ì„±ì„ ìœ„í•´)
                # continue í•˜ì—¬ ë‹¤ìŒ ì—°ë„/ISSNìœ¼ë¡œ ì§„í–‰
                continue

    final_csv_path = _collect_merge(issns, int(year_start), int(year_end))

    # per-year metrics.csv í•©ì‚° -> Summary ë°˜ì˜
    final_path = Path(final_csv_path)
    summary_path = final_path.with_name(final_path.stem + "_ror_extract_name_summary.csv")
    build_summary_from_metrics_for_final(str(final_csv_path), str(summary_path))

    html_path = None
    if make_html:
        html_path = make_html_from_csv(str(final_csv_path))

    return str(final_csv_path), html_path


# ======================================================================
# (ì˜µì…˜) ë¡œì»¬ ë‹¨ë… í…ŒìŠ¤íŠ¸
# ======================================================================
if __name__ == "__main__":
    example_issns = ['0043-1354','0011-9164','0733-9429']
    out_csv, _ = run_pipeline_cached(
        issns=example_issns,
        year_start=2017, year_end=2019,
        email='Your Email Here',
        include_only_with_abstract=False,
        make_html=False
    )
    print("FINAL:", out_csv)