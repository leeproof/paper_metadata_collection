import os, json, pandas as pd, asyncio
from pathlib import Path
from typing import List, Optional
from pyalex import Sources, config
import re
import csv
from contextlib import redirect_stdout
from pandas.errors import ParserError

# ê° ë‹¨ê³„ë³„ ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ import (í•¨ìˆ˜í™”)
from libs import real1_test_openalex_sequential_optimized_0722 as real1
from libs import real2_test_openalex_to_csv as real2
from libs import real3_ror_mapping_processor as real3
from libs import real4_ror_extract as real4
from libs import real5_name_mapping as real5
from libs import real6_visualize_fast as real6


# Google Drive adapter
import json as _json
import ast as _ast
from collections.abc import Mapping

try:
    import streamlit as st
except Exception:
    st = None # ë¡œì»¬ í™˜ê²½ì—ì„œë„ ë™ì‘í•˜ë„ë¡ ì„ íƒì  import
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

FORCE_JOURNAL_KEY: str | None = None  # âœ… ì‹¤í–‰ ë™ì•ˆ ëŒ€í‘œí‚¤ë¥¼ ê°•ì œë¡œ ê³ ì •


def _parse_keys_years_from_final(final_csv_path: str):
    """
    ìµœì¢… CSV íŒŒì¼ëª…ì—ì„œ keys(ì €ë„ ìŠ¬ëŸ¬ê·¸ë“¤)ì™€ ì—°ë„ ë²”ìœ„ë¥¼ íŒŒì‹±.
    í—ˆìš© íŒ¨í„´:
      1) <keys>_<y1>_<y2>_ror_extract_name.csv  (ë‹¤ì—°ë„)
      2) <keys>_<y1>_ror_extract_name.csv       (ë‹¨ì¼ì—°ë„ â†’ y2=y1ë¡œ ì²˜ë¦¬)
    """
    name = Path(final_csv_path).name

    # 1) y1_y2 íŒ¨í„´ ì‹œë„
    m = re.match(r"^(?P<keys>.+)_(?P<y1>\d{4})_(?P<y2>\d{4})_ror_extract_name\.csv$", name)
    if m:
        keys = m.group("keys").split("-")
        return keys, int(m.group("y1")), int(m.group("y2"))

    # 2) y1 ë‹¨ì¼ íŒ¨í„´ ì‹œë„ â†’ y2=y1
    m = re.match(r"^(?P<keys>.+)_(?P<y1>\d{4})_ror_extract_name\.csv$", name)
    if m:
        keys = m.group("keys").split("-")
        y = int(m.group("y1"))
        return keys, y, y

    raise ValueError(f"Unexpected final csv filename: {name}")

def _collect_metrics_paths_from_final(final_csv_path: str):
    """
    (ë©€í‹°ì €ë„ Ã— ë©€í‹°ì—°ë„)ìš© metrics ìˆ˜ì§‘:
      - ìµœì¢… CSV íŒŒì¼ëª…ì—ì„œ keysì™€ ì—°ë„ë²”ìœ„(y1..y2)ë¥¼ íŒŒì‹±
      - ê° (key, year)ì— ëŒ€í•´ ì•„ë˜ ìš°ì„ ìˆœìœ„ë¡œ 1ê°œì”© ì°¾ìŒ:
        1) ë¡œì»¬ ì¤‘ì²©: <base>/<key>/<year>/<key>_<year>_<year>_metrics.csv
        2) ë¡œì»¬ í‰ë©´: <base>/<key>_<year>_<year>_metrics.csv
        3) Drive ì •í™• ê²½ë¡œ: ROOT/<key>/<year>/<key>_<year>_<year>_metrics.csv
        4) Drive ì „ì—­ ì´ë¦„ê²€ìƒ‰: íŒŒì¼ëª…ì´ ì¼ì¹˜í•˜ëŠ” ì•„ë¬´ ê²½ë¡œ
      - ì°¾ìœ¼ë©´ í‘œì¤€ ë¡œì»¬ ê²½ë¡œ(<base>/<key>/<year>/...)ì— ë‘ê³  ë°˜í™˜ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨
    """
    from pathlib import Path as _Path

    base_dir = _Path(final_csv_path).parent
    keys, y1, y2 = _parse_keys_years_from_final(final_csv_path)

    import re as _re
    def _norm_key(k: str) -> str:
        s = (k or "").strip().lower()
        s = s.replace("-", "_")
        s = _re.sub(r"\s+", "_", s)
        s = _re.sub(r"_+", "_", s)
        return s
    keys = [_norm_key(k) for k in keys]  # â† ì´ í•œ ì¤„ì´ í•µì‹¬

    found = []
    seen = set()

    for key in keys:
        for y in range(y1, y2 + 1):
            std_local = base_dir / key / str(y) / f"{key}_{y}_{y}_metrics.csv"  # í‘œì¤€ ì €ì¥ ìœ„ì¹˜
            flat_local = base_dir / f"{key}_{y}_{y}_metrics.csv"

            hit = None
            # 1) ë¡œì»¬(ì¤‘ì²©/í‰ë©´)
            for cand in (std_local, flat_local):
                if cand.exists():
                    hit = cand
                    break

            # 2) Driveì—ì„œ ê°€ì ¸ì˜¤ê¸° (í‘œì¤€ ê²½ë¡œì— ì €ì¥)
            if hit is None:
                std_local.parent.mkdir(parents=True, exist_ok=True)
                # 2-a) ì˜ˆìƒ í´ë” êµ¬ì¡°ì—ì„œ ì‹œë„
                if _gdrive_download_metric_by_key_exact_path(key, y, std_local):
                    hit = std_local
                else:
                    # 2-b) ì „ì—­ ì´ë¦„ê²€ìƒ‰
                    if _gdrive_download_metric_by_name_search(key, y, std_local):
                        hit = std_local

            if hit is not None:
                rp = str(hit.resolve()) if hit.exists() else str(hit)
                if rp not in seen:
                    found.append(hit)
                    seen.add(rp)

    # (ì„ íƒ) ë””ë²„ê·¸: ê¸°ëŒ€ ê°œìˆ˜ vs ì‹¤ì œ ìˆ˜ì§‘ ê°œìˆ˜
    expected = len(keys) * (y2 - y1 + 1)
    print(f"[SUMMARY] metrics targets: expected={expected}, collected={len(found)}")
    if len(found) != expected:
        try:
            missing = []
            have = {p.name for p in found}
            for key in keys:
                for y in range(y1, y2 + 1):
                    name = f"{key}_{y}_{y}_metrics.csv"
                    if name not in have:
                        missing.append(name)
            if missing:
                print("[SUMMARY] missing metrics:", ", ".join(missing[:20]), ("... (+more)" if len(missing) > 20 else ""))
        except Exception:
            pass

    return found

# ==============================================================
# ê¸°ê´€ì •ë³´(ROR) ê¸°ì¤€ ê²°ì¸¡/ë³´ì™„ ê³„ì‚° í—¬í¼
# ==============================================================

def _load_mapped_affiliations_for_final(final_csv_path: str) -> list[str]:
    """
    final_csv_path(â€¦_ror_extract_name.csv)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ,
    workdir_tmp ì „ì²´ì—ì„œ mapped_affiliations_*.csvë¥¼ ì°¾ì•„
    normalized_affiliation ë¬¸ìì—´ì„ ëª¨ë‘ ëª¨ì€ë‹¤.
    (ê°™ì€ ì´ë¦„ì´ë©´ ì¤‘ë³µ ì œê±°)
    """
    from pathlib import Path
    import pandas as pd

    base = Path(final_csv_path)
    mapped_set = set()

    # workdir_tmp ì „ì²´ì—ì„œ mapped_affiliations_*.csv ìˆ˜ì§‘
    root = base.parent  # ë³´í†µ LOCAL_WORKDIR (= workdir_tmp)
    try:
        for p in root.rglob("mapped_affiliations_*.csv"):
            try:
                df = pd.read_csv(p, encoding="utf-8-sig")
            except Exception:
                continue
            if "normalized_affiliation" not in df.columns:
                continue
            for s in df["normalized_affiliation"].dropna().astype(str):
                s = s.strip().rstrip(" .,\t")
                s = " ".join(s.split())
                if s:
                    mapped_set.add(s)
    except Exception:
        pass

    return sorted(mapped_set)


def _compute_inst_missing_and_recovered_for_final(final_csv_path: str) -> tuple[int, int]:
    """
    final_csv ê¸°ì¤€ìœ¼ë¡œ,
    - inst_missing  : authorships ì•ˆì— ROR URLì´ ì „í˜€ ì—†ëŠ” ë…¼ë¬¸ ìˆ˜
    - inst_recovered: ê·¸ ì¤‘ì—ì„œ mapped_affiliationsì— ìˆëŠ” ê¸°ê´€ëª…ì´ í•˜ë‚˜ë¼ë„
                      ë“±ì¥í•˜ëŠ” ë…¼ë¬¸ ìˆ˜
    ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    import pandas as pd
    import re
    from pathlib import Path

    base = Path(final_csv_path)
    try:
        df = pd.read_csv(base, encoding="utf-8-sig")
    except Exception:
        return 0, 0

    if "authorships" not in df.columns:
        return 0, 0

    rx_ror = re.compile(r"https?://ror\.org/[0-9a-z]+", re.I)

    authorships = df["authorships"].astype(str).fillna("")
    has_ror = authorships.apply(lambda s: bool(rx_ror.search(s)))
    mask_missing = ~has_ror
    inst_missing = int(mask_missing.sum())

    # ë³´ì™„ í›„ë³´ ê¸°ê´€ëª… ë¡œë”©
    mapped_affs = _load_mapped_affiliations_for_final(final_csv_path)
    if not mapped_affs or inst_missing == 0:
        return inst_missing, 0

    # RORì´ ì—†ëŠ” ë…¼ë¬¸ë“¤ë§Œ ëŒ€ìƒìœ¼ë¡œ, ê¸°ê´€ëª…ì´ í¬í•¨ë˜ëŠ”ì§€ ê²€ì‚¬
    missing_auth = authorships[mask_missing]

    def _has_mapped_aff(s: str) -> bool:
        for aff in mapped_affs:
            if aff and aff in s:
                return True
        return False

    inst_recovered = int(missing_auth.apply(_has_mapped_aff).sum())
    return inst_missing, inst_recovered


def _update_metrics_ror_for_piece(metrics_path: Path, final_csv_path: Path) -> None:
    """
    1) final_csv_path ê¸°ì¤€ìœ¼ë¡œ ê¸°ê´€ì •ë³´ ê²°ì¸¡/ë³´ì™„ ìˆ˜ ê³„ì‚°
    2) metrics.csvë¥¼ ì½ì–´ì„œ ë‹¤ìŒì„ ë°˜ì˜:
       - ror_missing_before_extract = ìµœì´ˆ ê²°ì¸¡ ìˆ˜
       - ror_missing                = ë³´ê°• í›„ ë‚¨ì€ ê²°ì¸¡ ìˆ˜
       - ror_enriched               = ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜
       - ror_missing_after_extract  = metrics.csvì—ì„œëŠ” ì œê±°
    """
    import pandas as pd
    import csv as _csv

    try:
        inst_missing, inst_recovered = _compute_inst_missing_and_recovered_for_final(str(final_csv_path))
    except Exception as e:
        print(f"[WARN] _update_metrics_ror_for_piece: inst calc failed for {final_csv_path}: {e}")
        return

    missing_after = max(0, inst_missing - inst_recovered)
    enriched = max(0, inst_recovered)

    kv: dict[str, str] = {}

    # ê¸°ì¡´ metrics ìˆìœ¼ë©´ ì½ì–´ì„œ ë‹¤ë¥¸ ê°’ì€ ìœ ì§€
    if metrics_path.exists():
        try:
            df = pd.read_csv(metrics_path, dtype=str)

            # key/value ì •ê·œí™”
            if "key" not in df.columns or "value" not in df.columns:
                cols = list(df.columns)
                if len(cols) >= 2:
                    df = df.rename(columns={cols[0]: "key", cols[1]: "value"})
                elif len(cols) == 1:
                    df = df.reset_index().rename(columns={"index": "key", cols[0]: "value"})

            if "key" in df.columns and "value" in df.columns:
                for _, row in df.iterrows():
                    k = str(row["key"]).strip()
                    if not k:
                        continue
                    v = "" if pd.isna(row["value"]) else str(row["value"]).strip()
                    kv[k] = v
        except Exception as e:
            print(f"[WARN] _update_metrics_ror_for_piece: read failed for {metrics_path}: {e}")

    # ROR ê´€ë ¨ í‚¤ ë®ì–´ì“°ê¸°
    kv["ror_missing_before_extract"] = str(inst_missing)
    kv["ror_missing"] = str(missing_after)
    kv["ror_enriched"] = str(enriched)
    # ìš”êµ¬ì‚¬í•­: metrics.csvì—ì„œ ror_missing_after~ëŠ” ì œê±°
    kv.pop("ror_missing_after_extract", None)

    try:
        metrics_path.parent.mkdir(parents=True, exist_ok=True)
        with metrics_path.open("w", newline="", encoding="utf-8-sig") as f:
            w = _csv.writer(f)
            w.writerow(["key", "value"])
            for k, v in kv.items():
                w.writerow([k, v])
    except Exception as e:
        print(f"[WARN] _update_metrics_ror_for_piece: write failed for {metrics_path}: {e}")


def build_summary_from_metrics_for_final(final_csv_path: str, out_csv_path=None):
    import pandas as pd
    from pathlib import Path

def build_summary_from_metrics_for_final(final_csv_path: str, out_csv_path=None):
    import pandas as pd
    from pathlib import Path

    def _to_num(x):
        if x is None: return None
        s = str(x).strip()
        if s=="" or s.lower()=="nan": return None
        s = s.replace(",","")
        if s.endswith("%"): s = s[:-1]
        try:
            f = float(s); return int(f) if float(int(f))==f else f
        except Exception: return None

    totals = {
        "total_collected": 0.0, "final_csv_rows": 0.0,
        "authorships_removed": 0.0,
        "id_pattern_removed": 0.0,
        "col_mismatch_removed": 0.0,            
        "doi_missing": 0.0, "doi_enriched": 0.0,
        "ror_missing": 0.0, "ror_enriched": 0.0,
        "ror_missing_before_extract": 0.0, "ror_missing_after_extract": 0.0,
    }
    editorial = {"authorships_removed","authorships_removed_empty_list"}  # id_pattern_removed ì œì™¸

    got_any=False
    for _,_,m in _iter_metrics_dfs_from_final(final_csv_path):
        if "key" not in m.columns or "value" not in m.columns:
            cols = list(m.columns)
            if len(cols) == 1:
                m = m.reset_index().rename(columns={"index":"key", cols[0]:"value"})
            elif len(cols) >= 2:
                m = m.rename(columns={cols[0]:"key", cols[1]:"value"})
        # âœ… íŒŒì¼ ë‚´ë¶€ ì¤‘ë³µ keyëŠ” 'ë§ˆì§€ë§‰ ê°’'ë§Œ ì‚¬ìš© (appendë¡œ ì¤‘ë³µ ëˆ„ì ë˜ëŠ” ê²½ìš° ë°©ì§€)
        m = m.dropna(subset=["key"]).copy()
        m["key"] = m["key"].astype(str).str.strip()
        m = m[m["key"] != ""]
        m = m.drop_duplicates(subset=["key"], keep="last")

        got_any = True
        for _, row in m.iterrows():
            k = (row.get("key") or "").strip()
            v = _to_num(row.get("value"))
            if v is None:
                continue
            if k in editorial:
                totals["authorships_removed"] += v
            elif k == "json_rows":
                totals["total_collected"] += v
            elif k in totals:
                totals[k] += v

    def _ival(x):
        try: return int(x) if x is not None else 0
        except Exception: return 0

    if not got_any:
        df = pd.DataFrame([{
            "ìµœì´ˆ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜":0,
            "ìµœì¢… ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜":0,
            "Editorial Material ì‚­ì œ ìˆ˜":0,
            "ID íŒ¨í„´ ë¶ˆì¼ì¹˜ ì‚­ì œ ìˆ˜":0,
            "ì»¬ëŸ¼ìˆ˜ ë¶ˆì¼ì¹˜ ì‚­ì œ ìˆ˜": 0,
            "ê²€ì¦_ì°¨ì´(=0ì´ì–´ì•¼ ì •ìƒ)": 0,
            "ìµœì¢… CSV í–‰ ìˆ˜(í•©ê³„)":0,
            "DOI ê²°ì¸¡ ìˆ˜(í•©ì‚°)":0,
            "DOI ë³´ê°• ìˆ˜(í•©ì‚°)":0,
            "DOI ë³´ê°•ë¥ ":"0.00%",
            "ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)":0,
            "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)":0,
            "ROR ID ë³´ê°•ë¥ ":"0.00%",
        }])
        if out_csv_path: df.to_csv(out_csv_path, index=False, encoding="utf-8-sig")
        return df

    tot = totals.copy()

    # -------------------------------
    # ğŸ”¥ ê¸°ê´€ì •ë³´ ê¸°ì¤€ìœ¼ë¡œ ROR ì§€í‘œ ì¬ì •ì˜
    # -------------------------------
    inst_missing, inst_recovered = _compute_inst_missing_and_recovered_for_final(final_csv_path)

    # 1) before/after í•„ë“œ ì¬ì„¤ì •
    tot["ror_missing_before_extract"] = inst_missing
    tot["ror_missing_after_extract"]  = max(0, inst_missing - inst_recovered)

    # 2) summaryì—ì„œ ì‹¤ì œë¡œ ì“°ì´ëŠ” ROR ID ê²°ì¸¡/ë³´ê°• ìˆ˜ ì¬ì •ì˜
    #    - ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)   = ë³´ê°• í›„ ë‚¨ì€ ê²°ì¸¡ ìˆ˜
    #    - ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)   = ë³´ê°•ëœ ë¬¸í—Œ ìˆ˜
    #    - ë¶„ëª¨ (ROR ID ê²°ì¸¡ + ë³´ê°•) = ìµœì´ˆ ê²°ì¸¡ ìˆ˜(inst_missing)
    tot["ror_missing"]  = max(0, inst_missing - inst_recovered)  # after ê¸°ì¤€
    tot["ror_enriched"] = max(0, inst_recovered)

    doi_denom = _ival(tot["doi_missing"]) + _ival(tot["doi_enriched"])
    ror_denom = _ival(tot["ror_missing"]) + _ival(tot["ror_enriched"])

    doi_rate = f"{(100.0*_ival(tot['doi_enriched'])/doi_denom):.2f}%" if doi_denom else "0.00%"
    ror_rate = f"{(100.0*_ival(tot['ror_enriched'])/ror_denom):.2f}%" if ror_denom else "0.00%"

    # ìµœì´ˆ/ìµœì¢… ë¶„ë¦¬: json_rows ìš°ì„ , ì—†ìœ¼ë©´ total_collected í´ë°±
    _first_rows = _ival(tot.get("json_rows", None))
    if _first_rows is None:
        _first_rows = _ival(tot.get("total_collected", 0))

    _start = _ival(tot.get("json_rows", tot["total_collected"]))
    _final = _ival(tot["final_csv_rows"])
    _ed = _ival(tot["authorships_removed"])
    _id = _ival(tot["id_pattern_removed"])
    _col = _ival(tot["col_mismatch_removed"])
    _check = _start - _ed - _id - _col - _final

    df = pd.DataFrame([{
        "ìµœì´ˆ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": _start,
        "ìµœì¢… ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": _final,
        "Editorial Material ì‚­ì œ ìˆ˜": _ed,
        "ID íŒ¨í„´ ë¶ˆì¼ì¹˜ ì‚­ì œ ìˆ˜": _id,
        "ì»¬ëŸ¼ìˆ˜ ë¶ˆì¼ì¹˜ ì‚­ì œ ìˆ˜": _col,
        "ê²€ì¦_ì°¨ì´(=0ì´ì–´ì•¼ ì •ìƒ)": _check,
        "ìµœì¢… CSV í–‰ ìˆ˜(í•©ê³„)": _final,
        "DOI ê²°ì¸¡ ìˆ˜(í•©ì‚°)": _ival(tot["doi_missing"]),
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": _ival(tot["doi_enriched"]),
        "DOI ë³´ê°•ë¥ ": doi_rate,
        "ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)": _ival(tot["ror_missing"]),
        "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": _ival(tot["ror_enriched"]),
        "ROR ID ë³´ê°•ë¥ ": ror_rate,
    }])
    if out_csv_path:
        Path(out_csv_path).parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(out_csv_path, index=False, encoding="utf-8-sig")
    return df


def run_pipeline(issns: List[str], year_start: int, year_end: int,
                 email: str = 's0124kw@gmail.com', include_only_with_abstract: bool = False,
                 make_html: bool = False):

    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
    issns: ì €ë„ ISSN ë¦¬ìŠ¤íŠ¸
    year_start, year_end: ì—°ë„ ë²”ìœ„
    email: OpenAlex APIìš© ì´ë©”ì¼
    """

    """
    íŒŒì¼ëª… prefix ê³„ì‚°
    """
    config.email = email
    journal_prefixes = []
    for issn in issns:
        try:
            src = next(
                Sources().filter(issn=[issn])
                         .select(['display_name'])
                         .paginate(per_page=1)
            )
            display = src[0]['display_name'] if isinstance(src, list) else src['display_name']
        except Exception:
            display = issn  # fallback: ISSN ê·¸ëŒ€ë¡œ ì‚¬ìš©
        safe = re.sub(r'\W+', '_', display).strip('_')
        journal_prefixes.append(safe)
    prefix = "-".join(journal_prefixes)


    # 1ï¸âƒ£ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë° DOI ë³´ê°• (real1)
    json_base = f"{prefix}_{year_start}_{year_end}"
    json_parts = [f"{json_base}_part{i+1}.json" for i in range(3)]
    json_merged = f"{json_base}.json"
    csv_file = f"{json_base}.csv"
    csv_ror = f"{json_base}_ror.csv"
    csv_ror_extract = f"{json_base}_ror_extract.csv"
    csv_ror_extract_name = f"{json_base}_ror_extract_name.csv"
    html_ror_extract_name_network = f"{json_base}_ror_extract_name_network.html"
    cache_file = Path("ror_cache.pkl")

    # 1. real1: ë…¼ë¬¸ ìˆ˜ì§‘ ë° ë³´ê°•
    print("[1/7] ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë° DOI ë³´ê°•...")
    real1.main(issns=issns, year_start=year_start, year_end=year_end, email=email, prefix=prefix, include_only_with_abstract=include_only_with_abstract)

    # 2. real2: JSON â†’ CSV ë³€í™˜
    print("[2/7] JSON â†’ CSV ë³€í™˜...")
    real2.main(input_json=json_merged, output_csv=csv_file, prefix=prefix, year_start=year_start, year_end=year_end)

    # 3. real3: ROR ë§¤í•‘
    print("[3/7] ROR ë§¤í•‘...")
    asyncio.run(real3.process(
        input_csv=Path(csv_file),
        output_csv=Path(csv_ror),
        cache_file=cache_file,
        concurrency=20
    ))

    # 4. real4: ROR ì¶”ì¶œ
    print("[4/7] ROR ì¶”ì¶œ ë° í†µê³„...")
    real4.main(input_csv=csv_ror, output_csv=csv_ror_extract, prefix=prefix, year_start=year_start, year_end=year_end)

    # 5. real5: ROR ID â†’ ê¸°ê´€ëª… ë§¤í•‘
    print("[5/7] ROR ID â†’ ê¸°ê´€ëª… ë§¤í•‘...")
    real5.main(input_csv=csv_ror_extract, output_csv=csv_ror_extract_name, prefix=prefix, year_start=year_start, year_end=year_end)

    # 6. real6: í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (HTML ìƒì„±)
    html_path = None
    if make_html:
        print("[6/7] í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”...")
        html_path = real6.main(
            input_csv=csv_ror_extract_name,
            output_html=html_ror_extract_name_network,
            size_by="eigenvector",
            color_by="eigenvector"
        )
        if html_path:
            print(f"â†’ ì‹œê°í™” íŒŒì¼ ìƒì„±: {html_path}")


    print("[7/7] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

    return csv_ror_extract_name, html_path


def make_html_from_csv(final_csv_path: str) -> str:
    """
    ìµœì¢… CSVë¡œ ì‹œê°í™”ë¥¼ 2ê°€ì§€ ë²„ì „(degree / eigenvector)ìœ¼ë¡œ ìƒì„±í•œ ë’¤,
    í•˜ë‚˜ì˜ HTMLì— ë‘ ë·°ë¥¼ iframeìœ¼ë¡œ ë‚˜ë€íˆ ë‹´ì•„ ë°˜í™˜(í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜/ë¦¬í„´íƒ€ì… ë³€ê²½ ì—†ìŒ).
    """
    if not final_csv_path.endswith("_ror_extract_name.csv"):
        raise ValueError("final_csv_path must end with _ror_extract_name.csv")

    base = final_csv_path[:-len("_ror_extract_name.csv")]

    # 1) ê°œë³„ ì‹œê°í™” HTML ìƒì„±(ê°™ì€ í´ë”ì— ìœ ì§€)
    html_degree = f"{base}_degree_network.html"
    html_eigen  = f"{base}_eigenvector_network.html"

    # ê¶Œì¥ ì¡°í•©: í¬ê¸°=degree / ìƒ‰=eigenvector  &  í¬ê¸°=eigenvector / ìƒ‰=degree
    real6.main(
        input_csv=final_csv_path,
        output_html=html_degree,
        size_by="degree",
        color_by="eigenvector"
    )
    real6.main(
        input_csv=final_csv_path,
        output_html=html_eigen,
        size_by="eigenvector",
        color_by="degree"
    )

    # 2) ê°™ì€ í´ë”ì˜ ë‘ HTMLì„ iframeìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë˜í¼ ìƒì„±
    deg_name = Path(html_degree).name
    eig_name = Path(html_eigen).name

    output_html = final_csv_path.replace(
        "_ror_extract_name.csv",
        "_ror_extract_name_network.html"
    )

    wrapper = f"""<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Collaboration Networks (Degree & Eigenvector)</title>
  <style>
    body {{ background:#111; color:#eaeaea; font-family:system-ui, sans-serif; }}
    .row {{ display:flex; gap:12px; flex-wrap:wrap; padding:12px; }}
    .panel {{ flex:1 1 48%; min-width:380px; border:1px solid #333; border-radius:8px; background:#151515; }}
    .panel h3 {{ margin:8px 12px; font-weight:600; }}
    iframe {{ width:100%; height:820px; border:0; background:#111; border-top:1px solid #333; }}
  </style>
</head>
<body>
  <div class="row">
    <div class="panel">
      <h3>Degree-based view (size=degree, color=eigenvector)</h3>
      <iframe src="{deg_name}"></iframe>
    </div>
    <div class="panel">
      <h3>Eigenvector-based view (size=eigenvector, color=degree)</h3>
      <iframe src="{eig_name}"></iframe>
    </div>
  </div>
</body>
</html>
"""
    Path(output_html).write_text(wrapper, encoding="utf-8")
    return output_html

def make_html_string_from_csv(final_csv_path: str, size_by: str, color_by: str) -> str:
    """
    ìµœì¢… CSVì—ì„œ ë„¤íŠ¸ì›Œí¬ HTML ë¬¸ìì—´ ìƒì„±(íŒŒì¼ ì €ì¥ ì—†ìŒ).
    - size_by: "degree" ë˜ëŠ” "eigenvector"
    - color_by: "eigenvector" ë˜ëŠ” "degree"
    """
    res = real6.main(
        input_csv=final_csv_path,
        output_html=None,      # â† íŒŒì¼ ì €ì¥ ì•ˆ í•¨
        size_by=size_by,
        color_by=color_by
    )

    # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ ê²½ë¡œ ë¬¸ìì—´ì´ ëŒì•„ì˜¤ë©´ íŒŒì¼ì„ ì½ì–´ì„œ HTML ë¬¸ìì—´ë¡œ ì¹˜í™˜
    from pathlib import Path as _Path
    if isinstance(res, str):
        s = res.strip().lower()
        if s.startswith("<!doctype") or s.startswith("<html"):
            return res
        if res.endswith(".html") and _Path(res).exists():
            return _Path(res).read_text(encoding="utf-8")
    raise RuntimeError("HTML ìƒì„± ì‹¤íŒ¨: ë°˜í™˜ê°’ì´ ë¹„ì—ˆê±°ë‚˜ HTMLì´ ì•„ë‹™ë‹ˆë‹¤.")


# ì €ì¥ ì „ëµ ì„¤ì •
USE_JOURNAL_NAME_SLUG: bool = True
LOCAL_WORKDIR = Path("workdir_tmp")
LOCAL_WORKDIR.mkdir(parents=True, exist_ok=True)

_slug_rx = re.compile(r"[^a-zA-Z0-9]+")
def _to_slug(name: str) -> str:
    return _slug_rx.sub("_", name.strip().lower()).strip("_")

_JSLUG_CACHE: dict[str, str] = {}
def _resolve_journal_name_slug(issn: str) -> str:
    """ISSN -> OpenAlex display_name -> íŒŒì¼ì‹œìŠ¤í…œ ì•ˆì „ ìŠ¬ëŸ¬ê·¸"""
    if issn in _JSLUG_CACHE:
        return _JSLUG_CACHE[issn]
    try:
        src = next(
            Sources().filter(issn=[issn])
                    .select(['display_name'])
                    .paginate(per_page=1)
        )
        display = src[0]['display_name'] if isinstance(src, list) else src['display_name']
        base_name = display
    except Exception:
        base_name = issn
    slug = _to_slug(base_name)
    _JSLUG_CACHE[issn] = slug
    return slug

def _key_name_for(issn: str) -> str:
    """íŒŒì¼ëª…/í´ë”ëª…ì— ì“¸ í‚¤(ì €ë„ëª… ìŠ¬ëŸ¬ê·¸ ë˜ëŠ” ISSN)"""
    # âœ… ëŒ€í‘œí‚¤ê°€ ì´ë¯¸ ì •í•´ì¡Œë‹¤ë©´ ë¬´ì¡°ê±´ ê·¸ê²ƒë§Œ ì‚¬ìš©
    if 'FORCE_JOURNAL_KEY' in globals() and FORCE_JOURNAL_KEY:
        return FORCE_JOURNAL_KEY
    return _resolve_journal_name_slug(issn) if USE_JOURNAL_NAME_SLUG else issn

# ----- Secrets/ENVì—ì„œ Drive ì¸ì¦/í´ë” ì •ë³´ ì½ê¸° -----

def _get_gdrive_root_folder_id() -> Optional[str]:
    """
    Streamlit Cloud Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ë£¨íŠ¸ í´ë” ID ì½ê¸°
    - secrets: GDRIVE_FOLDER_ID
    - env    : GDRIVE_FOLDER_ID
    """
    if st is not None:
        try:
            return st.secrets["GDRIVE_FOLDER_ID"]
        except Exception:
            pass
    return os.getenv("GDRIVE_FOLDER_ID")


def _get_service_account_info() -> Optional[dict]:
    """
    âœ… ì˜¤ì§ Streamlit secretsì˜ [gcp_service_account]ë§Œ ì§€ì›
       (í•„ìš” ì‹œ ENV GDRIVE_SA_JSON JSON ë¬¸ìì—´ì€ ë³´ì¡°ìš©)
    - ë¬¸ìì—´/ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ í—ˆìš©
    - private_key ì˜ '\\n' â†’ ì‹¤ì œ ê°œí–‰ìœ¼ë¡œ ì¹˜í™˜
    """
    raw = None
    # 1) Streamlit secrets
    if st is not None:
        try:
            raw = st.secrets["gcp_service_account"]
        except Exception:
            print("[GDRIVE] gcp_service_account not found in st.secrets")
            raw = None
    # 2) (ì„ íƒ) í™˜ê²½ë³€ìˆ˜ JSON ë³´ì¡°
    if raw is None:
        raw = os.getenv("GDRIVE_SA_JSON")
        if raw is None:
            print("[GDRIVE] GDRIVE_SA_JSON not found in env")
    if raw is None:
        return None

    # ë”•ì…”ë„ˆë¦¬ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if isinstance(raw, Mapping):
        sa = dict(raw)
    else:
        s = str(raw).strip()
        # JSON ë¨¼ì € ì‹œë„
        try:
            sa = _json.loads(s)
        except Exception:
            # ì¼ë¶€ í™˜ê²½ì—ì„œ ë‹¨ì¼ë”°ì˜´í‘œ dict ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš°
            try:
                sa = _ast.literal_eval(s)
                if not isinstance(sa, Mapping):
                    raise ValueError("literal_eval did not return a mapping")
                sa = dict(sa)
            except Exception as e:
                print("[GDRIVE] Could not parse gcp_service_account string:", repr(e))
                print("[GDRIVE] First 120 chars:", s[:120])
                return None

    # private_key ê°œí–‰ ì •ê·œí™”
    pk = sa.get("private_key")
    if not pk:
        print("[GDRIVE] private_key missing in gcp_service_account")
        return None
    # private_keyê°€ í™˜ê²½/Secretsì— '\\n' í˜•íƒœë¡œ ë“¤ì–´ì˜¬ ë•Œ ì‹¤ì œ ê°œí–‰ìœ¼ë¡œ ë³€í™˜
    pk = sa.get("private_key")
    if pk:
        try:
            sa["private_key"] = str(pk).replace("\\n", "\n")
        except Exception:
            sa["private_key"] = pk
    return sa


def _gdrive_client():
    # âœ… oauth2client ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
    from oauth2client.service_account import ServiceAccountCredentials
    from pydrive2.auth import GoogleAuth
    from pydrive2.drive import GoogleDrive

    sa_info = _get_service_account_info()
    if not sa_info:
        raise RuntimeError("Google Drive ì„œë¹„ìŠ¤ê³„ì • ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. secrets ë˜ëŠ” ENVë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    scopes = ['https://www.googleapis.com/auth/drive']
    # í•µì‹¬: google-auth ëŒ€ì‹  oauth2clientë¡œ í¬ë¦¬ë´ì…œ ìƒì„±
    creds = ServiceAccountCredentials.from_json_keyfile_dict(sa_info, scopes=scopes)

    gauth = GoogleAuth()
    gauth.credentials = creds
    gauth.Authorize()  # â† oauth2client í¬ë¦¬ë´ì…œì€ authorize() ì§€ì›
    drive = GoogleDrive(gauth)
    return drive


def _find_folder(drive: GoogleDrive, parent_id: str, name: str) -> Optional[str]:
    q = f"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'"
    lst = drive.ListFile({'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True}).GetList()
    return lst[0]['id'] if lst else None

def _ensure_folder(drive: GoogleDrive, parent_id: str, name: str) -> str:
    fid = _find_folder(drive, parent_id, name)
    if fid:
        return fid
    f = drive.CreateFile({'title': name, 'parents':[{'id': parent_id}],
                          'mimeType': 'application/vnd.google-apps.folder'})
    f.Upload()
    return f['id']

def _find_file(drive: GoogleDrive, parent_id: str, name: str) -> Optional[str]:
    q = f"'{parent_id}' in parents and trashed=false and mimeType!='application/vnd.google-apps.folder' and title='{name}'"
    lst = drive.ListFile({'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True}).GetList()
    return lst[0]['id'] if lst else None

def _download_file(drive: GoogleDrive, file_id: str, local_path: Path):
    f = drive.CreateFile({'id': file_id})
    local_path.parent.mkdir(parents=True, exist_ok=True)
    f.GetContentFile(str(local_path))

def _upload_file(drive: GoogleDrive, parent_id: str, local_path: Path, name: Optional[str] = None):
    """ë™ì¼ ì´ë¦„ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°(ì—…ì„œíŠ¸)"""
    name = name or local_path.name
    q = (
        f"'{parent_id}' in parents and trashed=false "
        f"and mimeType!='application/vnd.google-apps.folder' and title='{name}'"
    )
    exist = drive.ListFile({
        'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True
    }).GetList()

    if exist:
        f = drive.CreateFile({'id': exist[0]['id']})
        f.SetContentFile(str(local_path))
        f.Upload(param={'supportsAllDrives': True})
        return f['id']

    f = drive.CreateFile({'title': name, 'parents':[{'id': parent_id}]})
    f.SetContentFile(str(local_path))
    f.Upload(param={'supportsAllDrives': True})
    return f['id']


def _gdrive_locate_metric_by_key(key: str, year: int):
    """
    ROOT / <key> / <year> / <key>_<year>_<year>_metrics.csv ë¥¼ ì°¾ì•„ file_idë¥¼ ë°˜í™˜.
    (ISSN ì—†ì´ key(slug)ë§Œ ê°€ì§€ê³  ì°¾ê¸°)
    """
    root_id = _get_gdrive_root_folder_id()
    if not root_id:
        raise RuntimeError("GDRIVE_FOLDER_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    drive = _gdrive_client()

    # í´ë” ì°¾ì•„ê°€ê¸°
    key_folder = _ensure_folder(drive, root_id, key)
    year_folder = _ensure_folder(drive, key_folder, str(year))

    metrics_name = f"{key}_{year}_{year}_metrics.csv"
    file_id = _find_file(drive, year_folder, metrics_name)
    return drive, year_folder, metrics_name, file_id


# ----- issn/year ì›ê²© íŒŒì¼ ìœ„ì¹˜ -----
def _gdrive_locate_piece(issn: str, year: int):
    """
    ROOT / <key> / <year> / <key>_<year>_ror_extract_name.csv
    """
    root_id = _get_gdrive_root_folder_id()
    if not root_id:
        raise RuntimeError("GDRIVE_FOLDER_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    key = _key_name_for(issn)
    drive = _gdrive_client()
    key_folder = _ensure_folder(drive, root_id, key)
    year_folder = _ensure_folder(drive, key_folder, str(year))
    fname = f"{key}_{year}_ror_extract_name.csv"
    file_id = _find_file(drive, year_folder, fname)
    return drive, key, key_folder, year_folder, fname, file_id


def _gdrive_download_metric_by_key_exact_path(key: str, year: int, local_path: Path) -> bool:
    """
    ì˜ˆìƒ ê²½ë¡œ(í‚¤/ì—°ë„ í´ë”)ì—ì„œ metrics íŒŒì¼ì„ ì°¾ì•„ ë¡œì»¬ë¡œ ì €ì¥.
    ê¸°ë³¸: ROOT/{key}/{year}/
    Fallback: ROOT/storage/{key}/{year}/   (ê³µìœ ë“œë¼ì´ë¸Œ êµ¬ì¡° ì§€ì›)
    """
    try:
        root_id = _get_gdrive_root_folder_id()
        if not root_id:
            return False
        drive = _gdrive_client()

        def _locate_year_folder():
            # 1) ROOT/{key}/{year}
            key_id = _find_folder(drive, root_id, key)
            if key_id:
                yid = _find_folder(drive, key_id, str(year))
                if yid:
                    return yid
            # 2) ROOT/storage/{key}/{year}
            storage_id = _find_folder(drive, root_id, "storage")
            if storage_id:
                key2 = _find_folder(drive, storage_id, key)
                if key2:
                    yid2 = _find_folder(drive, key2, str(year))
                    if yid2:
                        return yid2
            return None

        year_folder_id = _locate_year_folder()
        if not year_folder_id:
            return False

        metrics_name = f"{key}_{year}_{year}_metrics.csv"
        file_id = _find_file(drive, year_folder_id, metrics_name)
        if not file_id:
            return False

        _download_file(drive, file_id, local_path)
        return local_path.exists()
    except Exception as e:
        print(f"[WARN] _gdrive_download_metric_by_key_exact_path: {key=} {year=} {e}")
        return False

def _gdrive_download_metric_by_name_search(key: str, year: int, local_path: Path) -> bool:
    """
    Google Drive ì „ì²´ì—ì„œ íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰(íŒ€ë“œë¼ì´ë¸Œ í¬í•¨).
    1) ì •í™• ì¼ì¹˜(title = ...)
    2) ì‹¤íŒ¨ ì‹œ ë¶€ë¶„ ì¼ì¹˜(title contains ...)ë¡œ ì™„í™”
    """
    try:
        root_id = _get_gdrive_root_folder_id()
        if not root_id:
            return False
        drive = _gdrive_client()

        metrics_name = f"{key}_{year}_{year}_metrics.csv"

        # 1) ì •í™• ì¼ì¹˜
        q_eq = (
            f"title = '{metrics_name}' and trashed = false "
            f"and mimeType != 'application/vnd.google-apps.folder'"
        )
        lst = drive.ListFile({
            'q': q_eq,
            'includeItemsFromAllDrives': True,
            'supportsAllDrives': True
        }).GetList()

        # 2) ë¶€ë¶„ ì¼ì¹˜(ì •í™• ì¼ì¹˜ê°€ ì—†ì„ ë•Œë§Œ)
        if not lst:
            # ì˜ˆ: key ì¼ë¶€ + ì—°ë„ + 'metrics.csv'ë§Œ ë§ì•„ë„ íšŒìˆ˜
            #    title contains 'journal_of_hydraulic_engineering_2024' and title contains 'metrics.csv'
            q_ct = (
                f"title contains '{key}_{year}_{year}' and "
                f"title contains 'metrics.csv' and "
                f"trashed = false and mimeType != 'application/vnd.google-apps.folder'"
            )
            lst = drive.ListFile({
                'q': q_ct,
                'includeItemsFromAllDrives': True,
                'supportsAllDrives': True
            }).GetList()

        if not lst:
            return False

        file_id = lst[0]['id']  # ì²« ì¼ì¹˜ í•­ëª© ì‚¬ìš©
        _download_file(drive, file_id, local_path)
        return local_path.exists()
    except Exception as e:
        print(f"[WARN] _gdrive_download_metric_by_name_search: {key=} {year=} {e}")
        return False



from io import StringIO

def _gdrive_read_metric_csv_by_key_exact_path_to_df(key: str, year: int):
    """
    ì •í™• ê²½ë¡œì—ì„œ metricsë¥¼ ë©”ëª¨ë¦¬ë¡œ ì½ê¸°
      ê¸°ë³¸: ROOT/{key}/{year}/
      Fallback: ROOT/storage/{key}/{year}/
    """
    import pandas as pd
    from io import StringIO

    try:
        root_id = _get_gdrive_root_folder_id()
        if not root_id:
            return None
        drive = _gdrive_client()

        def _locate_year_folder():
            # 1) ROOT/{key}/{year}
            key_id = _find_folder(drive, root_id, key)
            if key_id:
                yid = _find_folder(drive, key_id, str(year))
                if yid:
                    return yid
            # 2) ROOT/storage/{key}/{year}
            storage_id = _find_folder(drive, root_id, "storage")
            if storage_id:
                key2 = _find_folder(drive, storage_id, key)
                if key2:
                    yid2 = _find_folder(drive, key2, str(year))
                    if yid2:
                        return yid2
            return None

        year_folder_id = _locate_year_folder()
        if not year_folder_id:
            return None

        metrics_name = f"{key}_{year}_{year}_metrics.csv"
        file_id = _find_file(drive, year_folder_id, metrics_name)
        if not file_id:
            return None

        f = drive.CreateFile({'id': file_id})
        csv_text = f.GetContentString(mimetype='text/csv')
        return pd.read_csv(StringIO(csv_text), dtype=str)
    except Exception as e:
        print(f"[WARN] _gdrive_read_metric_csv_by_key_exact_path_to_df: {key=} {year=} {e}")
        return None

def _gdrive_read_metric_csv_by_name_search_to_df(key: str, year: int):
    """Drive ì „ì²´ì—ì„œ íŒŒì¼ëª…ìœ¼ë¡œ ì°¾ì•„ DFë¡œ ì½ê¸° (ë¶€ë¶„ ì¼ì¹˜ê¹Œì§€ í—ˆìš©)"""
    import pandas as pd
    try:
        root_id = _get_gdrive_root_folder_id()
        if not root_id:
            return None
        drive = _gdrive_client()
        metrics_name = f"{key}_{year}_{year}_metrics.csv"

        # 1) ì •í™• ì¼ì¹˜
        q = f"title = '{metrics_name}' and trashed = false and mimeType != 'application/vnd.google-apps.folder'"
        lst = drive.ListFile({'q': q, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True}).GetList()

        # 2) ë¶€ë¶„ ì¼ì¹˜ (ì •í™• ì¼ì¹˜ ì—†ì„ ë•Œë§Œ)
        if not lst:
            q2 = (
                f"title contains '{key}_{year}_{year}' and title contains 'metrics.csv' "
                f"and trashed = false and mimeType != 'application/vnd.google-apps.folder'"
            )
            lst = drive.ListFile({'q': q2, 'includeItemsFromAllDrives': True, 'supportsAllDrives': True}).GetList()

        if not lst:
            return None
        f = drive.CreateFile({'id': lst[0]['id']})
        csv_text = f.GetContentString(mimetype='text/csv')
        return pd.read_csv(StringIO(csv_text), dtype=str)
    except Exception as e:
        print(f"[WARN] _gdrive_read_metric_csv_by_name_search_to_df: {key=} {year=} {e}")
        return None

def _iter_metrics_dfs_from_final(final_csv_path: str):
    """ìµœì¢… CSV íŒŒì¼ëª…ì—ì„œ (keys, years) ë³µì› â†’ ê° (key, year)ì˜ metrics DFë¥¼ ë©”ëª¨ë¦¬ë¡œ yield"""
    from pathlib import Path as _Path
    import re as _re

    base_dir = _Path(final_csv_path).parent
    keys, y1, y2 = _parse_keys_years_from_final(final_csv_path)

    # í‚¤ ì •ê·œí™”(ë“œë¼ì´ë¸Œ í´ë”/íŒŒì¼ ëª…ê³¼ ì–´ê¸‹ë‚˜ì§€ ì•Šê²Œ ë‹¨ì¼ ìŠ¬ëŸ¬ê·¸ ê·œì¹™)
    def _norm_key(k: str) -> str:
        s = (k or "").strip().lower()
        s = s.replace("-", "_")
        s = _re.sub(r"\s+", "_", s)
        s = _re.sub(r"_+", "_", s)
        return s
    keys = [_norm_key(k) for k in keys]

    for key in keys:
        for y in range(y1, y2 + 1):
            # 1) ë¡œì»¬(ìˆìœ¼ë©´ ë°”ë¡œ ì½ê¸°: ì €ì¥ ìš©ëŸ‰ ì¦ê°€ ì—†ìŒ)
            nested = base_dir / key / str(y) / f"{key}_{y}_{y}_metrics.csv"
            flat   = base_dir / f"{key}_{y}_{y}_metrics.csv"
            df = None
            for cand in (nested, flat):
                if cand.exists():
                    try:
                        tmp = pd.read_csv(cand, dtype=str)

                        # --- í—¤ë”/ì¸ë±ìŠ¤ ì •ê·œí™” ---
                        # 1) 'key','value'ê°€ ì—†ë‹¤ë©´, ì²« ë‘ ì»¬ëŸ¼ì„ key/valueë¡œ ê°„ì£¼
                        if not {"key","value"}.issubset(set(tmp.columns)):
                            cols = list(tmp.columns)
                            # (a) ë‘ ì»¬ëŸ¼ ì´ìƒ â†’ ì•ì˜ ë‘ ê°œë§Œ ì‚¬ìš©
                            if len(cols) >= 2:
                                tmp = tmp.rename(columns={cols[0]: "key", cols[1]: "value"})
                            # (b) í•œ ì»¬ëŸ¼ + ì¸ë±ìŠ¤ì— í‚¤ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ â†’ ì¸ë±ìŠ¤ë¥¼ keyë¡œ ìŠ¹ê²©
                            elif len(cols) == 1:
                                only = cols[0]
                                # ì¸ë±ìŠ¤ê°€ ì˜ë¯¸ ìˆê³ , ë‹¨ì¼ ê°’ ì»¬ëŸ¼ì´ valueì¼ ê°€ëŠ¥ì„±
                                tmp = tmp.reset_index().rename(columns={"index":"key", only:"value"})
                            else:
                                tmp = None

                        # 2) 'key','value'ê°€ ìƒê²¼ë‹¤ë©´ í•„ìš”í•œ ë‘ ì»¬ëŸ¼ë§Œ ìœ ì§€
                        if tmp is not None and {"key","value"}.issubset(set(tmp.columns)):
                            df = tmp[["key","value"]].copy()
                            break
                        else:
                            # ê·¸ë˜ë„ ì•ˆ ë§ìœ¼ë©´ ìŠ¤í‚µ
                            df = None

                    except Exception as e:
                        print(f"[WARN] local metrics read failed: {cand} {e}")

            # 2) Drive ë©”ëª¨ë¦¬ ì½ê¸° (ì •í™• ê²½ë¡œ â†’ ì „ì—­ ê²€ìƒ‰)
            if df is None:
                df = _gdrive_read_metric_csv_by_key_exact_path_to_df(key, y)
            if df is None:
                df = _gdrive_read_metric_csv_by_name_search_to_df(key, y)

            if df is not None and not df.empty and "key" in df.columns and "value" in df.columns:
                yield key, y, df
            else:
                print(f"[WARN] metrics not found for key={key}, year={y}")


def _gdrive_piece_exists(issn: str, year: int) -> bool:
    try:
        _, _, _, _, _, file_id = _gdrive_locate_piece(issn, year)
        return file_id is not None
    except Exception:
        return False

def _gdrive_download_piece(issn: str, year: int, local_path: Path) -> bool:
    try:
        drive, key, _, year_folder, _, file_id = _gdrive_locate_piece(issn, year)
        if not file_id:
            return False

        # 1) ìµœì¢… CSV ë‚´ë ¤ë°›ê¸°
        _download_file(drive, file_id, local_path)

        # 2) ë³´ì¡° íŒŒì¼ë“¤ ì°¾ê¸° ìœ„í•œ í—¬í¼
        def _find_file(drive, parent_id, name):
            for f in drive.ListFile({'q': f"'{parent_id}' in parents and trashed=false"}).GetList():
                if f['title'] == name:
                    return f['id']
            return None

        # 2-1) metrics.csv
        metrics_name = f"{key}_{year}_{year}_metrics.csv"
        m_id = _find_file(drive, year_folder, metrics_name)
        if m_id:
            _download_file(drive, m_id, local_path.parent / metrics_name)

        # 2-2) run_log_YYYY.txt
        log_name = f"run_log_{year}.txt"
        log_id = _find_file(drive, year_folder, log_name)
        if log_id:
            _download_file(drive, log_id, local_path.parent / log_name)

        # 2-3) mapped_affiliations_YYYY.csv
        mapped_name = f"mapped_affiliations_{year}.csv"
        map_id = _find_file(drive, year_folder, mapped_name)
        if map_id:
            _download_file(drive, map_id, local_path.parent / mapped_name)

        return True
    except Exception:
        return False

def _gdrive_upload_piece(issn: str, year: int, local_path: Path):
    drive, key, _, year_folder, fname, _ = _gdrive_locate_piece(issn, year)

    # 1) ìµœì¢… CSV ì—…ë¡œë“œ
    _upload_file(drive, year_folder, local_path, fname)

    # 2) metrics.csvê°€ ë¡œì»¬ ì—°ë„ í´ë”ì— ìˆìœ¼ë©´ í•¨ê»˜ ì—…ë¡œë“œ
    metrics_csv = local_path.parent / f"{key}_{year}_{year}_metrics.csv"
    if metrics_csv.exists():
        _upload_file(drive, year_folder, metrics_csv, metrics_csv.name)

    # 3) run_log_YYYY.txt ì—…ë¡œë“œ
    log_file = local_path.parent / f"run_log_{year}.txt"
    if log_file.exists():
        _upload_file(drive, year_folder, log_file, log_file.name)

    # 4) mapped_affiliations_YYYY.csv ì—…ë¡œë“œ
    mapped_csv = local_path.parent / f"mapped_affiliations_{year}.csv"
    if mapped_csv.exists():
        _upload_file(drive, year_folder, mapped_csv, mapped_csv.name)


# ======================================================================
# ì—°Â·ì €ë„ ë‹¨ìœ„ ì €ì¥/ì¬ì‚¬ìš© (Drive ì‚¬ìš©)
# ======================================================================

# [# ADDED] ISSN ì •ê·œí™”
_issn_rx = re.compile(r"^\d{4}-\d{3}[\dxX]$")

def _normalize_issn_list(issns: List[str]) -> List[str]:
    norm = []
    for s in issns:
        if not s:
            continue
        s = s.strip()
        if "-" not in s and len(s) == 8:
            s = s[:4] + "-" + s[4:]
        if _issn_rx.match(s):
            norm.append(s.upper())
        else:
            print(f"[WARN] ì˜ëª»ëœ ISSN í˜•ì‹ ê±´ë„ˆëœ€: {s!r}")
    # ì…ë ¥ ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
    seen, out = set(), []
    for t in norm:
        if t not in seen:
            out.append(t); seen.add(t)
    return out

# [# ADDED] ë¡œì»¬ ì„ì‹œ(ì¡°ê° ë‹¤ìš´ë¡œë“œ/ìƒì„± ìœ„ì¹˜)
def _local_piece_path(issn: str, year: int) -> Path:
    key = _key_name_for(issn)
    return LOCAL_WORKDIR / key / str(year) / f"{key}_{year}_ror_extract_name.csv"


# --- NEW: run_log â†’ mapped_affiliations ìƒì„±ìš© í—¬í¼ë“¤ ---

def _normalize_aff(s: str) -> str:
    """
    count.pyì—ì„œ ì“°ë˜ normalize ê·œì¹™ê³¼ ë™ì¼í•˜ê²Œ ë§ì¶¤:
    - ì•ë’¤ ê³µë°± ì œê±°
    - ëì˜ ì /ì‰¼í‘œ/íƒ­ ì œê±°
    - ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
    """
    s = s.strip()
    s = s.rstrip(" .,\t")
    s = " ".join(s.split())
    return s


def _build_mapped_affiliations_from_log(log_path: Path, out_csv: Path) -> None:
    """
    run_log_YYYY.txt ì•ˆì˜ [ROR_RESULT_QUERY] ë¡œê·¸ë¥¼ ì½ì–´
    ë§¤í•‘ ì„±ê³µí•œ ì¿¼ë¦¬ ë¬¸ìì—´ì„ normalize í›„
    normalized_affiliation ë‹¨ì¼ ì»¬ëŸ¼ CSVë¡œ ì €ì¥.
    """
    import re, csv

    if not log_path.exists():
        print(f"[_build_mapped_affiliations_from_log] ë¡œê·¸ íŒŒì¼ ì—†ìŒ: {log_path}")
        return

    try:
        lines = log_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    except Exception as e:
        print(f"[_build_mapped_affiliations_from_log] ë¡œê·¸ ì½ê¸° ì‹¤íŒ¨: {e}")
        return

    # count.pyì—ì„œ ì“°ë˜ íŒ¨í„´ê³¼ ë™ì¼
    ror_pattern = re.compile(
        r'\[ROR_RESULT_QUERY\]\s*([\'"])(.*?)\1\s*(?:â†’|->)\s*([\'"])(.*?)\3'
    )
    # group 2 â†’ query_str, group 4 â†’ ror_id

    success_norms = set()
    for line in lines:
        m = ror_pattern.search(line)
        if not m:
            continue
        query_str = m.group(2)
        ror_id = m.group(4)
        if not ror_id:
            continue  # ë¹ˆ ë”°ì˜´í‘œ = ë§¤í•‘ ì‹¤íŒ¨
        norm = _normalize_aff(query_str)
        if norm:
            success_norms.add(norm)

    if not success_norms:
        print(f"[_build_mapped_affiliations_from_log] ë§¤í•‘ ì„±ê³µ ì¿¼ë¦¬ê°€ ì—†ì–´ CSV ìƒëµ: {log_path}")
        return

    out_csv.parent.mkdir(parents=True, exist_ok=True)
    with out_csv.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerow(["normalized_affiliation"])
        for val in sorted(success_norms):
            w.writerow([val])

    print(f"[_build_mapped_affiliations_from_log] {len(success_norms)}ê°œ ê¸°ê´€ëª…ì„ {out_csv.name}ì— ê¸°ë¡")


# [# ADDED] ì—°Â·ì €ë„ 1ê±´ ì²˜ë¦¬ (ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„± í›„ ì—…ë¡œë“œ)
# [# ADDED] ì—°Â·ì €ë„ 1ê±´ ì²˜ë¦¬ (ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„± í›„ ì—…ë¡œë“œ)
def _run_one_piece(issn: str, year: int, email: str,
                   include_only_with_abstract: bool = False) -> Path:
    local_out = _local_piece_path(issn, year)
    
    # ì´ë¯¸ ì™„ì„±ëœ ì¡°ê°ì´ ìˆê³ , ê°•ì œ ë®ì–´ì“°ê¸° ì˜µì…˜ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if local_out.exists() and not bool(os.environ.get('OVERWRITE_LOCAL_PIECE')):
        return local_out

    try:
        # 1) ì›ê²© ì¡´ì¬ ì‹œ â†’ ë‹¤ìš´ë¡œë“œ
        if _gdrive_piece_exists(issn, year):
            if _gdrive_download_piece(issn, year, local_out):
                return local_out

        # 2) ì—†ìœ¼ë©´ ìƒì„±
        local_out.parent.mkdir(parents=True, exist_ok=True)
        prefix = _key_name_for(issn)

        config.email = email
        real1.main(
            issns=[issn],
            year_start=year,
            year_end=year,
            email=email,
            prefix=prefix,
            include_only_with_abstract=include_only_with_abstract,
            anchor_path=str(local_out),
        )

        json_merged    = f"{prefix}_{year}_{year}.json"
        tmp_csv        = f"{prefix}_{year}_{year}.csv"
        tmp_csv_ror    = f"{prefix}_{year}_{year}_ror.csv"
        tmp_csv_ror_ex = f"{prefix}_{year}_{year}_ror_extract.csv"
        tmp_csv_name   = f"{prefix}_{year}_{year}_ror_extract_name.csv"

        real2.main(
            input_json=json_merged,
            output_csv=tmp_csv,
            prefix=prefix,
            year_start=year,
            year_end=year,
            anchor_path=str(local_out),
        )

        # real3
        log_file = local_out.parent / f"run_log_{year}.txt"

        # ì´ì „ ë¡œê·¸ê°€ ìˆë‹¤ë©´ ì§€ìš°ê³  ìƒˆë¡œ ìƒì„±
        try:
            if log_file.exists():
                log_file.unlink()
        except Exception:
            pass

        print(f"[INFO] ROR ë§¤í•‘ ë¡œê·¸ë¥¼ {log_file} ì— ê¸°ë¡í•©ë‹ˆë‹¤.")

        with log_file.open("w", encoding="utf-8") as lf, redirect_stdout(lf):
            asyncio.run(
                real3.process(
                    input_csv=Path(tmp_csv),
                    output_csv=Path(tmp_csv_ror),
                    cache_file=Path("ror_cache.pkl"),
                    concurrency=20,
                    anchor_path=str(local_out),
                )
            )

        # 3-1. run_logë¡œë¶€í„° mapped_affiliations_{year}.csv ìƒì„±
        mapped_csv = local_out.parent / f"mapped_affiliations_{year}.csv"
        _build_mapped_affiliations_from_log(log_file, mapped_csv)

        real4.main(
            input_csv=tmp_csv_ror,
            output_csv=tmp_csv_ror_ex,
            prefix=prefix,
            year_start=year,
            year_end=year,
            anchor_path=str(local_out),
        )

        real5.main(
            input_csv=tmp_csv_ror_ex,
            output_csv=tmp_csv_name,
            prefix=prefix,
            year_start=year,
            year_end=year,
        )

        # ìµœì¢… ì¡°ê°: ë¡œì»¬ í‘œì¤€ ìœ„ì¹˜ë¡œ ì´ë™
        Path(tmp_csv_name).parent.mkdir(parents=True, exist_ok=True)
        Path(tmp_csv_name).replace(local_out)

        # 2-1) ì´ ì—°ë„ì˜ metrics.csvì— ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜ë¥¼ ë°˜ì˜í•˜ê³ ,
        #      ror_missing_after_extractëŠ” ì œê±°í•œë‹¤.
        metrics_path = local_out.parent / f"{prefix}_{year}_{year}_metrics.csv"
        _update_metrics_ror_for_piece(metrics_path, local_out)

        # 3) ì›ê²© ì—…ë¡œë“œ
        _gdrive_upload_piece(issn, year, local_out)

        # ì—°ë„ ë‹¨ìœ„ ì²˜ë¦¬ í›„ ì ê¹ ì‰¬ê¸°(ë°±ì˜¤í”„)
        # 429 ì—ëŸ¬ ì™„í™”(ROR ì¿¼ë¦¬ ëª°ë¦¬ë©´ ë°œìƒ)
        import time, random
        time.sleep(5 + random.uniform(0, 0.5))

        return local_out

    except Exception:
        # âœ… ì–´ë–¤ ë‹¨ê³„ì—ì„œë“  ì‹¤íŒ¨í•˜ë©´, ê¹¨ì§„ ì¡°ê° CSVëŠ” ë‚¨ê¸°ì§€ ì•Šë„ë¡ ì •ë¦¬
        if local_out.exists():
            try:
                local_out.unlink()
            except Exception:
                # ì‚­ì œë„ ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ë¬´ì‹œí•˜ê³  ì›ë˜ ì˜ˆì™¸ë§Œ ë‹¤ì‹œ ì˜¬ë¦¼
                pass
        raise

# [# ADDED] ì¡°ê° ë³‘í•© â†’ ìµœì¢… CSV
def _collect_merge(issns: List[str], year_start: int, year_end: int) -> Path:
    issns = _normalize_issn_list(issns)
    piece_paths: List[Path] = []
    keys: List[str] = []

    for issn in issns:
        key = _key_name_for(issn)
        keys.append(key)
        for y in range(year_start, year_end + 1):
            local_piece = _local_piece_path(issn, y)
            if local_piece.exists():
                piece_paths.append(local_piece)
            else:
                if _gdrive_download_piece(issn, y, local_piece):
                    piece_paths.append(local_piece)

    if not piece_paths:
        raise FileNotFoundError("ì„ íƒ ë²”ìœ„ì˜ ì¡°ê° CSVê°€ ì—†ìŠµë‹ˆë‹¤.")

    dfs = []
    for p in piece_paths:
        p = Path(p)
        if not p.exists():
            print(f"[_collect_merge] ê²½ê³ : ì¡°ê° íŒŒì¼ ì—†ìŒ: {p}")
            continue

        # 1) ìš°ì„ : C ì—”ì§„ìœ¼ë¡œ ë¹ ë¥´ê³  ê²¬ê³ í•˜ê²Œ ì‹œë„ (low_memory=False í—ˆìš©)
        try:
            df_piece = pd.read_csv(
                p,
                encoding="utf-8-sig",
                quoting=csv.QUOTE_MINIMAL,
                quotechar='"',
                escapechar='\\',
                low_memory=False
            )
            dfs.append(df_piece)
            continue
        except ParserError as e:
            print(f"[_collect_merge] C engine ParserError for {p}: {e} -> retry with python engine")
        except Exception as e:
            print(f"[_collect_merge] C engine exception for {p}: {e} -> retry with python engine")

        # 2) C ì—”ì§„ ì‹¤íŒ¨ ì‹œ: python ì—”ì§„ìœ¼ë¡œ ì¬ì‹œë„ (low_memory ì˜µì…˜ ì—†ìŒ)
        try:
            df_piece = pd.read_csv(
                p,
                engine="python",
                encoding="utf-8-sig",
                quoting=csv.QUOTE_MINIMAL,
                quotechar='"',
                escapechar='\\',
                on_bad_lines='skip' # ê¸°ì¡´ì—ëŠ” errorë¡œ í–ˆìŒ
            )
            dfs.append(df_piece)
            continue
        except Exception as e:
            print(f"[_collect_merge] python engine read failed for {p}: {e} -> falling back to csv.DictReader")

        # 3) ìµœí›„ì˜ ì•ˆì „ë§: csv.DictReaderë¡œ ë¬´ì¡°ê±´ ì½ì–´ì„œ ëª¨ë“  í–‰ì„ í™•ë³´ (ëˆ„ë½ ë°©ì§€)
        try:
            rows = []
            with open(p, 'r', encoding='utf-8-sig', newline='') as fh:
                rdr = csv.DictReader(fh)
                for row in rdr:
                    rows.append(row)
            df_piece = pd.DataFrame(rows)
            dfs.append(df_piece)
            print(f"[_collect_merge] csv.DictReader fallback succeeded for {p} (rows={len(df_piece)})")
        except Exception as e2:
            print(f"[_collect_merge] csv.DictReader fallback also failed for {p}: {e2}")
            raise
    # ë³‘í•©
    if not dfs:
        merged = pd.DataFrame()
    else:
        merged = pd.concat(dfs, ignore_index=True)



    # # [# ADDED] ì¤‘ë³µ ì œê±°(ê°€ëŠ¥í•˜ë©´ DOI ê¸°ì¤€)                      final.ipynbì™€ app.pyì˜ ê²°ê³¼íŒŒì¼ì´ ë™ì¼í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •
    # if "doi" in merged.columns:
    #     merged = merged.drop_duplicates(subset=["doi"])
    # else:
    #     merged = merged.drop_duplicates()

    # ë³´ê¸° ì¢‹ì€ ì—´ ìˆœì„œ(ì„ íƒ)
    preferred = [c for c in ["title", "doi", "published_year", "host_venue_issn_l",
                             "institution_name", "ror_id"] if c in merged.columns]
    merged = merged[[*preferred, *[c for c in merged.columns if c not in preferred]]]

    merged_name = f"{'-'.join(keys)}_{year_start}_{year_end}_ror_extract_name.csv"
    out_path = LOCAL_WORKDIR / merged_name
    merged.to_csv(out_path, index=False, encoding="utf-8-sig")
    return out_path


# ======================================================================
# [ì—”ë“œí¬ì¸íŠ¸] run_pipeline_cached â€” app.pyê°€ í˜¸ì¶œ (Drive ì˜êµ¬ ì €ì¥)
# ======================================================================
def run_pipeline_cached(issns: List[str], year_start: int, year_end: int,
                        email: str = 's0124kw@gmail.com',
                        include_only_with_abstract: bool = False,
                        make_html: bool = False,
                        base_dir: Path = Path("storage")):
    """
    1) ê° ì €ë„Ã—ì—°ë„ ì¡°ê°ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒì„± í›„ Drive ì—…ë¡œë“œ
    2) ì¡°ê°ë“¤ì„ ëª¨ì•„ ë¡œì»¬ì—ì„œ ìµœì¢… CSV ë³‘í•©
    3) (ì˜µì…˜) HTML ìƒì„±
    ì£¼ì˜: base_dir ì¸ìëŠ” í˜¸í™˜ì„±ë§Œ ìœ ì§€(Drive ì‚¬ìš©ìœ¼ë¡œ ë¬´ì‹œ)
    """
    issns = _normalize_issn_list(issns)

    # ì´ì „ ì‹¤í–‰ì˜ ì”ì¬ ì œê±° (ì „ì—­ ê°•ì œ ê³ ì • ì‚¬ìš© ì•ˆí•¨)
    global FORCE_JOURNAL_KEY
    FORCE_JOURNAL_KEY = None

    for issn in issns:
        for y in range(int(year_start), int(year_end) + 1):
            try:
                _run_one_piece(issn, y, email, include_only_with_abstract)
            except Exception as e:
                import traceback, sys
                # ì—ëŸ¬ ë¡œê¹…: ì–´ë–¤ ISSN/ì—°ë„ì—ì„œ ì‹¤íŒ¨í–ˆëŠ”ì§€ ëª…í™•íˆ ë‚¨ê¹€
                print(f"[run_pipeline_cached] ERROR processing ISSN={issn}, year={y}: {e}", file=sys.stderr)
                traceback.print_exc()
                # (ì„ íƒ) ì‹¤íŒ¨ì‹œ placeholder *_metrics.csvë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆìŒ(ìš”ì•½ ì§‘ê³„ ì™„ì„±ì„ ìœ„í•´)
                # continue í•˜ì—¬ ë‹¤ìŒ ì—°ë„/ISSNìœ¼ë¡œ ì§„í–‰
                continue

    final_csv_path = _collect_merge(issns, int(year_start), int(year_end))

    # per-year metrics.csv í•©ì‚° -> Summary ë°˜ì˜
    final_path = Path(final_csv_path)
    summary_path = final_path.with_name(final_path.stem + "_ror_extract_name_summary.csv")
    build_summary_from_metrics_for_final(str(final_csv_path), str(summary_path))

    html_path = None
    if make_html:
        html_path = make_html_from_csv(str(final_csv_path))

    return str(final_csv_path), html_path


# ======================================================================
# (ì˜µì…˜) ë¡œì»¬ ë‹¨ë… í…ŒìŠ¤íŠ¸
# ======================================================================
if __name__ == "__main__":
    example_issns = ['0043-1354','0011-9164','0733-9429']
    out_csv, _ = run_pipeline_cached(
        issns=example_issns,
        year_start=2017, year_end=2019,
        email='Your Email Here',
        include_only_with_abstract=False,
        make_html=False
    )
    print("FINAL:", out_csv)