import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"

import streamlit as st
import streamlit.components.v1 as components
import nest_asyncio
import pandas as pd
import datetime
from pathlib import Path
from zoneinfo import ZoneInfo
from io import BytesIO
import zipfile
import json
import re
import ast


from new import run_pipeline, make_html_from_csv, run_pipeline_cached, make_html_string_from_csv

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
now_kst = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

# ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ë°©ì§€ (async ì‚¬ìš© ì‹œ í•„ìˆ˜)
nest_asyncio.apply()


st.set_page_config(page_title="OpenAlex Paper Metadata Pipeline", layout="wide")

# ---- View state persistence (rerunì—ë„ íƒ­ ìœ ì§€) ----
if "view" not in st.session_state:
    _default = st.query_params.get("view", "Preview")
    st.session_state.view = _default if _default in ("Preview", "ë‹¤ìš´ë¡œë“œ", "Summary") else "Preview"

def _on_change_view():
    st.session_state.view = st.session_state._view_radio
    st.query_params["view"] = st.session_state.view



# ìºì‹œ ë°ì½”ë ˆì´í„° ì œê±° (íŒŒì¼ ìƒì„±/ì™¸ë¶€ I/OëŠ” ìºì‹œ ë¹„ê¶Œì¥)
def cached_run(issns, year_start, year_end, email,
               progress_bar=None, progress_text=None):

    # storage í•˜ìœ„ì— ê²°ê³¼ ì €ì¥
    final_csv_path, _ = run_pipeline_cached(
        issns=issns, year_start=year_start, year_end=year_end, email=email,
        include_only_with_abstract=False, make_html=True,
        base_dir=Path("storage")
    )

    if progress_bar:
        progress_bar.progress(0.7)
    if progress_text:
        progress_text.info("ìµœì¢… CSV ìƒì„± ë° í™•ì¸ ì¤‘ ..")

    #ìµœì¢… CSV ê²½ë¡œ ë¬¸ìì—´ -> Path ê°ì²´
    out_path = Path(final_csv_path)

    final_html = make_html_from_csv(str(out_path))

    if progress_bar:
        progress_bar.progress(1.0)
    if progress_text:
        progress_text.success("ëª¨ë“  êµ¬ê°„ ì²˜ë¦¬ ë° ìµœì¢… íŒŒì¼ ìƒì„± ì™„ë£Œ")

    st.sidebar.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ Â· {now_kst:%H:%M:%S}")

    return str(out_path), final_html

# === ìƒˆ í—¬í¼: ì„¼íŠ¸ëŸ´ë¦¬í‹° CSV ì°¾ê¸° ===
def _find_centrality_csvs(final_csv_path: str):
    from pathlib import Path
    base = Path(final_csv_path).with_suffix("")  # ..._ror_extract_name
    base_str = str(base)
    candidates = [
        f"{base_str}_centrality.csv",
        f"{base_str}_ror_extract_name_centrality.csv",
    ]
    # logsì—ì„œ workdir_tmpì— ì €ì¥ë˜ëŠ” ê²½ìš°ë„ ì»¤ë²„
    search_dirs = [base.parent, base.parent / "workdir_tmp"]
    found = None
    for d in search_dirs:
        for name in candidates:
            p = d / Path(name).name
            if p.exists():
                found = p
                break
        if found: break
    return found

# === ìƒˆ í—¬í¼: ê³µí†µ ìš”ì•½í‘œ ===
# app.py
def _summary_metrics_common(final_csv_path: str):
    final_csv = Path(final_csv_path)
    root_base = re.sub(r'_(?:ror(?:_extract(?:_name)?)?)(?:_summary)?$', '', final_csv.stem)

    # 1. ëª¨ë“  metrics.json íŒŒì¼ì—ì„œ í†µê³„ ìˆ˜ì¹˜ ì§‘ê³„
    def sum_metrics(keys):
        acc = {k: 0 for k in keys}
        # íƒìƒ‰ ê²½ë¡œë¥¼ ë‹¨ìˆœí™”í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ì •
        search_dirs = [final_csv.parent, final_csv.parent / "workdir_tmp"]
        candidates = []
        for d in search_dirs:
            if d.exists():
                candidates.extend(d.rglob(f"{root_base}_metrics.json")) # íŒŒì¼ëª…ì„ ë” ì •í™•í•˜ê²Œ ì§€ì •

        # ì¤‘ë³µ ë°©ì§€
        candidates = sorted(list(set(candidates)))

        for mf in candidates:
            try:
                data = json.loads(mf.read_text(encoding="utf-8")) or {}
                for k in keys:
                    v = data.get(k)
                    if v is not None:
                        # ë¹„ìœ¨(rate)ì€ ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ë®ì–´ì“°ê³ , ë‚˜ë¨¸ì§€ëŠ” í•©ì‚°
                        if k.endswith("_rate"):
                            acc[k] = float(v)
                        elif isinstance(v, (int, float)):
                            acc[k] += int(v)
            except Exception:
                continue
        return acc

    metric_keys = [
        "total_collected", "authorships_removed_empty_list",
        "doi_missing", "doi_enriched", "doi_enrich_rate",
        "ror_missing", "ror_enriched", "ror_enrich_rate",
        "ror_missing_after_extract"
    ]
    metrics = sum_metrics(metric_keys)

    # 2. CSVì—ì„œ ì§ì ‘ ê³„ì‚°í•´ì•¼ í•˜ëŠ” ë³´ì¡° ì§€í‘œ
    node_count = edge_count = ror_missing_before = 0
    try:
        df_final = pd.read_csv(final_csv_path, dtype={'authorships': str, 'org_names': str})

        # ROR ID ê²°ì¸¡ ìˆ˜ (ë§¤í•‘ ì‹œë„ ì „ ê¸°ì¤€)
        if "authorships" in df_final.columns:
            rx = re.compile(r'https?://ror\.org/[0-9a-z]+', re.I)
            ror_urls = df_final["authorships"].fillna('').apply(lambda s: rx.findall(s))
            ror_missing_before = int((ror_urls.str.len() == 0).sum())

        # ë…¸ë“œ ë° ì—£ì§€ ìˆ˜
        cent_csv = _find_centrality_csvs(final_csv_path)
        if cent_csv and cent_csv.exists():
            cent_df = pd.read_csv(cent_csv)
            node_col = "org" if "org" in cent_df.columns else cent_df.columns[0]
            node_count = cent_df[node_col].nunique()

            if "org_names" in df_final.columns:
                from itertools import combinations
                node_set = set(cent_df[node_col].astype(str))
                all_edges = set()
                for orgs_str in df_final["org_names"].dropna():
                    try:
                        orgs = ast.literal_eval(orgs_str)
                        # í•„í„°ë§ëœ ë…¸ë“œë§Œ í¬í•¨í•˜ì—¬ ì—£ì§€ ê³„ì‚°
                        valid_orgs = [o for o in orgs if o in node_set]
                        if len(valid_orgs) >= 2:
                            for u, v in combinations(sorted(list(set(valid_orgs))), 2):
                                all_edges.add((u, v))
                    except Exception:
                        continue
                edge_count = len(all_edges)
    except Exception as e:
        st.warning(f"CSV ê¸°ë°˜ ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 3. ìµœì¢… ê°’ ê²°ì • (Metrics ìš°ì„ , ì—†ìœ¼ë©´ ê³„ì‚° ê°’ìœ¼ë¡œ ëŒ€ì²´)
    total_papers = metrics.get("total_collected", 0)
    editorial_removed = metrics.get("authorships_removed_empty_list", 0)
    doi_missing = metrics.get("doi_missing", 0)
    doi_enriched = metrics.get("doi_enriched", 0)
    doi_enrich_rate = metrics.get("doi_enrich_rate", 0.0)
    ror_missing = metrics.get("ror_missing", ror_missing_before) # metrics.json ìš°ì„ 
    ror_enriched = metrics.get("ror_enriched", 0)
    ror_enrich_rate = metrics.get("ror_enrich_rate", 0.0)

    # ë³´ê°•ë¥  ì¬ê³„ì‚° (í•©ì‚°ëœ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ë” ì •í™•)
    if doi_missing > 0:
        doi_enrich_rate = (doi_enriched / doi_missing) * 100
    if ror_missing > 0:
        ror_enrich_rate = (ror_enriched / ror_missing) * 100

    # 4. ìš”ì•½ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    summary_data = {
        "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": int(total_papers),
        "Editorial Material ì‚­ì œ ìˆ˜": int(editorial_removed),
        "DOI ê²°ì¸¡ ìˆ˜": int(doi_missing),
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": int(doi_enriched),
        "DOI ë³´ê°•ë¥ ": f"{doi_enrich_rate:.2f}%",
        "ROR ID ê²°ì¸¡ ìˆ˜": int(ror_missing),
        "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": int(ror_enriched),
        "ROR ID ë³´ê°•ë¥ ": f"{ror_enrich_rate:.2f}%",
        "ë…¸ë“œ ìˆ˜": int(node_count),
        "ì—£ì§€ ìˆ˜": int(edge_count),
    }

    df_summary = pd.DataFrame([summary_data])
    return df_summary

# === ìƒˆ í—¬í¼: ì¤‘ì‹¬ì„±ë³„ Top ë…¸ë“œ & ì—£ì§€ í‘œ ===
def _summary_top_nodes_and_edges(final_csv_path: str):

    cent_csv = _find_centrality_csvs(final_csv_path)
    if cent_csv is None:
        return pd.DataFrame(), pd.DataFrame()

    cent = pd.read_csv(cent_csv)
    node_col = "node" if "node" in cent.columns else cent.columns[0]
    candidates = [c for c in ["degree","eigenvector","betweenness","closeness"] if c in cent.columns]
    if not candidates:
        # ë…¸ë“œëª…ë§Œ ìˆëŠ” ê²½ìš°
        return pd.DataFrame(), pd.DataFrame()

    # Top ë…¸ë“œ í‘œ ë§Œë“¤ê¸°
    top_nodes = {}
    for c in candidates:
        tmp = cent[[node_col, c]].dropna()
        tmp = tmp.sort_values(c, ascending=False)
        names = tmp[node_col].astype(str).head(10).tolist()
        # ì´ë¦„ë§Œ ë³´ì—¬ë‹¬ë¼ëŠ” ìš”êµ¬ì— ë§ì¶° ì ìˆ˜ëŠ” ì œì™¸
        top_nodes[c] = [*names, *([""]*(10-len(names)))]

    df_nodes = pd.DataFrame(top_nodes, index=[f"Top {i}" for i in range(1,11)])


    def _pairs_from_org_names(s):
        import ast
        arr = []
        if isinstance(s, str):
            t = s.strip()
            if t:
                try:
                    v = ast.literal_eval(t)
                    arr = list(v) if isinstance(v, (list, tuple)) else [v]
                except Exception:
                    try:
                        v = json.loads(t)
                        arr = v if isinstance(v, list) else [v]
                    except Exception:
                        for sep in [";", "|", ","]:
                            if sep in t:
                                arr = [x.strip() for x in t.split(sep) if x.strip()]
                                break
                        if not arr:
                            arr = [t]
        elif isinstance(s, list):
            arr = s

        arr = [str(a) for a in arr if a]
        pairs=[]
        for i in range(len(arr)):
            for j in range(i+1,len(arr)):
                a,b = sorted((arr[i],arr[j]))
                pairs.append((a,b))
        return pairs


    final_df = pd.read_csv(final_csv_path)
    all_pairs=[]
    if "org_names" in final_df.columns:
        final_df["org_names"].astype(str).apply(lambda s: all_pairs.extend(_pairs_from_org_names(s)))

    # ë…¸ë“œ ì ìˆ˜ map
    score_map = {c: dict(zip(cent[node_col].astype(str), cent[c])) for c in candidates}

    # ì¤‘ì‹¬ì„±ë³„ë¡œ ì—£ì§€ ì ìˆ˜ = ë…¸ë“œ ì ìˆ˜ í•©(ê°„ë‹¨ ê·¼ì‚¬)
    top_edges = {}
    for c in candidates:
        sm = score_map[c]
        scored=[]
        for a,b in set(all_pairs):
            s = float(sm.get(a,0)) + float(sm.get(b,0))
            scored.append((s, f"{a} - {b}"))
        scored.sort(key=lambda x:x[0], reverse=True)
        names = [name for _,name in scored[:10]]
        top_edges[c] = [*names, *([""]*(10-len(names)))]
    df_edges = pd.DataFrame(top_edges, index=[f"Top {i}" for i in range(1,11)])
    return df_nodes, df_edges





def sidebar_controls():
    st.sidebar.header("ì„¤ì •")
    issns_text = st.sidebar.text_area(
        "ISSN ë¦¬ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
        value="0043-1354\n0011-9164\n0733-9429",
        height=100,
    )
    issns = [s.strip() for s in issns_text.splitlines() if s.strip()]

    year_col1, year_col2 = st.sidebar.columns(2)
    year_start = year_col1.number_input("ì‹œì‘ ì—°ë„", value=2015, step=1)
    year_end = year_col2.number_input("ì¢…ë£Œ ì—°ë„", value=2024, step=1)

    email = st.sidebar.text_input("OpenAlex ì´ë©”ì¼", value="s0124kw@gmail.com")
    run_btn = st.sidebar.button("ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰", width="stretch")
    return issns, int(year_start), int(year_end), email, run_btn


def app():
    st.title("Paper Metadata Collection Pipeline")

    if "runs" not in st.session_state:
        st.session_state.runs = []

    issns, year_start, year_end, email, run_btn = sidebar_controls()

    # ì§„í–‰ë¥ /ë©”ì‹œì§€ ìŠ¬ë¡¯ -> ì§„í–‰ì¤‘ì¸ ìƒí™© ì¶œë ¥
    progress_bar = st.progress(0, text="ëŒ€ê¸° ì¤‘")
    progress_text = st.empty()

    # Preview í–‰ ìˆ˜(ê³ ì •)
    PREVIEW_N = 1000
    # CSV ë‹¤ìš´ë¡œë“œ ì„ê³„ê°’(MB)
    THRESHOLD_MB = 150

    # === ê³µí†µ Summary ë Œë”ë§ í•¨ìˆ˜ ===
    def _render_summary_block(sel_csv: str):
        with st.expander("Summary", expanded=True):
            # 1) ê³µí†µ ìš”ì•½í‘œ
            st.subheader("ê³µí†µ ìš”ì•½ ì§€í‘œ")
            summary_df = _summary_metrics_common(sel_csv)
            st.dataframe(summary_df, width="stretch")

            # 2) Summary CSV ë‹¤ìš´ë¡œë“œ (index=False, UTF-8-SIG)
            csv_bytes = summary_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "ë‹¤ìš´ë¡œë“œ Summary CSV",
                data=csv_bytes,
                file_name=Path(sel_csv).stem + "_summary.csv",
                mime="text/csv",
            )

            # 3) ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ&ì—£ì§€ (ê²°í•© í…Œì´ë¸”)
            st.subheader("ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ&ì—£ì§€")
            top_nodes, top_edges = _summary_top_nodes_and_edges(sel_csv)
            if not top_nodes.empty:
                combined = pd.DataFrame(index=top_nodes.index)
                for c in top_nodes.columns:
                    combined[(c, "ë…¸ë“œ")] = top_nodes[c]
                    combined[(c, "ì—£ì§€")] = top_edges[c] if c in top_edges.columns else ""
                combined.columns = [f"{m} Â· {sub}" for m, sub in combined.columns]
                st.dataframe(combined, width="stretch")
            else:
                st.info("ì¤‘ì‹¬ì„± CSVë¥¼ ì°¾ì§€ ëª»í•´ Top ë…¸ë“œ/ì—£ì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # === ê³µí†µ ë‹¤ìš´ë¡œë“œ ë¸”ë¡(ì‹œê°í™” í¬í•¨) ===
    def _render_download_block(sel_csv: str, sel_html: str):
        csv_path = Path(sel_csv)
        file_size_mb = csv_path.stat().st_size / (1024 * 1024)

        col_csv, col_html = st.columns(2)
        with col_csv:
            if file_size_mb <= THRESHOLD_MB:
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ ({file_size_mb:.1f} MB)",
                    data=csv_path.read_bytes(),
                    file_name=csv_path.name,
                    mime="text/csv",
                    width="stretch",
                )
            else:
                buf = BytesIO()
                with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
                    zf.writestr(csv_path.name, csv_path.read_bytes())
                zip_bytes = buf.getvalue()
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ (ZIP, ì›ë³¸ {file_size_mb:.1f} MB)",
                    data=zip_bytes,
                    file_name=csv_path.stem + ".zip",
                    mime="application/zip",
                    width="stretch",
                )

        with col_html:
            # Degree ê¸°ë°˜
            deg_html = make_html_string_from_csv(str(csv_path), size_by="degree", color_by="degree")
            st.download_button(
                label="Visualization ë‹¤ìš´ë¡œë“œ (Degree-based)",
                data=deg_html.encode("utf-8"),
                file_name=csv_path.stem + "_degree_network.html",
                mime="text/html",
                width="stretch",
            )
            # Eigenvector ê¸°ë°˜
            eig_html = make_html_string_from_csv(str(csv_path), size_by="eigenvector", color_by="eigenvector")
            st.download_button(
                label="Visualization ë‹¤ìš´ë¡œë“œ (Eigenvector-based)",
                data=eig_html.encode("utf-8"),
                file_name=csv_path.stem + "_eigenvector_network.html",
                mime="text/html",
                width="stretch",
            )

    # === ê³µí†µ Preview(ìƒìœ„ 1,000í–‰) ë¸”ë¡ ===
    def _render_preview_block(sel_csv: str):
        df = pd.read_csv(sel_csv)
        total_rows = len(df)
        st.caption(
            f"âš ï¸ ì•„ë˜ í‘œëŠ” ì´ {total_rows:,}í–‰ ì¤‘ "
            f"ìƒìœ„ {min(PREVIEW_N, total_rows):,}í–‰ë§Œ Previewì…ë‹ˆë‹¤. "
            "ì „ì²´ëŠ” ì•„ë˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
        )
        st.dataframe(df.head(PREVIEW_N), width="stretch")

        # â–¼ ë„¤íŠ¸ì›Œí¬ Preview(ì„ë² ë“œ) â€” degree / eigenvector
        with st.expander("ë„¤íŠ¸ì›Œí¬ Preview (Degree / Eigenvector)", expanded=False):
            tabs = st.tabs(["Degree ë„¤íŠ¸ì›Œí¬", "Eigenvector ë„¤íŠ¸ì›Œí¬"])

            # HTML ë¬¸ìì—´ ìƒì„± (ê¸°ì¡´ í—¬í¼ ì¬ì‚¬ìš©)
            deg_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="degree",     color_by="degree")
            eig_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="eigenvector", color_by="eigenvector")

            with tabs[0]:
                components.html(deg_html, height=800, scrolling=True)

            with tabs[1]:
                components.html(eig_html, height=800, scrolling=True)

    # === ì‹¤í–‰ ì—¬ë¶€ì— ë”°ë¼ ê²°ê³¼ ë Œë” ===
    if run_btn:
        # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
        start_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

        with st.spinner("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)"):
            progress_text.info("ì¤€ë¹„ ì¤‘..")
            final_csv, final_html = cached_run(
                issns, year_start, year_end, email,
                progress_bar=progress_bar, progress_text=progress_text
            )

        # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        end_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))
        duration = end_time - start_time

        # ì„¸ì…˜ ì´ë ¥ì— ê¸°ë¡ (ë¼ë²¨: issn|ì—°ë„|ì‹œê°)
        prefix = Path(final_csv).stem.replace(f"_{year_start}_{year_end}_ror_extract_name", "")
        label = f"{prefix} | {year_start}-{year_end} | {start_time.strftime('%m/%d %H:%M')}"
        st.session_state.runs.append({
            "label": label, "csv": final_csv, "html": final_html,
            "start": start_time.strftime('%Y-%m-%d %H:%M:%S'),
            "end": end_time.strftime('%Y-%m-%d %H:%M:%S'),
            "duration": str(duration)
        })

        # ê°€ì¥ ìµœê·¼ ì‹¤í–‰ ì„ íƒ
        sel = st.session_state.runs[-1]
        st.success(f"ì‹¤í–‰ ì™„ë£Œ: {sel['label']}")
        st.sidebar.markdown("---")
        st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
        st.sidebar.write(f" ì‹œì‘ {sel['start']}")
        st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
        st.sidebar.write(f" ì†Œìš” {sel['duration']}")

        # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
        view = st.radio(
            "ë³´ê¸° ì„ íƒ",
            ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
            index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
            key="_view_radio",
            horizontal=True,
            on_change=_on_change_view,
        )
        view = st.session_state.view

        if view == "Preview":
            _render_preview_block(sel["csv"])
        if view == "ë‹¤ìš´ë¡œë“œ":
            _render_download_block(sel["csv"], sel["html"])
        if view == "Summary":
            _render_summary_block(sel["csv"])

        # âœ… Summaryê°€ í•­ìƒ ë¶™ë„ë¡(ë‹¤ìš´ë¡œë“œ í›„ rerunë˜ì–´ë„)
        # ë³´ì¡°ë¡œ ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€
        st.session_state["last_csv"] = sel["csv"]

    else:
        # ì´ì „ ì‹¤í–‰ ì´ë ¥ ì„ íƒ/í‘œì‹œ
        if st.session_state.runs:
            st.markdown("### ì´ì „ ì‹¤í–‰ ê²°ê³¼ ë³´ê¸°")
            options = [r["label"] for r in st.session_state.runs]
            selected = st.selectbox("ì‹¤í–‰ ê²°ê³¼ ì„ íƒ", options, index=len(options)-1)
            sel = next(r for r in st.session_state.runs if r["label"] == selected)

            # ì‚¬ì´ë“œë°”ì— ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
            st.sidebar.markdown("---")
            st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
            st.sidebar.write(f" ì‹œì‘ {sel['start']}")
            st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
            st.sidebar.write(f" ì†Œìš” {sel['duration']}")

            # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
            view = st.radio(
                "ë³´ê¸° ì„ íƒ",
                ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
                index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
                key="_view_radio",
                horizontal=True,
                on_change=_on_change_view,
            )
            view = st.session_state.view

            if view == "Preview":
                _render_preview_block(sel["csv"])
            if view == "ë‹¤ìš´ë¡œë“œ":
                _render_download_block(sel["csv"], sel["html"])
            if view == "Summary":
                _render_summary_block(sel["csv"])

            # ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€(ì„ íƒ ìœ ì§€ìš©)
            st.session_state["last_csv"] = sel["csv"]
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")



if __name__ == "__main__":
    app()
