import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"

import streamlit as st
import streamlit.components.v1 as components
import nest_asyncio
import pandas as pd
import datetime
from pathlib import Path
from zoneinfo import ZoneInfo
from io import BytesIO
import zipfile
import json
import re

from new import run_pipeline, make_html_from_csv, run_pipeline_cached, make_html_string_from_csv

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
now_kst = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

# ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ë°©ì§€ (async ì‚¬ìš© ì‹œ í•„ìˆ˜)
nest_asyncio.apply()


st.set_page_config(page_title="OpenAlex Paper Metadata Pipeline", layout="wide")

# ---- View state persistence (rerunì—ë„ íƒ­ ìœ ì§€) ----
if "view" not in st.session_state:
    _default = st.query_params.get("view", "Preview")
    st.session_state.view = _default if _default in ("Preview", "ë‹¤ìš´ë¡œë“œ", "Summary") else "Preview"

def _on_change_view():
    st.session_state.view = st.session_state._view_radio
    st.query_params["view"] = st.session_state.view



# ìºì‹œ ë°ì½”ë ˆì´í„° ì œê±° (íŒŒì¼ ìƒì„±/ì™¸ë¶€ I/OëŠ” ìºì‹œ ë¹„ê¶Œì¥)
def cached_run(issns, year_start, year_end, email,
               progress_bar=None, progress_text=None):

    # storage í•˜ìœ„ì— ê²°ê³¼ ì €ì¥
    final_csv_path, _ = run_pipeline_cached(
        issns=issns, year_start=year_start, year_end=year_end, email=email,
        include_only_with_abstract=False, make_html=True,
        base_dir=Path("storage")
    )

    if progress_bar:
        progress_bar.progress(0.7)
    if progress_text:
        progress_text.info("ìµœì¢… CSV ìƒì„± ë° í™•ì¸ ì¤‘ ..")

    #ìµœì¢… CSV ê²½ë¡œ ë¬¸ìì—´ -> Path ê°ì²´
    out_path = Path(final_csv_path)

    final_html = make_html_from_csv(str(out_path))

    if progress_bar:
        progress_bar.progress(1.0)
    if progress_text:
        progress_text.success("ëª¨ë“  êµ¬ê°„ ì²˜ë¦¬ ë° ìµœì¢… íŒŒì¼ ìƒì„± ì™„ë£Œ")

    st.sidebar.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ Â· {now_kst:%H:%M:%S}")

    return str(out_path), final_html

# === ìƒˆ í—¬í¼: ì„¼íŠ¸ëŸ´ë¦¬í‹° CSV ì°¾ê¸° ===
def _find_centrality_csvs(final_csv_path: str):
    from pathlib import Path
    base = Path(final_csv_path).with_suffix("")  # ..._ror_extract_name
    base_str = str(base)
    candidates = [
        f"{base_str}_centrality.csv",
        f"{base_str}_ror_extract_name_centrality.csv",
    ]
    # logsì—ì„œ workdir_tmpì— ì €ì¥ë˜ëŠ” ê²½ìš°ë„ ì»¤ë²„
    search_dirs = [base.parent, base.parent / "workdir_tmp"]
    found = None
    for d in search_dirs:
        for name in candidates:
            p = d / Path(name).name
            if p.exists():
                found = p
                break
        if found: break
    return found

# === ìƒˆ í—¬í¼: ê³µí†µ ìš”ì•½í‘œ ===
def _summary_metrics_common(final_csv_path: str):
    total_collected = None
    removed_authorships_empty = None

    doi_missing = doi_enriched = doi_enrich_rate = None
    ror_missing = ror_enriched = ror_enrich_rate = None
    ror_missing_before = None
    ror_missing_after = None

    final_csv = Path(final_csv_path)
    base_path = final_csv.with_suffix("")
    base = str(base_path)

    # ì ‘ë¯¸ì‚¬ ì œê±°í•´ ë£¨íŠ¸ë² ì´ìŠ¤ ì¶”ì¶œ
    root_base = re.sub(r'_(?:ror(?:_extract(?:_name)?)?)(?:_summary)?$', '', base)

    json_merged = Path(f"{root_base}.json")
    csv_file    = Path(final_csv)

    # export/summary íŒŒì¼ì´ë©´ CSV ê¸¸ì´ í´ë°± ê¸ˆì§€
    is_export = final_csv.name.endswith("_export.csv") or final_csv.name.endswith("_summary.csv")

    # ì ‘ë‘ì‚¬/ì—°ë„ ì¶”ì¶œ
    m = re.search(r"_(\d{4})_(\d{4})$", root_base)
    y0 = y1 = None
    prefixes = []
    prefix_all = ""
    if m:
        y0, y1 = int(m.group(1)), int(m.group(2))
        prefix_all = Path(root_base).name[:-(len(m.group(0)))]
        prefixes = prefix_all.split("-") if prefix_all else []

    # âœ… ê²€ìƒ‰ ë””ë ‰í„°ë¦¬ë¥¼ ë°”ê¹¥ì—ì„œ 'í•œ ë²ˆ' ì •ì˜ (sum_metricsì™€ JSON í´ë°±ì´ í•¨ê»˜ ì‚¬ìš©)
    root_dir = Path(root_base).parent
    cwd = Path.cwd()
    search_dirs = [
        root_dir,
        root_dir / "workdir_tmp",
        root_dir.parent,
        root_dir.parent.parent,
        Path("workdir_tmp"),
        cwd / "workdir_tmp",
        cwd,
        cwd.parent,
    ]

    def sum_metrics(keys, search_dirs):
        import csv, re
        from pathlib import Path

        # í•©ê³„(ì •ìˆ˜/ì‹¤ìˆ˜) ì´ˆê¸°í™”
        acc = {k: 0 for k in keys if not k.endswith("_rate")}
        # ë¹„ìœ¨ í‚¤ëŠ” ë‚˜ì¤‘ì— ê³„ì‚°
        rate_keys = [k for k in keys if k.endswith("_rate")]
        found = False

        pat = re.compile(r"(.+?)_(\d{4})_(\d{4})_metrics\.csv$", re.I)
        seen = set()

        def _to_num(s):
            try:
                if s is None or s == "": return None
                f = float(s)
                return int(f) if f.is_integer() else f
            except Exception:
                return None

        for base in search_dirs:
            base = Path(base)
            if not base.exists(): 
                continue
            for mf in base.rglob("*_metrics.csv"):
                rp = mf.resolve()
                if rp in seen: 
                    continue
                if not pat.search(mf.name):
                    continue
                seen.add(rp)
                try:
                    with mf.open("r", encoding="utf-8", newline="") as f:
                        rdr = csv.reader(f)
                        header = next(rdr, None)  # ["key","value"] ê¸°ëŒ€
                        for row in rdr:
                            if len(row) < 2:
                                continue
                            k, v = row[0], _to_num(row[1])
                            if k in acc and v is not None:
                                # rateëŠ” í•©ì‚°í•˜ì§€ ì•ŠìŒ(í›„ê³„ì‚°)
                                if not k.endswith("_rate"):
                                    acc[k] += v if isinstance(v, (int, float)) else 0
                    found = True
                except Exception:
                    pass

        # í•©ê³„ë¡œ ë¹„ìœ¨ ì¬ê³„ì‚°(ìˆì„ ë•Œë§Œ)
        # DOI: doi_enriched / (doi_missing + doi_enriched)
        if "doi_enriched" in acc and "doi_missing" in acc and "doi_enrich_rate" in rate_keys:
            denom = (acc["doi_missing"] or 0) + (acc["doi_enriched"] or 0)
            acc["doi_enrich_rate"] = round(100.0 * (acc["doi_enriched"] or 0) / denom, 2) if denom else 0.0

        # ROR: ror_enriched / (ror_missing + ror_enriched)
        if "ror_enriched" in acc and "ror_missing" in acc and "ror_enrich_rate" in rate_keys:
            denom = (acc["ror_missing"] or 0) + (acc["ror_enriched"] or 0)
            acc["ror_enrich_rate"] = round(100.0 * (acc["ror_enriched"] or 0) / denom, 2) if denom else 0.0

        # real4 ì „/í›„ ê²°ì¸¡ì„ ì“°ì‹œëŠ” ê²½ìš°(ì„ íƒ): ë³´ê°• ìˆ˜/ë¥  íŒŒìƒ
        if "ror_missing_before_extract" in acc and "ror_missing_after_extract" in acc:
            fixed = max(0, (acc["ror_missing_before_extract"] or 0) - (acc["ror_missing_after_extract"] or 0))
            acc.setdefault("ror_enriched", 0)
            acc["ror_enriched"] += fixed
            acc.setdefault("ror_missing", 0)
            acc["ror_missing"] = max(0, (acc["ror_missing"] or 0) - fixed)
            denom = (acc["ror_missing"] or 0) + (acc["ror_enriched"] or 0)
            if "ror_enrich_rate" in rate_keys:
                acc["ror_enrich_rate"] = round(100.0 * (acc["ror_enriched"] or 0) / denom, 2) if denom else 0.0

        return acc, found

    # 1) ì „ì²´ ìˆ˜ì§‘ ìˆ˜ & authorships=[] ì‚­ì œ ìˆ˜ â€”â€” metrics â†’ JSON â†’ (ë¹„-exportë§Œ) CSV
    acc, found = sum_metrics([
        "total_collected",
        "final_csv_rows",
        "authorships_removed_empty_list",
        "doi_missing", "doi_enriched", "doi_enrich_rate",
        "ror_missing", "ror_enriched", "ror_enrich_rate",
        "ror_missing_after_extract",
    ], search_dirs)

    if found:
        total_collected = acc.get("total_collected")
        removed_authorships_empty = acc.get("authorships_removed_empty_list", None)
        doi_missing = acc.get("doi_missing", None)
        doi_enriched = acc.get("doi_enriched", None)
        doi_enrich_rate = acc.get("doi_enrich_rate", None)
        ror_missing = acc.get("ror_missing", None)
        ror_enriched = acc.get("ror_enriched", None)
        ror_enrich_rate = acc.get("ror_enrich_rate", None)
        ror_missing_after = acc.get("ror_missing_after_extract", None)

    # (B) JSON í•©ë³¸ì—ì„œ ì¶”ë¡  (í˜„ì¬ í´ë” + ë‹¤ë¥¸ í›„ë³´ í´ë”ê¹Œì§€)
    def _try_load_json(p: Path):
        try:
            works = json.loads(p.read_text(encoding="utf-8")) or []
            return works
        except Exception:
            return None

    works = None
    if json_merged.exists():
        works = _try_load_json(json_merged)

    if works is None:
        fname_json = f"{prefix_all}_{y0}_{y1}.json" if (prefix_all and y0 is not None and y1 is not None) else None
        if fname_json:
            for d in search_dirs:
                cand = Path(d) / fname_json
                if cand.exists():
                    works = _try_load_json(cand)
                    if works is not None:
                        break

    if total_collected is None and works is not None:
        total_collected = len(works)

    if removed_authorships_empty is None and works is not None:
        removed_authorships_empty = sum(
            1 for w in works
            if not isinstance(w.get("authorships"), list) or len(w.get("authorships") or []) == 0
        )

    # (C) ìµœí›„ í´ë°±: CSV ê¸¸ì´ â€” ë‹¨, export/summary íŒŒì¼ì€ ê¸ˆì§€
    if total_collected is None and not is_export:
        try:
            if csv_file.exists():
                total_collected = len(pd.read_csv(csv_file))
        except Exception:
            pass

    # 2) DOI ê²°ì¸¡(ìµœì¢…) + ë³´ê°•(í•©ì‚°) + ë³´ê°•ë¥ 
    doi_missing_final = None
    if csv_file.exists():
        try:
            df_csv = pd.read_csv(csv_file, dtype={"doi": str})
            doi_missing_final = df_csv["doi"].isna().sum() + (df_csv["doi"].astype(str).str.strip() == "").sum()
        except Exception:
            pass

    # 3) ROR ê²°ì¸¡(ì „/í›„) + ë³´ê°• + ë³´ê°•ë¥ 
    ror_missing_before = None
    try:
        if csv_file.exists():
            tmp = pd.read_csv(csv_file, dtype={"authorships": str})
            if "authorships" in tmp.columns:
                rx = re.compile(r'https?://ror\.org/[0-9a-z]+', re.I)
                ror_before = tmp["authorships"].astype(str).apply(
                    lambda s: list(dict.fromkeys(rx.findall(s)))
                )
                ror_missing_before = int((ror_before.str.len() == 0).sum())
    except Exception:
        pass

    ror_augmented_rows = None
    ror_augment_rate = None
    if (ror_missing_before is not None) and (ror_missing_after is not None):
        try:
            ror_augmented_rows = max(0, int(ror_missing_before) - int(ror_missing_after))
            ror_augment_rate = (((ror_augmented_rows / ror_missing_before) if ror_missing_before > 0 else 0.0) * 100.0)
        except Exception:
            ror_augmented_rows = None
            ror_augment_rate = None

    # 4) ë…¸ë“œÂ·ì—£ì§€ ìˆ˜ (ì¤‘ì•™ì„± CSV ìˆìœ¼ë©´ í•´ë‹¹ ë…¸ë“œ ì§‘í•©ìœ¼ë¡œ í•„í„°)
    cent_csv = _find_centrality_csvs(final_csv_path)
    def _to_pairs(names):
        import ast
        arr = []
        if isinstance(names, str):
            s = names.strip()
            if s:
                try:
                    v = ast.literal_eval(s)
                    arr = list(v) if isinstance(v, (list, tuple)) else [v]
                except Exception:
                    try:
                        v = json.loads(s)
                        arr = v if isinstance(v, list) else [v]
                    except Exception:
                        for sep in [";", "|", ","]:
                            if sep in s:
                                arr = [t.strip() for t in s.split(sep) if t.strip()]
                                break
                        if not arr:
                            arr = [s]
        elif isinstance(names, list):
            arr = names

        arr = [str(a) for a in arr if a]
        pairs = []
        for i in range(len(arr)):
            for j in range(i+1, len(arr)):
                a,b = sorted((arr[i], arr[j]))
                pairs.append((a,b))
        return pairs

    node_count = edge_count = None
    try:
        df_final = pd.read_csv(final_csv)
        all_pairs=[]
        if "org_names" in df_final.columns:
            for pr in df_final["org_names"].apply(_to_pairs):
                all_pairs.extend(pr)
        if cent_csv is not None:
            cent = pd.read_csv(cent_csv)
            node_col = "node" if "node" in cent.columns else cent.columns[0]
            node_set = set(cent[node_col].astype(str).tolist())
            node_count = len(node_set)
            edge_count = len({(a,b) for (a,b) in all_pairs if a in node_set and b in node_set})
        else:
            nodes = {n for p in all_pairs for n in p}
            node_count = len(nodes)
            edge_count = len(set(all_pairs))
    except Exception:
        node_count = edge_count = None

    _ror_enriched_fallback = (None if (ror_missing_before is None or ror_missing_after is None)
                              else max(0, ror_missing_before - ror_missing_after))
    _ror_enrich_rate_fallback = (None if (ror_missing_before in (None, 0)) else
                                 ((_ror_enriched_fallback or 0) / ror_missing_before * 100))

    doi_missing_sum   = acc.get("doi_missing", None)
    doi_enriched_sum  = acc.get("doi_enriched", None)
    doi_rate_from_acc = acc.get("doi_enrich_rate", None)

    if doi_missing_sum in (None, 0):
        doi_missing_show = doi_missing_final
    else:
        doi_missing_show = doi_missing_sum

    doi_rate_calc = None
    if (doi_missing_sum not in (None, 0)) and (doi_enriched_sum is not None):
        try:
            doi_rate_calc = round(float(doi_enriched_sum) / float(doi_missing_sum) * 100, 2)
        except Exception:
            doi_rate_calc = None

    df_out = pd.DataFrame([{
        "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": total_collected,
        "Editorial Material ì‚­ì œ ìˆ˜": removed_authorships_empty,
        "DOI ê²°ì¸¡ ìˆ˜": doi_missing_show,
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": (0 if doi_enriched_sum is None else doi_enriched_sum),
        "DOI ë³´ê°•ë¥ ": (
            doi_rate_calc if doi_rate_calc is not None
            else (None if doi_rate_from_acc is None else round(float(doi_rate_from_acc), 2))
        ),
        "ROR ID ê²°ì¸¡ ìˆ˜": ror_missing_before,
        "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": (ror_enriched if ror_enriched is not None else _ror_enriched_fallback),
        "ROR ID ë³´ê°•ë¥ ": (None if (ror_enrich_rate is None and _ror_enrich_rate_fallback is None)
                          else round(float(ror_enrich_rate if ror_enrich_rate is not None else _ror_enrich_rate_fallback), 2)),
        "ë…¸ë“œ ìˆ˜": node_count,
        "ì—£ì§€ ìˆ˜": edge_count,
    }])

    def _fmt_pct(v):
        try:
            return f"{float(v):.2f}%"
        except Exception:
            return "0.00%"

    for col in ["DOI ë³´ê°•ë¥ ", "ROR ID ë³´ê°•ë¥ "]:
        if col in df_out.columns:
            df_out[col] = df_out[col].apply(_fmt_pct)

    for col in ["ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜","Editorial Material ì‚­ì œ ìˆ˜","DOI ê²°ì¸¡ ìˆ˜","DOI ë³´ê°• ìˆ˜(í•©ì‚°)",
                "ROR ID ê²°ì¸¡ ìˆ˜","ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)","ë…¸ë“œ ìˆ˜","ì—£ì§€ ìˆ˜"]:
        if col in df_out.columns:
            df_out[col] = df_out[col].apply(lambda v: 0 if v is None else v)

    return df_out


# === ìƒˆ í—¬í¼: ì¤‘ì‹¬ì„±ë³„ Top ë…¸ë“œ & ì—£ì§€ í‘œ ===
def _summary_top_nodes_and_edges(final_csv_path: str):

    cent_csv = _find_centrality_csvs(final_csv_path)
    if cent_csv is None:
        return pd.DataFrame(), pd.DataFrame()

    cent = pd.read_csv(cent_csv)
    node_col = "node" if "node" in cent.columns else cent.columns[0]
    candidates = [c for c in ["degree","eigenvector","betweenness","closeness"] if c in cent.columns]
    if not candidates:
        # ë…¸ë“œëª…ë§Œ ìˆëŠ” ê²½ìš°
        return pd.DataFrame(), pd.DataFrame()

    # Top ë…¸ë“œ í‘œ ë§Œë“¤ê¸°
    top_nodes = {}
    for c in candidates:
        tmp = cent[[node_col, c]].dropna()
        tmp = tmp.sort_values(c, ascending=False)
        names = tmp[node_col].astype(str).head(10).tolist()
        # ì´ë¦„ë§Œ ë³´ì—¬ë‹¬ë¼ëŠ” ìš”êµ¬ì— ë§ì¶° ì ìˆ˜ëŠ” ì œì™¸
        top_nodes[c] = [*names, *([""]*(10-len(names)))]

    df_nodes = pd.DataFrame(top_nodes, index=[f"Top {i}" for i in range(1,11)])


    def _pairs_from_org_names(s):
        import ast
        arr = []
        if isinstance(s, str):
            t = s.strip()
            if t:
                try:
                    v = ast.literal_eval(t)
                    arr = list(v) if isinstance(v, (list, tuple)) else [v]
                except Exception:
                    try:
                        v = json.loads(t)
                        arr = v if isinstance(v, list) else [v]
                    except Exception:
                        for sep in [";", "|", ","]:
                            if sep in t:
                                arr = [x.strip() for x in t.split(sep) if x.strip()]
                                break
                        if not arr:
                            arr = [t]
        elif isinstance(s, list):
            arr = s

        arr = [str(a) for a in arr if a]
        pairs=[]
        for i in range(len(arr)):
            for j in range(i+1,len(arr)):
                a,b = sorted((arr[i],arr[j]))
                pairs.append((a,b))
        return pairs


    final_df = pd.read_csv(final_csv_path)
    all_pairs=[]
    if "org_names" in final_df.columns:
        final_df["org_names"].astype(str).apply(lambda s: all_pairs.extend(_pairs_from_org_names(s)))

    # ë…¸ë“œ ì ìˆ˜ map
    score_map = {c: dict(zip(cent[node_col].astype(str), cent[c])) for c in candidates}

    # ì¤‘ì‹¬ì„±ë³„ë¡œ ì—£ì§€ ì ìˆ˜ = ë…¸ë“œ ì ìˆ˜ í•©(ê°„ë‹¨ ê·¼ì‚¬)
    top_edges = {}
    for c in candidates:
        sm = score_map[c]
        scored=[]
        for a,b in set(all_pairs):
            s = float(sm.get(a,0)) + float(sm.get(b,0))
            scored.append((s, f"{a} - {b}"))
        scored.sort(key=lambda x:x[0], reverse=True)
        names = [name for _,name in scored[:10]]
        top_edges[c] = [*names, *([""]*(10-len(names)))]
    df_edges = pd.DataFrame(top_edges, index=[f"Top {i}" for i in range(1,11)])
    return df_nodes, df_edges





def sidebar_controls():
    st.sidebar.header("ì„¤ì •")
    issns_text = st.sidebar.text_area(
        "ISSN ë¦¬ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
        value="0043-1354\n0011-9164\n0733-9429",
        height=100,
    )
    issns = [s.strip() for s in issns_text.splitlines() if s.strip()]

    year_col1, year_col2 = st.sidebar.columns(2)
    year_start = year_col1.number_input("ì‹œì‘ ì—°ë„", value=2015, step=1)
    year_end = year_col2.number_input("ì¢…ë£Œ ì—°ë„", value=2024, step=1)

    email = st.sidebar.text_input("OpenAlex ì´ë©”ì¼", value="s0124kw@gmail.com")
    run_btn = st.sidebar.button("ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰", width="stretch")
    return issns, int(year_start), int(year_end), email, run_btn


def app():
    # st.title("Paper Metadata Collection Pipeline")
    st.title("Paper-Based Inter-Institutional Connectivity Analysis Pipeline")
    if "runs" not in st.session_state:
        st.session_state.runs = []

    issns, year_start, year_end, email, run_btn = sidebar_controls()

    # ì§„í–‰ë¥ /ë©”ì‹œì§€ ìŠ¬ë¡¯ -> ì§„í–‰ì¤‘ì¸ ìƒí™© ì¶œë ¥
    progress_bar = st.progress(0, text="ëŒ€ê¸° ì¤‘")
    progress_text = st.empty()

    # Preview í–‰ ìˆ˜(ê³ ì •)
    PREVIEW_N = 1000
    # CSV ë‹¤ìš´ë¡œë“œ ì„ê³„ê°’(MB)
    THRESHOLD_MB = 150

    # === ê³µí†µ Summary ë Œë”ë§ í•¨ìˆ˜ ===
    def _render_summary_block(sel_csv: str):
        with st.expander("Summary", expanded=True):
            # 1) ê³µí†µ ìš”ì•½í‘œ
            st.subheader("ê³µí†µ ìš”ì•½ ì§€í‘œ")
            summary_df = _summary_metrics_common(sel_csv)
            st.dataframe(summary_df, width="stretch")

            # 2) Summary CSV ë‹¤ìš´ë¡œë“œ (index=False, UTF-8-SIG)
            csv_bytes = summary_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "ë‹¤ìš´ë¡œë“œ Summary CSV",
                data=csv_bytes,
                file_name=Path(sel_csv).stem + "_summary.csv",
                mime="text/csv",
            )

            # 3) ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ&ì—£ì§€ (ê²°í•© í…Œì´ë¸”)
            st.subheader("ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ&ì—£ì§€")
            top_nodes, top_edges = _summary_top_nodes_and_edges(sel_csv)
            if not top_nodes.empty:
                combined = pd.DataFrame(index=top_nodes.index)
                for c in top_nodes.columns:
                    combined[(c, "ë…¸ë“œ")] = top_nodes[c]
                    combined[(c, "ì—£ì§€")] = top_edges[c] if c in top_edges.columns else ""
                combined.columns = [f"{m} Â· {sub}" for m, sub in combined.columns]
                st.dataframe(combined, width="stretch")
            else:
                st.info("ì¤‘ì‹¬ì„± CSVë¥¼ ì°¾ì§€ ëª»í•´ Top ë…¸ë“œ/ì—£ì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # === ê³µí†µ ë‹¤ìš´ë¡œë“œ ë¸”ë¡(ì‹œê°í™” í¬í•¨) ===
    def _render_download_block(sel_csv: str, sel_html: str):
        csv_path = Path(sel_csv)
        file_size_mb = csv_path.stat().st_size / (1024 * 1024)

        col_csv, col_html = st.columns(2)
        with col_csv:
            if file_size_mb <= THRESHOLD_MB:
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ ({file_size_mb:.1f} MB)",
                    data=csv_path.read_bytes(),
                    file_name=csv_path.name,
                    mime="text/csv",
                    width="stretch",
                )
            else:
                buf = BytesIO()
                with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
                    zf.writestr(csv_path.name, csv_path.read_bytes())
                zip_bytes = buf.getvalue()
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ (ZIP, ì›ë³¸ {file_size_mb:.1f} MB)",
                    data=zip_bytes,
                    file_name=csv_path.stem + ".zip",
                    mime="application/zip",
                    width="stretch",
                )

        with col_html:
            # Degree ê¸°ë°˜
            deg_html = make_html_string_from_csv(str(csv_path), size_by="degree", color_by="degree")
            st.download_button(
                label="Visualization ë‹¤ìš´ë¡œë“œ (Degree-based)",
                data=deg_html.encode("utf-8"),
                file_name=csv_path.stem + "_degree_network.html",
                mime="text/html",
                width="stretch",
            )
            # Eigenvector ê¸°ë°˜
            eig_html = make_html_string_from_csv(str(csv_path), size_by="eigenvector", color_by="eigenvector")
            st.download_button(
                label="Visualization ë‹¤ìš´ë¡œë“œ (Eigenvector-based)",
                data=eig_html.encode("utf-8"),
                file_name=csv_path.stem + "_eigenvector_network.html",
                mime="text/html",
                width="stretch",
            )

    # === ê³µí†µ Preview(ìƒìœ„ 1,000í–‰) ë¸”ë¡ ===
    def _render_preview_block(sel_csv: str):
        df = pd.read_csv(sel_csv)
        total_rows = len(df)
        st.caption(
            f"âš ï¸ ì•„ë˜ í‘œëŠ” ì´ {total_rows:,}í–‰ ì¤‘ "
            f"ìƒìœ„ {min(PREVIEW_N, total_rows):,}í–‰ë§Œ Previewì…ë‹ˆë‹¤. "
            "ì „ì²´ëŠ” ì•„ë˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
        )
        st.dataframe(df.head(PREVIEW_N), width="stretch")

        # â–¼ ë„¤íŠ¸ì›Œí¬ Preview(ì„ë² ë“œ) â€” degree / eigenvector
        with st.expander("ë„¤íŠ¸ì›Œí¬ Preview (Degree / Eigenvector)", expanded=False):
            tabs = st.tabs(["Degree ë„¤íŠ¸ì›Œí¬", "Eigenvector ë„¤íŠ¸ì›Œí¬"])

            # HTML ë¬¸ìì—´ ìƒì„± (ê¸°ì¡´ í—¬í¼ ì¬ì‚¬ìš©)
            deg_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="degree",     color_by="degree")
            eig_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="eigenvector", color_by="eigenvector")

            with tabs[0]:
                components.html(deg_html, height=800, scrolling=True)

            with tabs[1]:
                components.html(eig_html, height=800, scrolling=True)

    # === ì‹¤í–‰ ì—¬ë¶€ì— ë”°ë¼ ê²°ê³¼ ë Œë” ===
    if run_btn:
        # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
        start_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

        with st.spinner("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)"):
            progress_text.info("ì¤€ë¹„ ì¤‘..")
            final_csv, final_html = cached_run(
                issns, year_start, year_end, email,
                progress_bar=progress_bar, progress_text=progress_text
            )

        # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        end_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))
        duration = end_time - start_time

        # ì„¸ì…˜ ì´ë ¥ì— ê¸°ë¡ (ë¼ë²¨: issn|ì—°ë„|ì‹œê°)
        prefix = Path(final_csv).stem.replace(f"_{year_start}_{year_end}_ror_extract_name", "")
        label = f"{prefix} | {year_start}-{year_end} | {start_time.strftime('%m/%d %H:%M')}"
        st.session_state.runs.append({
            "label": label, "csv": final_csv, "html": final_html,
            "start": start_time.strftime('%Y-%m-%d %H:%M:%S'),
            "end": end_time.strftime('%Y-%m-%d %H:%M:%S'),
            "duration": str(duration)
        })

        # ê°€ì¥ ìµœê·¼ ì‹¤í–‰ ì„ íƒ
        sel = st.session_state.runs[-1]
        st.success(f"ì‹¤í–‰ ì™„ë£Œ: {sel['label']}")
        st.sidebar.markdown("---")
        st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
        st.sidebar.write(f" ì‹œì‘ {sel['start']}")
        st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
        st.sidebar.write(f" ì†Œìš” {sel['duration']}")

        # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
        view = st.radio(
            "ë³´ê¸° ì„ íƒ",
            ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
            index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
            key="_view_radio",
            horizontal=True,
            on_change=_on_change_view,
        )
        view = st.session_state.view

        if view == "Preview":
            _render_preview_block(sel["csv"])
        if view == "ë‹¤ìš´ë¡œë“œ":
            _render_download_block(sel["csv"], sel["html"])
        if view == "Summary":
            _render_summary_block(sel["csv"])

        # âœ… Summaryê°€ í•­ìƒ ë¶™ë„ë¡(ë‹¤ìš´ë¡œë“œ í›„ rerunë˜ì–´ë„)
        # ë³´ì¡°ë¡œ ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€
        st.session_state["last_csv"] = sel["csv"]

    else:
        # ì´ì „ ì‹¤í–‰ ì´ë ¥ ì„ íƒ/í‘œì‹œ
        if st.session_state.runs:
            st.markdown("### ì´ì „ ì‹¤í–‰ ê²°ê³¼ ë³´ê¸°")
            options = [r["label"] for r in st.session_state.runs]
            selected = st.selectbox("ì‹¤í–‰ ê²°ê³¼ ì„ íƒ", options, index=len(options)-1)
            sel = next(r for r in st.session_state.runs if r["label"] == selected)

            # ì‚¬ì´ë“œë°”ì— ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
            st.sidebar.markdown("---")
            st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
            st.sidebar.write(f" ì‹œì‘ {sel['start']}")
            st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
            st.sidebar.write(f" ì†Œìš” {sel['duration']}")

            # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
            view = st.radio(
                "ë³´ê¸° ì„ íƒ",
                ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
                index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
                key="_view_radio",
                horizontal=True,
                on_change=_on_change_view,
            )
            view = st.session_state.view

            if view == "Preview":
                _render_preview_block(sel["csv"])
            if view == "ë‹¤ìš´ë¡œë“œ":
                _render_download_block(sel["csv"], sel["html"])
            if view == "Summary":
                _render_summary_block(sel["csv"])

            # ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€(ì„ íƒ ìœ ì§€ìš©)
            st.session_state["last_csv"] = sel["csv"]
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")



if __name__ == "__main__":
    app()
