import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"

import streamlit as st
import streamlit.components.v1 as components
import nest_asyncio
import pandas as pd
import datetime
from pathlib import Path
from zoneinfo import ZoneInfo
from io import BytesIO
import zipfile
import json
import ast
import csv
import re

from new import run_pipeline, make_html_from_csv, run_pipeline_cached, make_html_string_from_csv

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
now_kst = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

# ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ë°©ì§€ (async ì‚¬ìš© ì‹œ í•„ìˆ˜)
nest_asyncio.apply()


st.set_page_config(page_title="OpenAlex Paper Metadata Pipeline", layout="wide")

# ---- View state persistence (rerunì—ë„ íƒ­ ìœ ì§€) ----
if "view" not in st.session_state:
    _default = st.query_params.get("view", "Preview")
    st.session_state.view = _default if _default in ("Preview", "ë‹¤ìš´ë¡œë“œ", "Summary") else "Preview"

def _on_change_view():
    st.session_state.view = st.session_state._view_radio
    st.query_params["view"] = st.session_state.view



# ìºì‹œ ë°ì½”ë ˆì´í„° ì œê±° (íŒŒì¼ ìƒì„±/ì™¸ë¶€ I/OëŠ” ìºì‹œ ë¹„ê¶Œì¥)
def cached_run(issns, year_start, year_end, email,
               progress_bar=None, progress_text=None):

    # storage í•˜ìœ„ì— ê²°ê³¼ ì €ì¥
    final_csv_path, _ = run_pipeline_cached(
        issns=issns, year_start=year_start, year_end=year_end, email=email,
        include_only_with_abstract=False, make_html=True,
        base_dir=Path("storage")
    )

    if progress_bar:
        progress_bar.progress(0.7)
    if progress_text:
        progress_text.info("ìµœì¢… CSV ìƒì„± ë° í™•ì¸ ì¤‘ ..")

    #ìµœì¢… CSV ê²½ë¡œ ë¬¸ìì—´ -> Path ê°ì²´
    out_path = Path(final_csv_path)

    final_html = make_html_from_csv(str(out_path))

    if progress_bar:
        progress_bar.progress(1.0)
    if progress_text:
        progress_text.success("ëª¨ë“  êµ¬ê°„ ì²˜ë¦¬ ë° ìµœì¢… íŒŒì¼ ìƒì„± ì™„ë£Œ")

    st.sidebar.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ Â· {now_kst:%H:%M:%S}")

    return str(out_path), final_html

# === ìƒˆ í—¬í¼: ì„¼íŠ¸ëŸ´ë¦¬í‹° CSV ì°¾ê¸° ===
def _find_centrality_csvs(final_csv_path: str):
    from pathlib import Path
    base = Path(final_csv_path).with_suffix("")  # ..._ror_extract_name
    base_str = str(base)
    candidates = [
        f"{base_str}_centrality.csv",
        f"{base_str}_ror_extract_name_centrality.csv",
    ]
    # logsì—ì„œ workdir_tmpì— ì €ì¥ë˜ëŠ” ê²½ìš°ë„ ì»¤ë²„
    search_dirs = [base.parent, base.parent / "workdir_tmp"]
    found = None
    for d in search_dirs:
        for name in candidates:
            p = d / Path(name).name
            if p.exists():
                found = p
                break
        if found: break
    return found


def _mk_metric_pattern(prefix_all, y0, y1):
    return re.compile(rf"^{re.escape(prefix_all)}_{y0}_{y1}_metrics\.csv$", re.I)

def _summary_metrics_common_multi(sel_csv_list: list[str]):
    """
    ì—¬ëŸ¬ ì‹¤í–‰(ì—¬ëŸ¬ ì €ë„ Ã— ì—¬ëŸ¬ ì—°ë„)ì˜ ìµœì¢… CSV ëª©ë¡ì„ 'ë‹¨ì¼ Summaryì™€ ì™„ì „íˆ ë™ì¼í•œ ê³„ì‚° ë¡œì§'ìœ¼ë¡œ
    í•©ì‚°í•œ 1í–‰ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - metrics í•©ì‚°: ê° ì‹¤í–‰ í´ë”(root_dir, workdir_tmp)ë§Œ ì œí•œì ìœ¼ë¡œ ìŠ¤ìº” + íŒŒì¼ëª… 'ì™„ì „ì¼ì¹˜' í•„í„°
    - DOI/ROR ë³´ê°•ë¥ : ë‹¨ì¼ê³¼ ë™ì¼í•˜ê²Œ 'í•©ì‚°ëœ ë¶„ì/ë¶„ëª¨' ê¸°ì¤€ìœ¼ë¡œ ì¬ê³„ì‚°í•˜ì—¬ %.2f%% í¬ë§·
    - ë…¸ë“œ/ì—£ì§€: org_names ê¸°ë°˜ìœ¼ë¡œ ìœ ë‹ˆì˜¨ / ì¤‘ì•™ì„± CSV ìˆìœ¼ë©´ ê·¸ ë…¸ë“œ ì§‘í•© ìš°ì„  ì ìš© (ë‹¨ì¼ê³¼ ë™ì¼)
    - ë°˜í™˜ ì»¬ëŸ¼/ìˆœì„œ/í˜•ì‹: ë‹¨ì¼ Summary í…Œì´ë¸”ê³¼ ë™ì¼
    """
    import pandas as pd

    # 0) í—¬í¼
    def _to_num(s):
        try:
            if s is None or s == "":
                return None
            f = float(s)
            return int(f) if f.is_integer() else f
        except Exception:
            return None

    def _fmt_pct(v):
        try:
            return f"{float(v):.2f}%"
        except Exception:
            return "0.00%"

    # 1) ê° ì‹¤í–‰ì˜ (root_dir, prefix_all, y0, y1) ë©”íƒ€ ì¶”ì¶œ
    roots_with_meta = []
    csv_paths = [Path(p) for p in sel_csv_list]
    for final_csv in csv_paths:
        base_path = final_csv.with_suffix("")  # ..._final.csv â†’ ..._final
        base = str(base_path)
        # *_final[_ror/_ror_extract(_name)][_summary]? ì ‘ë¯¸ì‚¬ ì œê±°
        root_base = re.sub(r'_(?:ror(?:_extract(?:_name)?)?)(?:_summary)?$', '', base)
        m = re.search(r"_(\d{4})_(\d{4})$", root_base)
        if not m:
            continue
        y0, y1 = int(m.group(1)), int(m.group(2))
        prefix_all = Path(root_base).name[:-(len(m.group(0)))]
        root_dir = Path(root_base).parent
        roots_with_meta.append((root_dir, prefix_all, y0, y1))

    # 2) metrics í•©ì‚°(ì •í™• ì¼ì¹˜ íŒŒì¼ë§Œ, í˜„ì¬ ì‹¤í–‰ í´ë”ë§Œ ìŠ¤ìº”)
    keys = [
        "total_collected", "final_csv_rows", "authorships_removed_empty_list",
        "doi_missing", "doi_enriched", "doi_enrich_rate",
        "ror_missing", "ror_enriched", "ror_enrich_rate",
        "ror_missing_after_extract",
    ]
    acc = {k: 0 for k in keys if not k.endswith("_rate")}
    pats = [_mk_metric_pattern(prefix, y0, y1) for (root, prefix, y0, y1) in roots_with_meta]
    search_dirs = []
    for (root, *_meta) in roots_with_meta:
        root = Path(root)
        search_dirs += [root, root / "workdir_tmp"]

    seen = set()
    for base in search_dirs:
        base = Path(base)
        if not base.exists():
            continue
        for mf in base.rglob("*_metrics.csv"):
            rp = mf.resolve()
            if rp in seen:
                continue
            name = mf.name
            # â˜… ì„ íƒëœ ì‹¤í–‰ë“¤ì˜ metricsë§Œ: íŒŒì¼ëª… 'ì™„ì „ì¼ì¹˜'
            if not any(pat.match(name) for pat in pats):
                continue
            seen.add(rp)
            try:
                import csv
                with mf.open("r", encoding="utf-8", newline="") as f:
                    rdr = csv.reader(f)
                    _ = next(rdr, None)  # header skip
                    for row in rdr:
                        if len(row) < 2:
                            continue
                        k, v = row[0], _to_num(row[1])
                        if k in acc and v is not None and (not k.endswith("_rate")):
                            acc[k] += v
            except Exception:
                pass

    # 3) ROR ê²°ì¸¡(ë³´ê°• ì „) í•©ì‚° â€” ìµœì¢… CSVë“¤ì˜ authorshipsì—ì„œ ì§ì ‘ ê³„ì‚° (ë‹¨ì¼ê³¼ ë™ì¼ ë¡œì§)
    ror_missing_before_total = None
    try:
        rx = re.compile(r'https?://ror\.org/[0-9a-z]+', re.I)
        total = 0
        for csv_file in csv_paths:
            try:
                tmp = pd.read_csv(csv_file, dtype={"authorships": str})
                if "authorships" in tmp.columns:
                    before = tmp["authorships"].astype(str).apply(
                        lambda s: list(dict.fromkeys(rx.findall(s)))
                    )
                    total += int((before.str.len() == 0).sum())
            except Exception:
                pass
        ror_missing_before_total = total
    except Exception:
        pass

    # 4) ë…¸ë“œ/ì—£ì§€ â€” org_names ìœ ë‹ˆì˜¨ + ì¤‘ì•™ì„± CSV ìš°ì„  (ë‹¨ì¼ê³¼ ë™ì¼ ë¡œì§)
    node_count = edge_count = None
    try:
        all_pairs = []
        node_universe = set()
        cent_nodes = set()
        have_cent_nodes = False

        def _to_pairs(names):
            arr=[]
            if isinstance(names, str):
                s = names.strip()
                if s:
                    # literal â†’ json â†’ êµ¬ë¶„ì ìˆœìœ¼ë¡œ íŒŒì‹±
                    try:
                        v = ast.literal_eval(s)
                        arr = list(v) if isinstance(v, (list, tuple)) else [v]
                    except Exception:
                        try:
                            v = json.loads(s)
                            arr = v if isinstance(v, list) else [v]
                        except Exception:
                            for sep in [";","|",","]:
                                if sep in s:
                                    arr = [x.strip() for x in s.split(sep) if x.strip()]
                                    break
                            if not arr:
                                arr=[s]
            elif isinstance(names, list):
                arr = names

            # ë¬´í–¥ ìŒ(ì¤‘ë³µ/ìˆœì„œ ì œê±°)
            pairs=[]
            arr=[x for x in arr if x]
            for i in range(len(arr)):
                for j in range(i+1, len(arr)):
                    a,b = sorted((arr[i], arr[j]))
                    pairs.append((a,b))
            return pairs

        for csv_file in csv_paths:
            # org_names â†’ ìŒ/ë…¸ë“œ
            try:
                df_final = pd.read_csv(csv_file)
                if "org_names" in df_final.columns:
                    for pr in df_final["org_names"].apply(_to_pairs):
                        all_pairs.extend(pr)
                # í˜„ì¬ê¹Œì§€ ìŒ“ì¸ ìŒì—ì„œ ë…¸ë“œ ìœ ë‹ˆì˜¨
                node_universe.update({n for p in all_pairs for n in p})
            except Exception:
                pass

            # ì¤‘ì•™ì„± CSV ë…¸ë“œ ì§‘í•©
            try:
                cent_csv = _find_centrality_csvs(str(csv_file))  # ë‹¨ì¼ê³¼ ë™ì¼í•œ í—¬í¼ ì‚¬ìš©
                if cent_csv:
                    cent = pd.read_csv(cent_csv)
                    node_col = "node" if "node" in cent.columns else cent.columns[0]
                    cent_nodes.update(set(cent[node_col].astype(str).tolist()))
                    have_cent_nodes = True
            except Exception:
                pass

        # ë‹¤ì¤‘ ìš”ì•½ë„ í•­ìƒ 'ìµœì¢… CSV ì „ì²´' ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œ/ì—£ì§€ ìˆ˜ ê³„ì‚°
        node_set = node_universe
        edge_count = len(set(all_pairs))
        node_count = len(node_set)

    except Exception:
        node_count = edge_count = None

    # 5) DOI/ROR ë³´ê°•ë¥  ì¬ê³„ì‚°(í•©ì‚° ê¸°ì¤€) â€” ë‹¨ì¼ê³¼ ë™ì¼ í¬ë§·('%.2f%%')
    doi_missing_sum  = acc.get("doi_missing")
    doi_enriched_sum = acc.get("doi_enriched")
    ror_missing_after = acc.get("ror_missing_after_extract")
    ror_enriched_sum  = acc.get("ror_enriched")

    # (A) DOI ë³´ê°•ë¥ : í•©ì‚°ëœ ë¶„ì/ë¶„ëª¨ ê¸°ì¤€ (ë‹¨ì¼ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš© ì¤‘ì¸ ë¶„ëª¨ì— ë§ì¶° ë‘ì„¸ìš”)
    doi_rate = None
    try:
        # ë³´í¸ì ìœ¼ë¡œëŠ” (enriched / (missing + enriched)) ë˜ëŠ” (enriched / missing) ì¤‘ í•œ ë°©ì‹ì„ ì”ë‹ˆë‹¤.
        # ë‹¨ì¼ í•¨ìˆ˜ê°€ ì‚¬ìš© ì¤‘ì¸ ë¶„ëª¨ì— ë§ì¶° ì•„ë˜ í•œ ì¤„ì„ ì„ íƒí•˜ì„¸ìš”.
        # 1) enriched / (missing + enriched)
        denom = (doi_missing_sum or 0) + (doi_enriched_sum or 0)
        # 2) (ë‹¨ì¼ì´ 'missing'ë§Œì„ ë¶„ëª¨ë¡œ ì“°ëŠ” ê²½ìš°)
        # denom = (doi_missing_sum or 0)
        doi_rate = (100.0 * (doi_enriched_sum or 0) / denom) if denom else 0.0
    except Exception:
        doi_rate = None

    # (B) ROR ë³´ê°•ë¥ : í•©ì‚°ëœ ë¶„ì/ë¶„ëª¨ ê¸°ì¤€
    #    - ë¶„ì: ror_enriched_sum ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    #    - ì—†ìœ¼ë©´ (ë³´ê°• ì „ ê²°ì¸¡ - ë³´ê°• í›„ ê²°ì¸¡)ì„ ì‚¬ìš© (ë‹¨ì¼ê³¼ ë™ì¼í•œ fallback)
    ror_enriched_calc = ror_enriched_sum
    if ror_enriched_calc in (None, 0) and (ror_missing_before_total is not None) and (ror_missing_after is not None):
        ror_enriched_calc = max(0, ror_missing_before_total - ror_missing_after)

    ror_rate = None
    try:
        # ë‹¨ì¼ ê³„ì‚°ê³¼ ë™ì¼í•œ ë¶„ëª¨ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        # 1) enriched / (missing + enriched)
        denom_r = (acc.get("ror_missing") or 0) + (ror_enriched_calc or 0)
        # 2) (ë‹¨ì¼ì´ 'missing'ë§Œì„ ë¶„ëª¨ë¡œ ì“°ëŠ” ê²½ìš°)
        # denom_r = (acc.get("ror_missing") or 0)
        ror_rate = (100.0 * (ror_enriched_calc or 0) / denom_r) if denom_r else 0.0
    except Exception:
        ror_rate = None

    # 6) ë‹¨ì¼ Summaryì™€ 'ë™ì¼' ì»¬ëŸ¼/ìˆœì„œ/í¬ë§·ìœ¼ë¡œ í–‰ êµ¬ì„±
    data = {
        "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": acc.get("total_collected"),
        "Editorial Material ì‚­ì œ ìˆ˜": acc.get("authorships_removed_empty_list"),
        "DOI ê²°ì¸¡ ìˆ˜": doi_missing_sum,
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": (doi_enriched_sum or 0) if doi_enriched_sum is not None else None,
        "DOI ë³´ê°•ë¥ ": (None if doi_rate is None else _fmt_pct(doi_rate)),
        "ROR ID ê²°ì¸¡ ìˆ˜": ror_missing_before_total,
        "ROR ID ë³´ê°• ìˆ˜": ror_enriched_calc,
        "ROR ID ë³´ê°•ë¥ ": (None if ror_rate is None else _fmt_pct(ror_rate)),
        "ë…¸ë“œ ìˆ˜": node_count,
        "ì—£ì§€ ìˆ˜": edge_count,
    }
    df_out = pd.DataFrame([data])

    return df_out


# === ìƒˆ í—¬í¼: ê³µí†µ ìš”ì•½í‘œ ===
def _summary_metrics_common(final_csv_path: str):
    import csv, json, re
    from pathlib import Path
    import pandas as pd

    # ---- 0) ê²½ë¡œ/ë©”íƒ€ íŒŒì‹± (ë¨¼ì € í•´ì•¼ í•¨)
    final_csv = Path(final_csv_path)
    base_path = final_csv.with_suffix("")
    base = str(base_path)

    # ì ‘ë¯¸ì‚¬ ì œê±°í•´ ë£¨íŠ¸ë² ì´ìŠ¤ ì¶”ì¶œ
    root_base = re.sub(r'_(?:ror(?:_extract(?:_name)?)?)(?:_summary)?$', '', base)
    json_merged = Path(f"{root_base}.json")
    csv_file    = final_csv

    # export/summary íŒŒì¼ì´ë©´ CSV ê¸¸ì´ í´ë°± ê¸ˆì§€
    is_export = final_csv.name.endswith("_export.csv") or final_csv.name.endswith("_summary.csv")

    # ì ‘ë‘ì‚¬/ì—°ë„ ì¶”ì¶œ
    m = re.search(r"_(\d{4})_(\d{4})$", root_base)
    y0 = y1 = None
    prefix_all = ""
    if m:
        y0, y1 = int(m.group(1)), int(m.group(2))
        prefix_all = Path(root_base).name[:-(len(m.group(0)))]

    # ê²€ìƒ‰ ë””ë ‰í„°ë¦¬
    root_dir = Path(root_base).parent
    search_dirs = [root_dir, root_dir / "workdir_tmp"]

    # ---- 1) metrics.csv í•©ì‚° í•¨ìˆ˜ (ì´ì œ ì •ì˜ â†’ ì´í›„ í˜¸ì¶œ)
    def sum_metrics(keys, search_dirs, allowed_names=None):
        acc = {k: 0 for k in keys if not k.endswith("_rate")}
        rate_keys = [k for k in keys if k.endswith("_rate")]
        found = False
        seen = set()

        def _to_num(s):
            try:
                if s is None or s == "":
                    return None
                f = float(s)
                return int(f) if f.is_integer() else f
            except Exception:
                return None

        for base in search_dirs:
            base = Path(base)
            if not base.exists():
                continue
            for mf in base.rglob("*_metrics.csv"):
                if allowed_names and (mf.name not in allowed_names):
                    continue
                rp = mf.resolve()
                if rp in seen:
                    continue
                seen.add(rp)
                try:
                    with mf.open("r", encoding="utf-8", newline="") as f:
                        rdr = csv.reader(f)
                        _ = next(rdr, None)  # ["key","value"] í—¤ë” ìŠ¤í‚µ í—ˆìš©
                        for row in rdr:
                            if len(row) < 2:
                                continue
                            k, v = row[0], _to_num(row[1])
                            if (k in acc) and (v is not None) and (not k.endswith("_rate")):
                                acc[k] += v
                    found = True
                except Exception:
                    pass

        # í•©ê³„ë¡œ ë¹„ìœ¨ ì¬ê³„ì‚°: (enriched / (missing + enriched)) Ã— 100
        if "doi_enriched" in acc and "doi_missing" in acc and "doi_enrich_rate" in rate_keys:
            denom = (acc["doi_missing"] or 0) + (acc["doi_enriched"] or 0)
            acc["doi_enrich_rate"] = round(100.0 * (acc["doi_enriched"] or 0) / denom, 2) if denom else 0.0

        if "ror_enriched" in acc and "ror_missing" in acc and "ror_enrich_rate" in rate_keys:
            denom = (acc["ror_missing"] or 0) + (acc["ror_enriched"] or 0)
            acc["ror_enrich_rate"] = round(100.0 * (acc["ror_enriched"] or 0) / denom, 2) if denom else 0.0

        # real4 ì „/í›„ ê²°ì¸¡ì´ ìˆìœ¼ë©´ íŒŒìƒì¹˜ë¡œ ror_enriched ë³´ì •
        if "ror_missing_before_extract" in acc and "ror_missing_after_extract" in acc:
            fixed = max(0, (acc["ror_missing_before_extract"] or 0) - (acc["ror_missing_after_extract"] or 0))
            acc.setdefault("ror_enriched", 0)
            acc["ror_enriched"] += fixed
            acc.setdefault("ror_missing", 0)
            acc["ror_missing"] = max(0, (acc["ror_missing"] or 0) - fixed)
            denom = (acc["ror_missing"] or 0) + (acc["ror_enriched"] or 0)
            if "ror_enrich_rate" in rate_keys:
                acc["ror_enrich_rate"] = round(100.0 * (acc["ror_enriched"] or 0) / denom, 2) if denom else 0.0

        return acc, found

    # ---- 2) metrics.csv í•©ì‚° (ì´ë²ˆ ì‹¤í–‰ì˜ íŒŒì¼ëª…ë§Œ í—ˆìš©)
    allowed_names = set()
    if prefix_all and (y0 is not None) and (y1 is not None):
        allowed_names = {f"{prefix_all}_{yy}_{yy}_metrics.csv" for yy in range(int(y0), int(y1) + 1)}

    acc, found = sum_metrics([
        "total_collected","final_csv_rows","authorships_removed_empty_list",
        "doi_missing","doi_enriched","doi_enrich_rate",
        "ror_missing","ror_enriched","ror_enrich_rate",
        "ror_missing_after_extract",
    ], search_dirs, allowed_names=allowed_names)

    # ---- 3) í•©ì‚° ê²°ê³¼ ë³€ìˆ˜ë“¤
    total_collected = acc.get("total_collected") if found else None
    removed_authorships_empty = acc.get("authorships_removed_empty_list") if found else None
    doi_missing_sum   = acc.get("doi_missing") if found else None
    doi_enriched_sum  = acc.get("doi_enriched") if found else None
    doi_rate_from_acc = acc.get("doi_enrich_rate") if found else None
    ror_missing_sum   = acc.get("ror_missing") if found else None
    ror_enriched_sum  = acc.get("ror_enriched") if found else None
    ror_rate_from_acc = acc.get("ror_enrich_rate") if found else None
    ror_missing_after = acc.get("ror_missing_after_extract") if found else None

    # ---- 4) JSON í´ë°± (metricsê°€ ì—†ì„ ë•Œë§Œ)
    def _try_load_json(p: Path):
        try:
            return json.loads(p.read_text(encoding="utf-8")) or []
        except Exception:
            return None

    works = None
    if total_collected is None:
        if json_merged.exists():
            works = _try_load_json(json_merged)
        if works is None and prefix_all and (y0 is not None) and (y1 is not None):
            fname_json = f"{prefix_all}_{y0}_{y1}.json"
            for d in search_dirs:
                cand = Path(d) / fname_json
                if cand.exists():
                    works = _try_load_json(cand)
                    if works is not None:
                        break
        if works is not None:
            total_collected = len(works)
            removed_authorships_empty = sum(
                1 for w in works
                if not isinstance(w.get("authorships"), list) or len(w.get("authorships") or []) == 0
            )

    # ---- 5) ìµœí›„ í´ë°±: CSV ê¸¸ì´ (export/summaryë©´ ê¸ˆì§€)
    if total_collected is None and not is_export:
        try:
            if csv_file.exists():
                total_collected = len(pd.read_csv(csv_file))
        except Exception:
            pass

    # ---- 6) ìµœì¢… CSVì—ì„œ ì§ì ‘ ì„¼ ê°’(ì°¸ê³ /í´ë°±ìš©)
    doi_missing_final = None
    if csv_file.exists():
        try:
            df_csv = pd.read_csv(csv_file, dtype={"doi": str})
            doi_missing_final = df_csv["doi"].isna().sum() + (df_csv["doi"].astype(str).str.strip() == "").sum()
        except Exception:
            pass

    # ROR ë³´ê°• ì „ ê²°ì¸¡(ìµœì¢… CSVì˜ authorshipsì—ì„œ ì¶”ì •)
    ror_missing_before = None
    try:
        if csv_file.exists():
            tmp = pd.read_csv(csv_file, dtype={"authorships": str})
            if "authorships" in tmp.columns:
                rx = re.compile(r'https?://ror\.org/[0-9a-z]+', re.I)
                ror_before = tmp["authorships"].astype(str).apply(
                    lambda s: list(dict.fromkeys(rx.findall(s)))
                )
                ror_missing_before = int((ror_before.str.len() == 0).sum())
    except Exception:
        pass

    # ROR ë³´ê°• ìˆ˜/ë³´ê°•ë¥  í´ë°± ê³„ì‚°
    _ror_enriched_fallback = (None if (ror_missing_before is None or ror_missing_after is None)
                              else max(0, ror_missing_before - ror_missing_after))
    _ror_enrich_rate_fallback = (None if (ror_missing_before in (None, 0)) else
                                 ((_ror_enriched_fallback or 0) / ror_missing_before * 100))

    # ---- 7) DOI ê²°ì¸¡ í‘œì‹œ: metricsê°€ ìˆìœ¼ë©´ ê·¸ ê°’, ì—†ìœ¼ë©´ ìµœì¢… CSV
    if doi_missing_sum is None:
        doi_missing_show = doi_missing_final   # íŒŒì¼ì„ ëª» ì½ì—ˆì„ ë•Œë§Œ CSVë¡œ ëŒ€ì²´
    else:
        doi_missing_show = doi_missing_sum

    # ---- 8) DOI ë³´ê°•ë¥  ê³„ì‚°(ì¼ê´€í™”): (enriched / (missing + enriched)) Ã— 100
    doi_rate_calc = None
    if (doi_missing_sum is not None) and (doi_enriched_sum is not None):
        denom = (doi_missing_sum or 0) + (doi_enriched_sum or 0)
        doi_rate_calc = round(100.0 * (doi_enriched_sum or 0) / denom, 2) if denom else 0.0

    # ---- 9) ë…¸ë“œÂ·ì—£ì§€ ìˆ˜ (ì¤‘ì•™ì„± CSV ìˆìœ¼ë©´ ê·¸ ë…¸ë“œ ì§‘í•© ìš°ì„ )
    cent_csv = _find_centrality_csvs(final_csv_path)
    def _to_pairs(names):
        arr = []
        if isinstance(names, str):
            s = names.strip()
            if s:
                try:
                    v = ast.literal_eval(s)
                    arr = list(v) if isinstance(v, (list, tuple)) else [v]
                except Exception:
                    try:
                        v = json.loads(s)
                        arr = v if isinstance(v, list) else [v]
                    except Exception:
                        for sep in [";", "|", ","]:
                            if sep in s:
                                arr = [t.strip() for t in s.split(sep) if t.strip()]
                                break
                        if not arr:
                            arr = [s]
        elif isinstance(names, list):
            arr = names
        arr = [str(a) for a in arr if a]
        pairs = []
        for i in range(len(arr)):
            for j in range(i+1, len(arr)):
                a,b = sorted((arr[i], arr[j]))
                pairs.append((a,b))
        return pairs

    node_count = edge_count = None
    try:
        df_final = pd.read_csv(final_csv)
        all_pairs=[]
        if "org_names" in df_final.columns:
            for pr in df_final["org_names"].apply(_to_pairs):
                all_pairs.extend(pr)
        # ì¤‘ì•™ì„± CSVê°€ ìˆì–´ë„, ìš”ì•½í‘œì˜ 'ë…¸ë“œ/ì—£ì§€ ìˆ˜'ëŠ” ìµœì¢… CSV ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        nodes = {n for p in all_pairs for n in p}
        node_count = len(nodes)
        edge_count = len(set(all_pairs))
    except Exception:
        node_count = edge_count = None

    # ---- 10) ì¶œë ¥ í…Œì´ë¸” êµ¬ì„± + í¬ë§·
    df_out = pd.DataFrame([{
        "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": total_collected,
        "Editorial Material ì‚­ì œ ìˆ˜": removed_authorships_empty,
        "DOI ê²°ì¸¡ ìˆ˜": doi_missing_show,
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": (0 if doi_enriched_sum is None else doi_enriched_sum),
        "DOI ë³´ê°•ë¥ ": (
            doi_rate_calc if doi_rate_calc is not None
            else (None if doi_rate_from_acc is None else round(float(doi_rate_from_acc), 2))
        ),
        "ROR ID ê²°ì¸¡ ìˆ˜": ror_missing_before,
        "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": (ror_enriched_sum if ror_enriched_sum is not None else _ror_enriched_fallback),
        "ROR ID ë³´ê°•ë¥ ": (None if (ror_rate_from_acc is None and _ror_enrich_rate_fallback is None)
                          else round(float(ror_rate_from_acc if ror_rate_from_acc is not None else _ror_enrich_rate_fallback), 2)),
        "ë…¸ë“œ ìˆ˜": node_count,
        "ì—£ì§€ ìˆ˜": edge_count,
    }])

    def _fmt_pct(v):
        try:
            return f"{float(v):.2f}%"
        except Exception:
            return "0.00%"

    for col in ["DOI ë³´ê°•ë¥ ", "ROR ID ë³´ê°•ë¥ "]:
        if col in df_out.columns:
            df_out[col] = df_out[col].apply(_fmt_pct)

    for col in ["ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜","Editorial Material ì‚­ì œ ìˆ˜","DOI ê²°ì¸¡ ìˆ˜","DOI ë³´ê°• ìˆ˜(í•©ì‚°)",
                "ROR ID ê²°ì¸¡ ìˆ˜","ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)","ë…¸ë“œ ìˆ˜","ì—£ì§€ ìˆ˜"]:
        if col in df_out.columns:
            df_out[col] = df_out[col].apply(lambda v: 0 if v is None else v)

    return df_out


# === ìƒˆ í—¬í¼: ì¤‘ì‹¬ì„±ë³„ Top ë…¸ë“œ & ì—£ì§€ í‘œ ===
def _summary_top_nodes_and_edges(final_csv_path: str):

    cent_csv = _find_centrality_csvs(final_csv_path)
    if cent_csv is None:
        # ì¤‘ì•™ì„± CSVê°€ ì—†ìœ¼ë©´ í•œë²ˆ ìƒì„± ì‹œë„ (ì‹¤íŒ¨í•´ë„ ì¡°ìš©íˆ íŒ¨ìŠ¤)
        try:
            from new import make_html_from_csv
            make_html_from_csv(final_csv_path)
        except Exception:
            pass
        cent_csv = _find_centrality_csvs(final_csv_path)
    if cent_csv is None:
        return pd.DataFrame(), pd.DataFrame()


    cent = pd.read_csv(cent_csv)
    node_col = "node" if "node" in cent.columns else cent.columns[0]
    candidates = [c for c in ["degree","eigenvector","betweenness","closeness"] if c in cent.columns]
    if not candidates:
        # ë…¸ë“œëª…ë§Œ ìˆëŠ” ê²½ìš°
        return pd.DataFrame(), pd.DataFrame()

    # Top ë…¸ë“œ í‘œ ë§Œë“¤ê¸°
    top_nodes = {}
    for c in candidates:
        tmp = cent[[node_col, c]].dropna()
        tmp = tmp.sort_values(c, ascending=False)
        names = tmp[node_col].astype(str).head(10).tolist()
        # ì´ë¦„ë§Œ ë³´ì—¬ë‹¬ë¼ëŠ” ìš”êµ¬ì— ë§ì¶° ì ìˆ˜ëŠ” ì œì™¸
        top_nodes[c] = [*names, *([""]*(10-len(names)))]

    df_nodes = pd.DataFrame(top_nodes, index=[f"Top {i}" for i in range(1,11)])


    def _pairs_from_org_names(s):
        import ast
        arr = []
        if isinstance(s, str):
            t = s.strip()
            if t:
                try:
                    v = ast.literal_eval(t)
                    arr = list(v) if isinstance(v, (list, tuple)) else [v]
                except Exception:
                    try:
                        v = json.loads(t)
                        arr = v if isinstance(v, list) else [v]
                    except Exception:
                        for sep in [";", "|", ","]:
                            if sep in t:
                                arr = [x.strip() for x in t.split(sep) if x.strip()]
                                break
                        if not arr:
                            arr = [t]
        elif isinstance(s, list):
            arr = s

        arr = [str(a) for a in arr if a]
        pairs=[]
        for i in range(len(arr)):
            for j in range(i+1,len(arr)):
                a,b = sorted((arr[i],arr[j]))
                pairs.append((a,b))
        return pairs


    final_df = pd.read_csv(final_csv_path)
    all_pairs=[]
    if "org_names" in final_df.columns:
        final_df["org_names"].astype(str).apply(lambda s: all_pairs.extend(_pairs_from_org_names(s)))

    # ë…¸ë“œ ì ìˆ˜ map
    score_map = {c: dict(zip(cent[node_col].astype(str), cent[c])) for c in candidates}

    # ì¤‘ì‹¬ì„±ë³„ë¡œ ì—£ì§€ ì ìˆ˜ = ë…¸ë“œ ì ìˆ˜ í•©(ê°„ë‹¨ ê·¼ì‚¬)
    top_edges = {}
    for c in candidates:
        sm = score_map[c]
        scored=[]
        for a,b in set(all_pairs):
            s = float(sm.get(a,0)) + float(sm.get(b,0))
            scored.append((s, f"{a} - {b}"))
        scored.sort(key=lambda x:x[0], reverse=True)
        names = [name for _,name in scored[:10]]
        top_edges[c] = [*names, *([""]*(10-len(names)))]
    df_edges = pd.DataFrame(top_edges, index=[f"Top {i}" for i in range(1,11)])
    return df_nodes, df_edges





def sidebar_controls():
    st.sidebar.header("ì„¤ì •")
    issns_text = st.sidebar.text_area(
        "ISSN ë¦¬ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
        value="0043-1354\n0011-9164\n0733-9429",
        height=100,
    )
    issns = [s.strip() for s in issns_text.splitlines() if s.strip()]

    year_col1, year_col2 = st.sidebar.columns(2)
    year_start = year_col1.number_input("ì‹œì‘ ì—°ë„", value=2015, step=1)
    year_end = year_col2.number_input("ì¢…ë£Œ ì—°ë„", value=2024, step=1)

    email = st.sidebar.text_input("OpenAlex ì´ë©”ì¼", value="s0124kw@gmail.com")
    run_btn = st.sidebar.button("ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰", width="stretch")
    return issns, int(year_start), int(year_end), email, run_btn


def app():
    # st.title("Paper Metadata Collection Pipeline")
    st.title("Paper-Based Inter-Institutional Connectivity Analysis Pipeline")
    if "runs" not in st.session_state:
        st.session_state.runs = []
    # --- NEW: ì‹¤í–‰ ìƒíƒœ ê¸°ë³¸ê°’ ---
    if "_job_state" not in st.session_state:
        st.session_state._job_state = "idle"   # idle | running | done
    if "_job" not in st.session_state:
        st.session_state._job = None
    if "view" not in st.session_state:
        st.session_state.view = "Preview"

    issns, year_start, year_end, email, run_btn = sidebar_controls()

    # ì§„í–‰ë¥ /ë©”ì‹œì§€ ìŠ¬ë¡¯ -> ì§„í–‰ì¤‘ì¸ ìƒí™© ì¶œë ¥
    progress_bar = st.progress(0, text="ëŒ€ê¸° ì¤‘")
    progress_text = st.empty()

    # Preview í–‰ ìˆ˜(ê³ ì •)
    PREVIEW_N = 1000
    # CSV ë‹¤ìš´ë¡œë“œ ì„ê³„ê°’(MB)
    THRESHOLD_MB = 150

    # === ê³µí†µ Summary ë Œë”ë§ í•¨ìˆ˜ ===
    def _render_summary_block(sel_csv: str):
        with st.expander("Summary", expanded=True):
            # 1) ê³µí†µ ìš”ì•½í‘œ
            st.subheader("ê³µí†µ ìš”ì•½ ì§€í‘œ")
            summary_df = _summary_metrics_common(sel_csv)
            st.dataframe(summary_df, width="stretch")

            # 2) Summary CSV ë‹¤ìš´ë¡œë“œ (index=False, UTF-8-SIG)
            csv_bytes = summary_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "ë‹¤ìš´ë¡œë“œ Summary CSV",
                data=csv_bytes,
                file_name=Path(sel_csv).stem + "_summary.csv",
                mime="text/csv",
            )

            # 3) ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ&ì—£ì§€ (ê²°í•© í…Œì´ë¸”)
            st.subheader("ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ&ì—£ì§€")
            top_nodes, top_edges = _summary_top_nodes_and_edges(sel_csv)
            if not top_nodes.empty:
                combined = pd.DataFrame(index=top_nodes.index)
                for c in top_nodes.columns:
                    combined[(c, "ë…¸ë“œ")] = top_nodes[c]
                    combined[(c, "ì—£ì§€")] = top_edges[c] if c in top_edges.columns else ""
                combined.columns = [f"{m} Â· {sub}" for m, sub in combined.columns]
                st.dataframe(combined, width="stretch")
            else:
                st.info("ì¤‘ì‹¬ì„± CSVë¥¼ ì°¾ì§€ ëª»í•´ Top ë…¸ë“œ/ì—£ì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def _render_summary_unified(sel):
        # sel: í˜„ì¬ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒëœ ì‹¤í–‰ dict(ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        all_csvs = [r["csv"] for r in st.session_state.runs] if "runs" in st.session_state else [sel["csv"]]
        if len(all_csvs) > 1:
            # ì—¬ëŸ¬ ì‹¤í–‰ ë¡œë“œ â†’ í•©ì‚° Summaryë¥¼ 'ê¸°ì¡´ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê·¸ëŒ€ë¡œ' ì¶œë ¥
            df = _summary_metrics_common_multi(all_csvs)  # ì´ì „ì— ì¶”ê°€í•œ í•©ì‚° í•¨ìˆ˜
            st.subheader("Summary")
            st.dataframe(df, use_container_width=True)
            st.download_button(
                "ë‹¤ìš´ë¡œë“œ Summary CSV",
                data=df.to_csv(index=False).encode("utf-8-sig"),
                file_name="summary.csv",
                mime="text/csv",
            )
        else:
            # ë‹¨ì¼ ì‹¤í–‰ â†’ ê¸°ì¡´ ë‹¨ì¼ Summary ê·¸ëŒ€ë¡œ
            _render_summary_block(sel["csv"])


    # === ê³µí†µ ë‹¤ìš´ë¡œë“œ ë¸”ë¡(ì‹œê°í™” í¬í•¨) ===
    def _render_download_block(sel_csv: str, sel_html: str):
        csv_path = Path(sel_csv)
        file_size_mb = csv_path.stat().st_size / (1024 * 1024)

        col_csv, col_html = st.columns(2)
        with col_csv:
            if file_size_mb <= THRESHOLD_MB:
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ ({file_size_mb:.1f} MB)",
                    data=csv_path.read_bytes(),
                    file_name=csv_path.name,
                    mime="text/csv",
                    width="stretch",
                )
            else:
                buf = BytesIO()
                with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
                    zf.writestr(csv_path.name, csv_path.read_bytes())
                zip_bytes = buf.getvalue()
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ (ZIP, ì›ë³¸ {file_size_mb:.1f} MB)",
                    data=zip_bytes,
                    file_name=csv_path.stem + ".zip",
                    mime="application/zip",
                    width="stretch",
                )

        with col_html:
            # ë„¤ ê°€ì§€ ì¤‘ì‹¬ì„± ì§€í‘œë³„ ë„¤íŠ¸ì›Œí¬ HTMLì„ ê°ê° ë‹¤ìš´ë¡œë“œ
            items = [
                ("degree",      "Degree-based",      "_degree_network.html"),
                ("eigenvector", "Eigenvector-based", "_eigenvector_network.html"),
                ("betweenness", "Betweenness-based", "_betweenness_network.html"),
                ("closeness",   "Closeness-based",   "_closeness_network.html"),
            ]

            for metric, label, suffix in items:
                html_str = make_html_string_from_csv(str(csv_path), size_by=metric, color_by=metric)
                st.download_button(
                    label=f"Visualization ë‹¤ìš´ë¡œë“œ ({label})",
                    data=html_str.encode("utf-8"),
                    file_name=csv_path.stem + suffix,
                    mime="text/html",
                    width = "stretch",
                )


    # === ê³µí†µ Preview(ìƒìœ„ 1,000í–‰) ë¸”ë¡ ===
    def _render_preview_block(sel_csv: str):
        df = pd.read_csv(sel_csv)
        total_rows = len(df)
        st.caption(
            f"âš ï¸ ì•„ë˜ í‘œëŠ” ì´ {total_rows:,}í–‰ ì¤‘ "
            f"ìƒìœ„ {min(PREVIEW_N, total_rows):,}í–‰ë§Œ Previewì…ë‹ˆë‹¤. "
            "'ë‹¤ìš´ë¡œë“œ' íƒ­ì—ì„œ íŒŒì¼ì„ ë‚´ë ¤ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        st.dataframe(df.head(PREVIEW_N), width="stretch")

        # â–¼ ë„¤íŠ¸ì›Œí¬ Preview(ì„ë² ë“œ) â€” degree / eigenvector
        with st.expander("ë„¤íŠ¸ì›Œí¬ Preview (Degree / Eigenvector / Betweenness / Closeness)", expanded=False):
            st.caption("â€» ì•ˆë‚´: 'Summary' íƒ­ì˜ ê³µí†µ ìš”ì•½ í‘œì— í‘œì‹œë˜ëŠ” 'ë…¸ë“œ/ì—£ì§€ ìˆ˜'ëŠ” **ìµœì¢… CSV ì „ì²´**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. "
                        "ì•„ë˜ ë¯¸ë¦¬ë³´ê¸° ë„¤íŠ¸ì›Œí¬ëŠ” **Centrality ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë…¸ë“œë“¤ì„ ê°•ì¡°/í•„í„°ë§**í•˜ì—¬ "
                        "ìš”ì•½ì˜ ìˆ˜ì¹˜ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ëŠ” 'ë‹¤ìš´ë¡œë“œ' íƒ­ì—ì„œ HTMLë¡œ ë‹¤ìš´ë¡œë“œ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            tabs = st.tabs(["Degree ë„¤íŠ¸ì›Œí¬", "Eigenvector ë„¤íŠ¸ì›Œí¬", "Betweenness ë„¤íŠ¸ì›Œí¬", "Closeness ë„¤íŠ¸ì›Œí¬"])

            # HTML ë¬¸ìì—´ ìƒì„± (ê¸°ì¡´ í—¬í¼ ì¬ì‚¬ìš©)
            deg_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="degree",       color_by="degree")
            eig_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="eigenvector",  color_by="eigenvector")
            bet_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="betweenness",  color_by="betweenness")
            clo_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="closeness",    color_by="closeness")


            with tabs[0]:
                components.html(deg_html, height=800, scrolling=True)
            with tabs[1]:
                components.html(eig_html, height=800, scrolling=True)
            with tabs[2]:
                components.html(bet_html, height=800, scrolling=True)
            with tabs[3]:
                components.html(clo_html, height=800, scrolling=True)

    # === ì‹¤í–‰ ì—¬ë¶€ì— ë”°ë¼ ê²°ê³¼ ë Œë” ===

    # A) ë²„íŠ¼ í´ë¦­ ì‹œ: ìƒíƒœë§Œ ì„¸íŒ…í•˜ê³  ì¦‰ì‹œ ì¬ì‹¤í–‰ â†’ í™”ë©´ì„ 'ì‹¤í–‰ ì¤‘' ìƒíƒœë¡œ ê³ ì •
    if run_btn:
        st.session_state._job = {
            "issns": issns,
            "year_start": int(year_start),
            "year_end": int(year_end),
            "email": email,
        }
        st.session_state._job_state = "running"
        
        #ë¬´í•œ ë¦¬ëŸ° ë°©ì§€
        if not st.session_state.get("_reran_once"):
            st.session_state["_reran_once"] = True
            st.rerun()

    # B) ì¬ì‹¤í–‰ í›„: ìƒíƒœë¥¼ ë³´ê³  ì‹¤ì œ ì‘ì—… ìˆ˜í–‰ (ì¤‘ê°„ì— ì•±ì´ ì¬ì‹¤í–‰ë¼ë„ ê³„ì† 'ì‹¤í–‰ ì¤‘' ìœ ì§€)
    if st.session_state._job_state == "running" and st.session_state._job:
        params = st.session_state._job
        start_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

        with st.spinner("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)"):
            progress_text.info("ì¤€ë¹„ ì¤‘..")
            final_csv, final_html = cached_run(
                params["issns"], params["year_start"], params["year_end"], params["email"],
                progress_bar=progress_bar, progress_text=progress_text
            )

        # ì¢…ë£Œ ì‹œê°„/ë¼ë²¨ ê¸°ë¡
        end_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))
        duration = end_time - start_time
        prefix = Path(final_csv).stem.replace(
            f"_{params['year_start']}_{params['year_end']}_ror_extract_name", ""
        )
        label = f"{prefix} | {params['year_start']}-{params['year_end']} | {start_time.strftime('%m/%d %H:%M')}"

        st.session_state.runs.append({
            "label": label, "csv": final_csv, "html": final_html,
            "start": start_time.strftime('%Y-%m-%d %H:%M:%S'),
            "end": end_time.strftime('%Y-%m-%d %H:%M:%S'),
            "duration": str(duration)
        })

        # ì‹¤í–‰ ì¢…ë£Œ í‘œì‹œ ë° ìƒíƒœ ì´ˆê¸°í™”
        st.session_state._job_state = "done"
        st.session_state._job = None

        # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ê°€ë“œ í•´ì œ
        st.session_state.pop("_reran_once", None)

        # ë°©ê¸ˆ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë Œë”(ê¸°ì¡´ UI íŒ¨í„´ ìœ ì§€)
        sel = st.session_state.runs[-1]
        st.success(f"ì‹¤í–‰ ì™„ë£Œ: {sel['label']}")

        st.sidebar.markdown("---")
        st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
        st.sidebar.write(f" ì‹œì‘ {sel['start']}")
        st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
        st.sidebar.write(f" ì†Œìš” {sel['duration']}")

        # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
        view = st.radio(
            "ë³´ê¸° ì„ íƒ",
            ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
            index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
            key="_view_radio",
            horizontal=True,
            on_change=_on_change_view,
        )
        view = st.session_state.view

        if view == "Preview":
            _render_preview_block(sel["csv"])
        if view == "ë‹¤ìš´ë¡œë“œ":
            _render_download_block(sel["csv"], sel["html"])
        if view == "Summary":
            _render_summary_unified(sel)

        # ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€(ì„ íƒ ìœ ì§€ìš©)
        st.session_state["last_csv"] = sel["csv"]


    else:
        # ì´ì „ ì‹¤í–‰ ì´ë ¥ ì„ íƒ/í‘œì‹œ
        if st.session_state.runs:
            st.markdown("### ì´ì „ ì‹¤í–‰ ê²°ê³¼ ë³´ê¸°")
            options = [r["label"] for r in st.session_state.runs]
            selected = st.selectbox("ì‹¤í–‰ ê²°ê³¼ ì„ íƒ", options, index=len(options)-1)
            sel = next(r for r in st.session_state.runs if r["label"] == selected)

            # ì‚¬ì´ë“œë°”ì— ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
            st.sidebar.markdown("---")
            st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
            st.sidebar.write(f" ì‹œì‘ {sel['start']}")
            st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
            st.sidebar.write(f" ì†Œìš” {sel['duration']}")

            # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
            view = st.radio(
                "ë³´ê¸° ì„ íƒ",
                ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
                index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
                key="_view_radio",
                horizontal=True,
                on_change=_on_change_view,
            )
            view = st.session_state.view

            if view == "Preview":
                _render_preview_block(sel["csv"])
            if view == "ë‹¤ìš´ë¡œë“œ":
                _render_download_block(sel["csv"], sel["html"])
            if view == "Summary":
                _render_summary_unified(sel)

            # ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€(ì„ íƒ ìœ ì§€ìš©)
            st.session_state["last_csv"] = sel["csv"]
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")



if __name__ == "__main__":
    app()
