import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"

import streamlit as st
import streamlit.components.v1 as components
import nest_asyncio
import pandas as pd
import datetime
from pathlib import Path
from zoneinfo import ZoneInfo
from io import BytesIO
import zipfile
import json
import ast
import csv
import re
import io


def _compute_inst_missing_and_recovered(final_csv_path: str):
    """
    ë°˜í™˜:
      ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜, ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜
    """
    df = pd.read_csv(final_csv_path, dtype={"authorships": str}, encoding="utf-8-sig")

    # 1) ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜: authorships ì•ˆì— ROR URLì´ í•˜ë‚˜ë„ ì—†ëŠ” í–‰
    rx_ror = re.compile(r"https?://ror\.org/[0-9a-z]+", re.I)
    has_ror = df["authorships"].astype(str).apply(lambda s: bool(rx_ror.search(s)))
    mask_missing = ~has_ror
    df_missing = df[mask_missing].copy()
    inst_missing = int(mask_missing.sum())

    # 2) ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜: ìœ„ missing í–‰ë“¤ ì¤‘, ë§¤í•‘ëœ ê¸°ê´€ëª…ì´ í•œ ê°œë¼ë„ ë“¤ì–´ ìˆëŠ” í–‰
    mapped_affs = _load_mapped_affiliations_for_final(final_csv_path)
    if not mapped_affs or inst_missing == 0:
        return inst_missing, 0  # ë³´ì™„ ìˆ˜ 0

    def _has_any_mapped(cell: str) -> bool:
        if not isinstance(cell, str):
            cell = str(cell)
        for aff in mapped_affs:
            if aff and aff in cell:
                return True
        return False

    recovered_mask = df_missing["authorships"].apply(_has_any_mapped)
    inst_recovered = int(recovered_mask.sum())

    return inst_missing, inst_recovered


# ------------------- START: Streamlit UI ì§„ë‹¨ í•¨ìˆ˜ -------------------
def diag_metrics_ui():
    """Streamlitì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì§„ë‹¨ UI.
    - í”„ë¡œì íŠ¸ ë£¨íŠ¸(.)ì—ì„œ *_metrics.csv, *metrics_aggregated*.csv ê²€ìƒ‰
    - í´ë”ë³„ ê°œìˆ˜, ìƒ˜í”Œ íŒŒì¼ëª…, ìƒ˜í”Œ íŒŒì¼ ë‚´ìš©(ìµœëŒ€ 40í–‰), ê·¸ë¦¬ê³  ì½”ë“œë‚´ FORCE_JOURNAL_KEY ìœ„ì¹˜ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    """
    import textwrap

    st.subheader("ì§„ë‹¨ ë„êµ¬: metrics íŒŒì¼, FORCE_JOURNAL_KEY ì ê²€")

    base = Path(".")

    # 1) íŒŒì¼ ìˆ˜ ë° í´ë”ë³„ ê°œìˆ˜
    metrics_files = sorted(base.rglob("*_metrics.csv"))
    st.write(f"ì „ì²´ *_metrics.csv íŒŒì¼ ìˆ˜: **{len(metrics_files)}**")

    # í´ë”ë³„ ì§‘ê³„
    folders = {}
    for p in metrics_files:
        folders.setdefault(str(p.parent), []).append(p)
    if folders:
        df = pd.DataFrame([{"folder": k, "count": len(v)} for k, v in folders.items()]).sort_values("count", ascending=False)
        st.write("í´ë”ë³„ *_metrics.csv ê°œìˆ˜ (ìƒìœ„ 30):")
        st.dataframe(df.head(30))
    else:
        st.write("í´ë”ë³„ *_metrics.csv íŒŒì¼ ì—†ìŒ")

    # 2) ìƒ˜í”Œ íŒŒì¼ë“¤ ì¶œë ¥ (ìƒìœ„ í´ë” ê° 1ê°œì”© ìµœëŒ€ 10ê°œ)
    st.write("---")
    st.write("ìƒ˜í”Œ íŒŒì¼ ë° ë‚´ìš© (í´ë”ë³„ ìµœëŒ€ 1ê°œ, ê° íŒŒì¼ ìµœëŒ€ 40í–‰ í‘œì‹œ)")
    sample_show_count = 0
    for folder, files in sorted(folders.items(), key=lambda x: -len(x[1])):
        if sample_show_count >= 10:
            break
        p = files[0]
        st.markdown(f"**{folder} / {p.name}**")
        try:
            text = p.read_text(encoding="utf-8", errors="replace")
            lines = text.splitlines()
            preview = "\n".join(lines[:40])
            st.code(preview, language="text")
        except Exception as e:
            st.write(f"  (íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨) {e}")
        sample_show_count += 1

    # 4) í”„ë¡œì íŠ¸ ë‚´ ì½”ë“œì—ì„œ FORCE_JOURNAL_KEY, _key_name_for, _resolve_journal_name_slug ìœ„ì¹˜ ê²€ìƒ‰
    st.write("---")
    st.write("ì½”ë“œ ì•ˆì—ì„œ FORCE_JOURNAL_KEY ê´€ë ¨ ë¬¸ìì—´ ê²€ìƒ‰ ê²°ê³¼ (íŒŒì¼ëª…: line_number : ì½”ë“œë¼ì¸)")
    search_terms = ["FORCE_JOURNAL_KEY", "_key_name_for(", "_resolve_journal_name_slug("]
    code_hits = []
    for py in sorted(base.rglob("*.py")):
        try:
            txt = py.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        for idx, line in enumerate(txt.splitlines(), start=1):
            for term in search_terms:
                if term in line:
                    code_hits.append((str(py), idx, line.strip()))
    if code_hits:
        for p, ln, line in code_hits[:200]:
            st.write(f"- {p}:{ln}: `{textwrap.shorten(line, width=200)}`")
    else:
        st.write("ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ")

    # 5) í˜„ì¬ ì „ì—­ FORCE_JOURNAL_KEY ê°’ ì¶œë ¥(ì¡´ì¬í•˜ë©´)
    st.write("---")
    try:
        import sys
        # ìš°ì„  í˜„ì¬ ëª¨ë“ˆ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ ì°¾ì•„ë³´ê³ , ì—†ìœ¼ë©´ í”íˆ ì“°ì´ëŠ” ëª¨ë“ˆëª…ë“¤(app, __main__)ì„ í™•ì¸
        val = None
        cur_mod = sys.modules.get(__name__)
        if cur_mod is not None and hasattr(cur_mod, "FORCE_JOURNAL_KEY"):
            val = getattr(cur_mod, "FORCE_JOURNAL_KEY")
        else:
            for alt in ("app", "__main__"):
                m = sys.modules.get(alt)
                if m and hasattr(m, "FORCE_JOURNAL_KEY"):
                    val = getattr(m, "FORCE_JOURNAL_KEY")
                    break
        st.write("app.FORCE_JOURNAL_KEY =", val)
    except Exception as e:
        st.write("ì „ì—­ FORCE_JOURNAL_KEY ì¡°íšŒ ì‹¤íŒ¨:", e)

    st.write("---")
    st.write("ì§„ë‹¨ ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ. ìœ„ ì¶œë ¥(íŠ¹íˆ í´ë”ë³„ metrics ë¶„í¬ì™€ code hits)ì„ ë³µì‚¬í•´ì„œ ì €ì—ê²Œ ë¶™ì—¬ë„£ì–´ ì£¼ì„¸ìš”.")
# ------------------- END: Streamlit UI ì§„ë‹¨ í•¨ìˆ˜ -------------------


from new import run_pipeline, make_html_from_csv, run_pipeline_cached, make_html_string_from_csv

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
now_kst = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

# ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ë°©ì§€ (async ì‚¬ìš© ì‹œ í•„ìˆ˜)
nest_asyncio.apply()


st.set_page_config(page_title="OpenAlex Paper Metadata Pipeline", layout="wide")

# ---- View state persistence (rerunì—ë„ íƒ­ ìœ ì§€) ----
if "view" not in st.session_state:
    _default = st.query_params.get("view", "Preview")
    st.session_state.view = _default if _default in ("Preview", "ë‹¤ìš´ë¡œë“œ", "Summary") else "Preview"

def _on_change_view():
    st.session_state.view = st.session_state._view_radio
    st.query_params["view"] = st.session_state.view



# ìºì‹œ ë°ì½”ë ˆì´í„° ì œê±° (íŒŒì¼ ìƒì„±/ì™¸ë¶€ I/OëŠ” ìºì‹œ ë¹„ê¶Œì¥)
def cached_run(issns, year_start, year_end, email,
               progress_bar=None, progress_text=None):

    # storage í•˜ìœ„ì— ê²°ê³¼ ì €ì¥
    final_csv_path, _ = run_pipeline_cached(
        issns=issns, year_start=year_start, year_end=year_end, email=email,
        include_only_with_abstract=False, make_html=True,
        base_dir=Path("storage")
    )

    if progress_bar:
        progress_bar.progress(0.7)
    if progress_text:
        progress_text.info("ìµœì¢… CSV ìƒì„± ë° í™•ì¸ ì¤‘ ..")

    #ìµœì¢… CSV ê²½ë¡œ ë¬¸ìì—´ -> Path ê°ì²´
    out_path = Path(final_csv_path)

    final_html = make_html_from_csv(str(out_path))

    if progress_bar:
        progress_bar.progress(1.0)
    if progress_text:
        progress_text.success("ëª¨ë“  êµ¬ê°„ ì²˜ë¦¬ ë° ìµœì¢… íŒŒì¼ ìƒì„± ì™„ë£Œ")

    st.sidebar.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ Â· {now_kst:%H:%M:%S}")

    return str(out_path), final_html

# === ìƒˆ í—¬í¼: ì„¼íŠ¸ëŸ´ë¦¬í‹° CSV ì°¾ê¸° ===
def _find_centrality_csvs(final_csv_path: str):
    base = Path(final_csv_path).with_suffix("")  # ..._ror_extract_name
    base_str = str(base)
    candidates = [
        f"{base_str}_centrality.csv",
        f"{base_str}_ror_extract_name_centrality.csv",
    ]
    # logsì—ì„œ workdir_tmpì— ì €ì¥ë˜ëŠ” ê²½ìš°ë„ ì»¤ë²„
    search_dirs = [base.parent, base.parent / "workdir_tmp"]
    found = None
    for d in search_dirs:
        for name in candidates:
            p = d / Path(name).name
            if p.exists():
                found = p
                break
        if found: break
    return found


def _mk_metric_pattern(prefix_all, y0, y1):
    return re.compile(rf"^{re.escape(prefix_all)}_{y0}_{y1}_metrics\.csv$", re.I)

def _summary_metrics_common_multi(sel_csv_list: list[str]):
    """
    ë©€í‹° ìµœì¢… CSV ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê³µí†µ ìš”ì•½ì§€í‘œë¥¼ í•©ì‚°í•´ 1í–‰ DataFrameìœ¼ë¡œ ë°˜í™˜.
    - ê° ìµœì¢… CSVì— ëŒ€í•´ new.pyì˜ build_summary_from_metrics_for_final()ì„ í˜¸ì¶œ
    - ìˆ«ì í•­ëª©ë§Œ í•©ì‚°, ë¹„ìœ¨ì€ í•©ì‚° ê²°ê³¼ë¡œ ì¬ê³„ì‚°
    - íŒŒì¼ ì €ì¥/ì¤‘ê°„ì‚°ì¶œ ì—†ìŒ (ë©”ëª¨ë¦¬ ì§í–‰)
    """

    from new import build_summary_from_metrics_for_final

    # ëˆ„ì  ë²„í‚·
    tot = {
        "ìµœì´ˆ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": 0,
        "ìµœì¢… ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": 0,
        "Editorial Material ì‚­ì œ ìˆ˜": 0,
        "ID íŒ¨í„´ ë¶ˆì¼ì¹˜ ì‚­ì œ ìˆ˜": 0,
        "ìµœì¢… CSV í–‰ ìˆ˜(í•©ê³„)": 0,
        "DOI ê²°ì¸¡ ìˆ˜(í•©ì‚°)": 0,
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": 0,
        "ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)": 0,
        "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": 0,
    }

    def _to_int(x):
        try:
            s = str(x).strip()
            if s.endswith("%"):
                s = s[:-1]
            s = s.replace(",", "")
            v = float(s)
            return int(v) if v.is_integer() else int(round(v))
        except Exception:
            return 0

    any_ok = False
    for path in sel_csv_list:
        try:
            df = build_summary_from_metrics_for_final(path)
            if isinstance(df, str):
                df = pd.read_csv(df, encoding="utf-8-sig")

            if df is None or df.empty:
                continue
            any_ok = True
            row = df.iloc[0]
            for k in tot.keys():
                tot[k] += _to_int(row.get(k, 0))
        except Exception as e:
            # ê° íŒŒì¼ ì˜¤ë¥˜ëŠ” ìŠ¤í‚µí•˜ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
            print(f"[WARN] multi summary read failed: {path} -> {e}")

    if not any_ok:
        return pd.DataFrame([{
            "ìµœì´ˆ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": 0,
            "ìµœì¢… ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": 0,
            "Editorial Material ì‚­ì œ ìˆ˜": 0,
            "ID íŒ¨í„´ ë¶ˆì¼ì¹˜ ì‚­ì œ ìˆ˜": 0,
            "ìµœì¢… CSV í–‰ ìˆ˜(í•©ê³„)": 0,
            "DOI ê²°ì¸¡ ìˆ˜(í•©ì‚°)": 0,
            "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": 0,
            "DOI ë³´ê°•ë¥ ": "0.00%",
            "ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)": 0,
            "ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)": 0,
            "ROR ID ë³´ê°•ë¥ ": "0.00%",
        }])

    # ë¹„ìœ¨ ì¬ê³„ì‚°
    doi_denom = tot["DOI ê²°ì¸¡ ìˆ˜(í•©ì‚°)"] + tot["DOI ë³´ê°• ìˆ˜(í•©ì‚°)"]
    ror_denom = tot["ROR ID ê²°ì¸¡ ìˆ˜(í•©ì‚°)"] + tot["ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)"]
    doi_rate = f"{(100.0 * tot['DOI ë³´ê°• ìˆ˜(í•©ì‚°)'] / doi_denom):.2f}%" if doi_denom else "0.00%"
    ror_rate = f"{(100.0 * tot['ROR ID ë³´ê°• ìˆ˜(í•©ì‚°)'] / ror_denom):.2f}%" if ror_denom else "0.00%"

    return pd.DataFrame([{
        **tot,
        "DOI ë³´ê°•ë¥ ": doi_rate,
        "ROR ID ë³´ê°•ë¥ ": ror_rate,
    }])




# --- diagnostics helper (ë¶™ì—¬ë„£ê¸°: app.pyì— í•œ ë²ˆë§Œ ì¶”ê°€) ---
def run_metrics_diagnostic(final_csv_list=None, extra_base_dirs=None):
    """
    final_csv_list: list of final CSV paths (strings). ë³´í†µ st.session_state.runsì˜ ê° item['csv']ë“¤.
    extra_base_dirs: ì¶”ê°€ë¡œ ìŠ¤ìº”í•  ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (strings) e.g. ['./workdir_tmp']
    Returns: (metrics_files, per_file_list, totals_dict)
    Also returns a small CSV in-memory for download.
    """
    def to_float_safe(x):
        try:
            if x is None:
                return None
            s = str(x).strip()
            if s == "":
                return None
            s = s.replace(",", "")
            return float(s)
        except Exception:
            return None

    # 1) collect candidate dirs
    scan_dirs = set()
    if final_csv_list:
        for p in final_csv_list:
            try:
                pth = Path(p)
                scan_dirs.add(pth.parent)
                scan_dirs.add(pth.parent / "workdir_tmp")
            except Exception:
                continue
    # allow caller to pass extra dirs
    if extra_base_dirs:
        for d in extra_base_dirs:
            scan_dirs.add(Path(d))

    # default fallback: current dir + './workdir_tmp'
    if not scan_dirs:
        scan_dirs.add(Path("."))
        scan_dirs.add(Path("workdir_tmp"))

    # 2) discover metrics files
    metrics_files = []
    for d in scan_dirs:
        if not d.exists():
            continue
        metrics_files.extend(list(d.rglob("*_metrics.csv")))
    metrics_files = sorted(set(metrics_files))

    # 3) read files and aggregate
    totals = {}
    per_file = []
    for mf in metrics_files:
        record = {}
        try:
            df = pd.read_csv(mf, encoding='utf-8-sig', on_bad_lines='warn')
            # support 'key','value' or two-column fallback
            if "key" in df.columns and "value" in df.columns:
                rows = list(zip(df["key"].astype(str), df["value"]))
            else:
                rows = []
                for r in df.itertuples(index=False):
                    if len(r) >= 2:
                        rows.append((str(r[0]), getattr(r, "1", r[1] if len(r) > 1 else None)))
            for k, v in rows:
                k = str(k).strip()
                if not k:
                    continue
                num = to_float_safe(v)
                record[k] = v if num is None else num
                if num is not None:
                    totals[k] = totals.get(k, 0.0) + num
        except Exception as e:
            record["_error"] = str(e)
        per_file.append((mf, record))

    # prepare CSV output of aggregated totals
    buf = io.StringIO()
    if totals:
        pd.DataFrame([totals]).to_csv(buf, index=False, encoding='utf-8')
    else:
        buf.write("")  # empty
    buf.seek(0)
    return metrics_files, per_file, totals, buf
# --- end diagnostics helper ---


# === ê¸°ê´€ì •ë³´ ê´€ë ¨ í—¬í¼: mapped affiliation ë¶ˆëŸ¬ì˜¤ê¸° + ê²°ì¸¡/ë³´ì™„ ê³„ì‚° ===
def _load_mapped_affiliations_for_final(final_csv_path: str) -> list[str]:
    """
    final_csv_path(â€¦_ror_extract_name.csv)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ,
    ê°™ì€ í´ë” / workdir_tmp ì— ìˆëŠ” ë‹¤ìŒ íŒ¨í„´ì˜ íŒŒì¼ë“¤ì—ì„œ
    normalized_affiliation ê°’ì„ ëª¨ë‘ ëª¨ì•„ ë°˜í™˜í•œë‹¤.

      - <prefix>_mapped_affiliations.csv      (êµ¬ë²„ì „ í˜¸í™˜)
      - mapped_affiliations_YYYY.csv          (ì—°ë„ë³„ ì‹ ê·œ ë²„ì „)
    """
    from pathlib import Path
    import pandas as pd

    base = Path(final_csv_path)
    stem = base.stem.replace("_ror_extract_name", "")

    candidates: list[Path] = []

    # 1) ì˜ˆì „ ê·œì¹™: <prefix>_mapped_affiliations.csv
    candidates.append(base.with_name(f"{stem}_mapped_affiliations.csv"))
    candidates.append(base.parent / "workdir_tmp" / f"{stem}_mapped_affiliations.csv")

    # 2) ìƒˆ ê·œì¹™: mapped_affiliations_YYYY.csv (ì—°ë„ë³„)
    try:
        for p in base.parent.glob("mapped_affiliations_*.csv"):
            candidates.append(p)
    except Exception:
        pass

    try:
        wt = base.parent / "workdir_tmp"
        if wt.exists():
            for p in wt.glob("*_mapped_affiliations.csv"):
                candidates.append(p)
            for p in wt.glob("mapped_affiliations_*.csv"):
                candidates.append(p)
    except Exception:
        pass

    # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤ë§Œ ëŒ€ìƒ
    candidates = [p for p in candidates if p.exists()]

    if not candidates:
        return []

    normed_set = set()

    for p in candidates:
        try:
            df = pd.read_csv(p, encoding="utf-8-sig")
        except Exception:
            continue

        if "normalized_affiliation" not in df.columns:
            continue

        aff_list = (
            df["normalized_affiliation"]
            .dropna()
            .astype(str)
            .tolist()
        )

        for s in aff_list:
            s = s.strip().rstrip(" .,\t")
            s = " ".join(s.split())
            if s:
                normed_set.add(s)

    return sorted(normed_set)


def _compute_inst_missing_and_recovered(final_csv_path: str):
    """
    ë°˜í™˜:
      ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜, ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜

    - ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜:
        authorships ì•ˆì— ROR URLì´ í•œ ê°œë„ ì—†ëŠ” ë…¼ë¬¸ ìˆ˜
    - ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜:
        ê·¸ ê²°ì¸¡ ë…¼ë¬¸ë“¤ ì¤‘ì—ì„œ mapped affiliation(ê¸°ê´€ëª…)ì´
        authorships í…ìŠ¤íŠ¸ ì•ˆì— í•œ ê°œë¼ë„ ë“±ì¥í•˜ëŠ” ë…¼ë¬¸ ìˆ˜
    """
    import re
    from pathlib import Path
    import pandas as pd

    df = pd.read_csv(Path(final_csv_path), dtype={"authorships": str}, encoding="utf-8-sig")

    # 1) ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜: ROR URLì´ í•˜ë‚˜ë„ ì—†ëŠ” í–‰
    rx_ror = re.compile(r"https?://ror\.org/[0-9a-z]+", re.I)
    has_ror = df["authorships"].astype(str).apply(lambda s: bool(rx_ror.search(s)))
    mask_missing = ~has_ror
    df_missing = df[mask_missing].copy()
    inst_missing = int(mask_missing.sum())

    # 2) ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜: ê²°ì¸¡í–‰ë“¤ ì¤‘ mapped affiliationì´ í•œ ê°œë¼ë„ ë“¤ì–´ ìˆëŠ” í–‰
    mapped_affs = _load_mapped_affiliations_for_final(final_csv_path)
    if not mapped_affs or inst_missing == 0:
        return inst_missing, 0

    def _has_any_mapped(cell: str) -> bool:
        if not isinstance(cell, str):
            cell = str(cell)
        for aff in mapped_affs:
            if aff and aff in cell:
                return True
        return False

    recovered_mask = df_missing["authorships"].apply(_has_any_mapped)
    inst_recovered = int(recovered_mask.sum())

    return inst_missing, inst_recovered


# === ìƒˆ í—¬í¼: ê³µí†µ ìš”ì•½í‘œ ===
def _summary_metrics_common(final_csv_path: str):

    # ---- 0) ê²½ë¡œ/ë©”íƒ€ íŒŒì‹± (ë¨¼ì € í•´ì•¼ í•¨)
    final_csv = Path(final_csv_path)
    base_path = final_csv.with_suffix("")
    base = str(base_path)

    # ì ‘ë¯¸ì‚¬ ì œê±°í•´ ë£¨íŠ¸ë² ì´ìŠ¤ ì¶”ì¶œ
    root_base = re.sub(r'_(?:ror(?:_extract(?:_name)?)?)(?:_summary)?$', '', base)
    json_merged = Path(f"{root_base}.json")
    csv_file    = final_csv

    # export/summary íŒŒì¼ì´ë©´ CSV ê¸¸ì´ í´ë°± ê¸ˆì§€
    is_export = final_csv.name.endswith("_export.csv") or final_csv.name.endswith("_summary.csv")

    # ì ‘ë‘ì‚¬/ì—°ë„ ì¶”ì¶œ
    m = re.search(r"_(\d{4})_(\d{4})$", root_base)
    y0 = y1 = None
    prefix_all = ""
    if m:
        y0, y1 = int(m.group(1)), int(m.group(2))
        prefix_all = Path(root_base).name[:-(len(m.group(0)))]

    # ê²€ìƒ‰ ë””ë ‰í„°ë¦¬
    root_dir = Path(root_base).parent
    search_dirs = [root_dir, root_dir / "workdir_tmp"]

    def sum_metrics(keys, search_dirs, allowed_names=None):


        acc = {k: 0 for k in keys if not k.endswith("_rate")}
        rate_keys = [k for k in keys if k.endswith("_rate")]
        found = False

        # ì¤‘ë³µ ì°¨ë‹¨: ê²½ë¡œ + íŒŒì¼ëª… + (ì„ íƒ) triplet
        seen_paths = set()   # ê²½ë¡œ ê¸°ì¤€(ì•ˆì „ë§)
        seen_names = set()   # íŒŒì¼ëª… ê¸°ì¤€ (ê²½ë¡œ ë‹¬ë¼ë„ 1íšŒ)

        # "xxx (1).csv" â†’ "xxx.csv"
        def _strip_copy_suffix(name: str) -> str:
            return re.sub(r' \(\d+\)\.csv$', '.csv', name, flags=re.IGNORECASE)

        def _to_num(s):
            try:
                if s is None or s == "":
                    return None
                f = float(s)
                return int(f) if f.is_integer() else f
            except Exception:
                return None

        for base in search_dirs:
            base = Path(base)
            if not base.exists():
                continue
            for mf in base.rglob("*_metrics.csv"):
                # 1) ì´ë²ˆ ì‹¤í–‰ ëŒ€ìƒ íŒŒì¼ë§Œ (ë³µì‚¬ë³¸ ì ‘ë¯¸ì‚¬ ì œê±° í›„ ë¹„êµ)
                base_name = _strip_copy_suffix(mf.name)
                if allowed_names and (base_name not in allowed_names):
                    continue

                # 2) íŒŒì¼ëª… ì¤‘ë³µ ì°¨ë‹¨ (ê²½ë¡œ ë‹¬ë¼ë„ ê°™ì€ ì´ë¦„ì´ë©´ 1íšŒë§Œ)
                if base_name in seen_names:
                    continue
                seen_names.add(base_name)

                # 3) ê²½ë¡œ ì¤‘ë³µ ì°¨ë‹¨(ì•ˆì „ë§)
                rp = mf.resolve()
                if rp in seen_paths:
                    continue
                seen_paths.add(rp)

                # 4) íŒŒì¼ ë‚´ë¶€ ì¤‘ë³µ keyëŠ” 'ë§ˆì§€ë§‰ ê°’'ë§Œ ì‚¬ìš©(í˜„ì¬ ì •ì±… ìœ ì§€)
                try:
                    last_by_key = {}
                    with mf.open("r", encoding="utf-8", newline="") as f:
                        rdr = csv.reader(f)
                        _ = next(rdr, None)  # header skip
                        for row in rdr:
                            if len(row) < 2:
                                continue
                            k = (row[0] or "").strip()
                            if not k:
                                continue
                            v = _to_num(row[1])
                            if v is None or k.endswith("_rate"):
                                continue
                            last_by_key[k] = v  # ë™ì¼ key ì¬ë“±ì¥ ì‹œ ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ê°±ì‹ 

                    for k, v in last_by_key.items():
                        if k in acc:
                            acc[k] += v
                    found = True
                except Exception:
                    # ê°œë³„ íŒŒì¼ ì‹¤íŒ¨ëŠ” ì „ì²´ ì‹¤íŒ¨ë¡œ ë³´ì§€ ì•ŠìŒ
                    pass

        # ---- ë¹„ìœ¨ ì¬ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ----
        if "doi_enriched" in acc and "doi_missing" in acc and "doi_enrich_rate" in rate_keys:
            denom = (acc["doi_missing"] or 0) + (acc["doi_enriched"] or 0)
            acc["doi_enrich_rate"] = round(100.0 * (acc["doi_enriched"] or 0) / denom, 2) if denom else 0.0
        if "ror_enriched" in acc and "ror_missing" in acc and "ror_enrich_rate" in rate_keys:
            denom = (acc["ror_missing"] or 0) + (acc["ror_enriched"] or 0)
            acc["ror_enrich_rate"] = round(100.0 * (acc["ror_enriched"] or 0) / denom, 2) if denom else 0.0

        return acc, found

    # ---- 2) metrics.csv í•©ì‚° (ì´ë²ˆ ì‹¤í–‰ì˜ íŒŒì¼ëª…ë§Œ í—ˆìš©)
    allowed_names = set()
    if prefix_all and (y0 is not None) and (y1 is not None):
        # ë©€í‹°ì €ë„ í”„ë¦¬í”½ìŠ¤("a-b-c")ì´ë©´ ì €ë„ë³„ íŒŒì¼ëª…ì„ ëª¨ë‘ í—ˆìš©
        keys = [k for k in prefix_all.split("-") if k.strip()]
        if not keys:
            keys = [prefix_all]
        years = range(int(y0), int(y1) + 1)
        allowed_names = {f"{k}_{yy}_{yy}_metrics.csv" for k in keys for yy in years}
    acc, found = sum_metrics([
        "total_collected","final_csv_rows",
        "authorships_removed_empty_list", "authorships_removed",
        "col_mismatch_removed", "id_pattern_removed",
        "doi_missing","doi_enriched","doi_enrich_rate",
        "ror_missing","ror_enriched","ror_enrich_rate",
        "ror_missing_before_extract",
    ], search_dirs, allowed_names=allowed_names)

    # ---- 3) í•©ì‚° ê²°ê³¼ ë³€ìˆ˜ë“¤
    total_collected = acc.get("total_collected") if found else None
    removed_authorships_empty = acc.get("authorships_removed_empty_list") if found else None
    removed_authorships = acc.get("authorships_removed") if found else None
    doi_missing_sum   = acc.get("doi_missing") if found else None
    doi_enriched_sum  = acc.get("doi_enriched") if found else None
    doi_rate_from_acc = acc.get("doi_enrich_rate") if found else None
    ror_missing_sum   = acc.get("ror_missing") if found else None
    ror_enriched_sum  = acc.get("ror_enriched") if found else None
    ror_rate_from_acc = acc.get("ror_enrich_rate") if found else None

    editorial_total = (removed_authorships_empty or 0) + (removed_authorships or 0)
    id_pattern_removed_sum = acc.get("id_pattern_removed") if found else None

    # ---- 4) JSON í´ë°± (metricsê°€ ì—†ì„ ë•Œë§Œ)
    def _try_load_json(p: Path):
        try:
            return json.loads(p.read_text(encoding="utf-8")) or []
        except Exception:
            return None

    works = None
    if total_collected is None:
        if json_merged.exists():
            works = _try_load_json(json_merged)
        if works is None and prefix_all and (y0 is not None) and (y1 is not None):
            fname_json = f"{prefix_all}_{y0}_{y1}.json"
            for d in search_dirs:
                cand = Path(d) / fname_json
                if cand.exists():
                    works = _try_load_json(cand)
                    if works is not None:
                        break
        if works is not None:
            total_collected = len(works)
            removed_authorships_empty = sum(
                1 for w in works
                if not isinstance(w.get("authorships"), list) or len(w.get("authorships") or []) == 0
            )

    # ---- 5) ìµœí›„ í´ë°±: CSV ê¸¸ì´ (export/summaryë©´ ê¸ˆì§€)
    if total_collected is None and not is_export:
        try:
            if csv_file.exists():
                total_collected = len(pd.read_csv(csv_file))
        except Exception:
            pass

    # ---- 6) ìµœì¢… CSVì—ì„œ ì§ì ‘ ì„¼ ê°’(ì°¸ê³ /í´ë°±ìš©)
    doi_missing_final = None
    if csv_file.exists():
        try:
            df_csv = pd.read_csv(csv_file, dtype={"doi": str})
            doi_missing_final = df_csv["doi"].isna().sum() + (df_csv["doi"].astype(str).str.strip() == "").sum()
        except Exception:
            pass

    # ROR ë³´ê°• ì „ ê²°ì¸¡(ìµœì¢… CSVì˜ authorshipsì—ì„œ ì¶”ì •)
    ror_missing_before = None
    try:
        if csv_file.exists():
            tmp = pd.read_csv(csv_file, dtype={"authorships": str})
            if "authorships" in tmp.columns:
                rx = re.compile(r'https?://ror\.org/[0-9a-z]+', re.I)
                ror_before = tmp["authorships"].astype(str).apply(
                    lambda s: list(dict.fromkeys(rx.findall(s)))
                )
                ror_missing_before = int((ror_before.str.len() == 0).sum())
    except Exception:
        pass

    # ---- 7) DOI ê²°ì¸¡ í‘œì‹œ: metricsê°€ ìˆìœ¼ë©´ ê·¸ ê°’, ì—†ìœ¼ë©´ ìµœì¢… CSV
    if doi_missing_sum is None:
        doi_missing_show = doi_missing_final   # íŒŒì¼ì„ ëª» ì½ì—ˆì„ ë•Œë§Œ CSVë¡œ ëŒ€ì²´
    else:
        doi_missing_show = doi_missing_sum

    # ---- 8) DOI ë³´ê°•ë¥  ê³„ì‚°(ì¼ê´€í™”): (enriched / (missing + enriched)) Ã— 100
    doi_rate_calc = None
    if (doi_missing_sum is not None) and (doi_enriched_sum is not None):
        denom = (doi_missing_sum or 0) + (doi_enriched_sum or 0)
        doi_rate_calc = round(100.0 * (doi_enriched_sum or 0) / denom, 2) if denom else 0.0

    # ---- 9) ë…¸ë“œÂ·ì—£ì§€ ìˆ˜ (ì¤‘ì•™ì„± CSV ìˆìœ¼ë©´ ê·¸ ë…¸ë“œ ì§‘í•© ìš°ì„ )
    cent_csv = _find_centrality_csvs(final_csv_path)
    def _to_pairs(names):
        arr = []
        if isinstance(names, str):
            s = names.strip()
            if s:
                try:
                    v = ast.literal_eval(s)
                    arr = list(v) if isinstance(v, (list, tuple)) else [v]
                except Exception:
                    try:
                        v = json.loads(s)
                        arr = v if isinstance(v, list) else [v]
                    except Exception:
                        for sep in [";", "|", ","]:
                            if sep in s:
                                arr = [t.strip() for t in s.split(sep) if t.strip()]
                                break
                        if not arr:
                            arr = [s]
        elif isinstance(names, list):
            arr = names
        arr = [str(a) for a in arr if a]
        pairs = []
        for i in range(len(arr)):
            for j in range(i+1, len(arr)):
                a,b = sorted((arr[i], arr[j]))
                pairs.append((a,b))
        return pairs

    node_count = edge_count = None
    try:
        df_final = pd.read_csv(final_csv)
        all_pairs=[]
        if "org_names" in df_final.columns:
            for pr in df_final["org_names"].apply(_to_pairs):
                all_pairs.extend(pr)
        # ì¤‘ì•™ì„± CSVê°€ ìˆì–´ë„, ìš”ì•½í‘œì˜ 'ë…¸ë“œ/ì—£ì§€ ìˆ˜'ëŠ” ìµœì¢… CSV ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        nodes = {n for p in all_pairs for n in p}
        node_count = len(nodes)
        edge_count = len(set(all_pairs))
    except Exception:
        node_count = edge_count = None

    # ---- 10) ì¶œë ¥ í…Œì´ë¸” êµ¬ì„± + í¬ë§· ----
    # ìµœì´ˆ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜(first_rows)ëŠ” ì´ì œ í‘œì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ,
    # í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ë‘ 
    first_rows = (0 if total_collected is None else total_collected)
    final_rows = (len(df_final) if 'df_final' in locals() else 0)  # ìµœì¢… CSV í–‰ìˆ˜ = ë¬¸í—Œ ì´ ìˆ˜

    # 10-1) ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜ & ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜ ê³„ì‚°
    inst_missing, inst_recovered = _compute_inst_missing_and_recovered(str(csv_file))

    # metricsì—ì„œ ror_enrichedê°€ ì œê³µë˜ë©´, ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜ë¡œ ìš°ì„  ì‚¬ìš©
    if ror_enriched_sum is not None:
        try:
            inst_recovered = int(ror_enriched_sum)
        except Exception:
            pass

    # 10-2) ë³´ê°• ì „/í›„ ê¸°ê´€ì •ë³´ í™œìš©ê°€ëŠ¥ ë¹„ìœ¨
    if final_rows > 0:
        # ë³´ê°• ì „: (ë¬¸í—Œ ì´ ìˆ˜ - ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜) / ë¬¸í—Œ ì´ ìˆ˜ * 100
        pre_util_rate  = (final_rows - inst_missing) / final_rows * 100.0
        # ë³´ê°• í›„: (ë¬¸í—Œ ì´ ìˆ˜ - ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜ + ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜) / ë¬¸í—Œ ì´ ìˆ˜ * 100
        post_util_rate = (final_rows - inst_missing + inst_recovered) / final_rows * 100.0
    else:
        pre_util_rate = post_util_rate = 0.0

    def _fmt_pct(v):
        try:
            return f"{float(v):.2f}%"
        except Exception:
            return "0.00%"

    # 10-3) ìš”ì•½í‘œ DataFrame (ì»¬ëŸ¼ ì´ë¦„ ì „ë¶€ ìƒˆë¡œ ì •ë¦¬)
    df_out = pd.DataFrame([{
        # ë¬¸í—Œ/ë¹„ë…¼ë¬¸
        "ë¬¸í—Œ ì´ ìˆ˜": final_rows,
        "ë¹„ë…¼ë¬¸ ë¬¸í—Œ ìˆ˜": editorial_total,

        # ê¸°ê´€ì •ë³´
        "ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜": inst_missing,
        "ë³´ê°• ì „ ê¸°ê´€ì •ë³´ í™œìš©ê°€ëŠ¥ ë¹„ìœ¨": _fmt_pct(pre_util_rate),
        "ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜": inst_recovered,
        "ë³´ê°• í›„ ê¸°ê´€ì •ë³´ í™œìš©ê°€ëŠ¥ ë¹„ìœ¨": _fmt_pct(post_util_rate),

        # DOI (ì´ë¦„ë§Œ ë³€ê²½)
        "DOI ê²°ì¸¡ ìˆ˜": doi_missing_show,
        "DOI ë³´ê°• ìˆ˜": (0 if doi_enriched_sum is None else doi_enriched_sum),
        "DOI ë³´ê°•ë¥ ": _fmt_pct(
            doi_rate_calc if doi_rate_calc is not None
            else (0.0 if doi_rate_from_acc is None else float(doi_rate_from_acc))
        ),

        # ë„¤íŠ¸ì›Œí¬ ìš”ì•½
        "ë…¸ë“œ ìˆ˜": node_count,
        "ì—£ì§€ ìˆ˜": edge_count,
    }])

    # ìˆ«ì ì»¬ëŸ¼ NaN â†’ 0 ì •ë¦¬
    for col in [
        "ë¬¸í—Œ ì´ ìˆ˜", "ë¹„ë…¼ë¬¸ ë¬¸í—Œ ìˆ˜",
        "ê¸°ê´€ì •ë³´ ê¸°ì¤€ ê²°ì¸¡ ìˆ˜", "ê¸°ê´€ì •ë³´ ë³´ì™„ ìˆ˜",
        "DOI ê²°ì¸¡ ìˆ˜", "DOI ë³´ê°• ìˆ˜",
        "ë…¸ë“œ ìˆ˜", "ì—£ì§€ ìˆ˜",
    ]:
        if col in df_out.columns:
            df_out[col] = df_out[col].apply(lambda v: 0 if v is None else v).astype(int)

    return df_out


# === ìƒˆ í—¬í¼: ì¤‘ì‹¬ì„±ë³„ Top ë…¸ë“œ & ì—£ì§€ í‘œ ===
# === ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ & ì—£ì§€ ===
def _summary_top_nodes_and_edges(final_csv_path: str):
    """
    - ì¤‘ì•™ì„± CSV(â€¦_centrality.csv)ì—ì„œ ê° centrality ìƒìœ„ 10 ë…¸ë“œ
    - ìµœì¢… CSV(org_names)ì—ì„œ ë™ì‹œì¶œí˜„ìˆ˜ ìƒìœ„ 10 ì—£ì§€
    ë°˜í™˜: (df_nodes_wide, df_edges_wide)
      - df_nodes_wide: columns = ['degree: ë…¸ë“œ','eigenvector: ë…¸ë“œ','betweenness: ë…¸ë“œ'], index=Top1..Top10
      - df_edges_wide: columns = ['degree: ì—£ì§€','eigenvector: ì—£ì§€','betweenness: ì—£ì§€'], index=Top1..Top10
    """
    import itertools
    from collections import Counter
    # --- ì¤‘ì•™ì„± CSV ì°¾ê¸° ---
    cent_csv = _find_centrality_csvs(final_csv_path)
    if cent_csv is None:
        # ì—†ìœ¼ë©´ í•œë²ˆ ìƒì„± ì‹œë„(ì‹¤íŒ¨í•´ë„ ì¡°ìš©íˆ íŒ¨ìŠ¤)
        try:
            from new import make_html_from_csv
            make_html_from_csv(final_csv_path)
        except Exception:
            pass
        cent_csv = _find_centrality_csvs(final_csv_path)
    if cent_csv is None:
        return pd.DataFrame(), pd.DataFrame()

    # --- ì¤‘ì•™ì„± ë¡œë“œ ---
    cent = pd.read_csv(cent_csv, dtype={"org": str})
    if "org" not in cent.columns:
        return pd.DataFrame(), pd.DataFrame()
    # closenessê¹Œì§€ ìë™ í¬í•¨ (ìˆì„ ë•Œë§Œ)
    candidates = [c for c in ["degree","eigenvector","betweenness","closeness"] if c in cent.columns]
    if not candidates:
        return pd.DataFrame(), pd.DataFrame()

    # ë…¸ë“œ Top10(ê° centrality ë³„)
    nodes_cols = {}
    for c in candidates:
        tmp = cent[["org", c]].dropna()
        tmp = tmp.sort_values(c, ascending=False)
        names = tmp["org"].astype(str).head(10).tolist()
        nodes_cols[f"{c}: ë…¸ë“œ"] = names + [""]*(10-len(names))
    df_nodes = pd.DataFrame(nodes_cols, index=[f"Top {i}" for i in range(1,11)])

    # --- ìµœì¢… CSVì—ì„œ ì—£ì§€(ë™ì‹œì¶œí˜„) ê³„ì‚° ---
    final_df = pd.read_csv(final_csv_path, dtype=str, encoding="utf-8-sig")
    if "org_names" not in final_df.columns:
        return df_nodes, pd.DataFrame()

    def _as_list(s):
        if s is None or str(s).strip()=="":
            return []
        if isinstance(s, list):
            return [str(x).strip() for x in s if str(x).strip()]
        txt = str(s).strip()
        # ë¦¬ìŠ¤íŠ¸ ë¦¬í„°ëŸ´ ìš°ì„ 
        try:
            v = ast.literal_eval(txt)
            if isinstance(v, list):
                return [str(x).strip() for x in v if str(x).strip()]
        except Exception:
            pass
        # êµ¬ë¶„ì
        for sep in (";", "|", ","):
            if sep in txt:
                return [t.strip() for t in txt.split(sep) if t.strip()]
        return [txt] if txt else []

    edge_counter = Counter()
    for orgs in final_df["org_names"].apply(_as_list):
        # ê°™ì€ ë…¼ë¬¸ ë‚´ ì¤‘ë³µ ì œê±°
        uniq = sorted(set([o for o in orgs if o]))
        if len(uniq) < 2:
            continue
        for a, b in itertools.combinations(uniq, 2):
            pair = f"{a} â€” {b}" if a <= b else f"{b} â€” {a}"
            edge_counter[pair] += 1

    top_edges = [k for k, _ in edge_counter.most_common(10)]
    # ì—£ì§€ëŠ” centralityì™€ ë¬´ê´€í•˜ê²Œ â€œë™ì‹œì¶œí˜„ìˆ˜â€ ê¸°ì¤€ìœ¼ë¡œ ë™ì¼ ëª©ë¡ì„ ê° colì— ì‚¬ìš©
    edges_cols = {f"{c}: ì—£ì§€": top_edges + [""]*(10-len(top_edges)) for c in candidates}
    df_edges = pd.DataFrame(edges_cols, index=[f"Top {i}" for i in range(1,11)])

    # candidatesê°€ 2ê°œ/1ê°œì¸ ê²½ìš°ë„ ì—´ ì •ë ¬ ë³´ì •
    order = []
    for c in ["degree","eigenvector","betweenness", "closeness"]:
        if f"{c}: ë…¸ë“œ" in df_nodes.columns:
            order.append(f"{c}: ë…¸ë“œ")
        if f"{c}: ì—£ì§€" in df_edges.columns:
            order.append(f"{c}: ì—£ì§€")
    df_nodes = df_nodes[[c for c in [f"{m}: ë…¸ë“œ" for m in ["degree","eigenvector","betweenness", "closeness"]] if c in df_nodes.columns]]
    df_edges = df_edges[[c for c in [f"{m}: ì—£ì§€" for m in ["degree","eigenvector","betweenness", "closeness"]] if c in df_edges.columns]]
    return df_nodes, df_edges


def sidebar_controls():
    st.sidebar.header("ì„¤ì •")
    issns_text = st.sidebar.text_area(
        "ISSN ë¦¬ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
        value="0043-1354\n0011-9164\n0733-9429",
        height=100,
    )
    issns = [s.strip() for s in issns_text.splitlines() if s.strip()]

    year_col1, year_col2 = st.sidebar.columns(2)
    year_start = year_col1.number_input("ì‹œì‘ ì—°ë„", value=2015, step=1)
    year_end = year_col2.number_input("ì¢…ë£Œ ì—°ë„", value=2024, step=1)

    email = st.sidebar.text_input("OpenAlex ì´ë©”ì¼", value="s0124kw@gmail.com")
    run_btn = st.sidebar.button("ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰", width="stretch")
    return issns, int(year_start), int(year_end), email, run_btn


def app():
    # st.title("Paper Metadata Collection Pipeline")
    st.title("Paper-Based Inter-Institutional Connectivity Analysis Pipeline")
    if "runs" not in st.session_state:
        st.session_state.runs = []
    # --- NEW: ì‹¤í–‰ ìƒíƒœ ê¸°ë³¸ê°’ ---
    if "_job_state" not in st.session_state:
        st.session_state._job_state = "idle"   # idle | running | done
    if "_job" not in st.session_state:
        st.session_state._job = None
    if "view" not in st.session_state:
        st.session_state.view = "Preview"

    issns, year_start, year_end, email, run_btn = sidebar_controls()



    # ì§„í–‰ë¥ /ë©”ì‹œì§€ ìŠ¬ë¡¯ -> ì§„í–‰ì¤‘ì¸ ìƒí™© ì¶œë ¥
    progress_bar = st.progress(0, text="ëŒ€ê¸° ì¤‘")
    progress_text = st.empty()

    # Preview í–‰ ìˆ˜(ê³ ì •)
    PREVIEW_N = 1000
    # CSV ë‹¤ìš´ë¡œë“œ ì„ê³„ê°’(MB)
    THRESHOLD_MB = 150

    # === ê³µí†µ Summary ë Œë”ë§ í•¨ìˆ˜ ===
    def _render_summary_block(sel_csv_or_list):
        """
        - ë‹¨ì¼ íŒŒì¼(str)  : _summary_metrics_common() ì‚¬ìš©
        - ë‹¤ì¤‘ íŒŒì¼(list): _summary_metrics_common_multi() ì‚¬ìš©
        - ê³µí†µìš”ì•½ì§€í‘œ ì•„ë˜ì— Top10 ë…¸ë“œ/ì—£ì§€ í‘œë¥¼ ë‹¨ì¼/ë©€í‹° ëª¨ë‘ì—ì„œ ë Œë”
        """

        with st.expander("Summary", expanded=True):
            st.subheader("ê³µí†µ ìš”ì•½ ì§€í‘œ")

            if isinstance(sel_csv_or_list, (list, tuple)):
                csv_list = list(sel_csv_or_list)
                if len(csv_list) == 0:
                    st.info("ìµœì¢… CSVë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
                    return
                summary_df = _summary_metrics_common_multi(csv_list)
                download_name = "multi_selection_summary.csv"
            else:
                sel_csv = sel_csv_or_list
                if not sel_csv:
                    st.info("ìµœì¢… CSVë¥¼ ì„ íƒí•˜ì„¸ìš”.")
                    return
                summary_df = _summary_metrics_common(sel_csv)
                download_name = Path(sel_csv).stem + "_summary.csv"

            if summary_df is None or summary_df.empty:
                st.warning("ìš”ì•½í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            st.dataframe(summary_df, use_container_width=True)

            # ë‹¤ìš´ë¡œë“œ(ë©”ëª¨ë¦¬ ì§í–‰, ì €ì¥ X)
            csv_bytes = summary_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "ìš”ì•½ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_bytes,
                file_name=download_name,
                mime="text/csv",
            )

            # ---- ì¤‘ì‹¬ì„±ë³„ Top10 ë…¸ë“œ&ì—£ì§€ ----
            # ë‹¨ì¼ CSV ì„ íƒì‹œì—ë§Œ í‘œì‹œ (ë©€í‹° ì„ íƒì€ í•©ì‚° ê¸°ì¤€ì´ ì• ë§¤í•˜ë¯€ë¡œ ìˆ¨ê¹€)
            if not isinstance(sel_csv_or_list, (list, tuple)):
                sel_csv = sel_csv_or_list
                nodes_wide, edges_wide = _summary_top_nodes_and_edges(sel_csv)
                st.markdown("### ì¤‘ì‹¬ì„±ë³„ Top 10 ë…¸ë“œ&ì—£ì§€")
                colA, colB = st.columns(2)
                with colA:
                    if not nodes_wide.empty:
                        st.dataframe(nodes_wide, use_container_width=True)
                    else:
                        st.info("ì¤‘ì•™ì„± CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì»¬ëŸ¼ ëˆ„ë½ìœ¼ë¡œ ë…¸ë“œ Top10ì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                with colB:
                    if not edges_wide.empty:
                        st.dataframe(edges_wide, use_container_width=True)
                    else:
                        st.info("ìµœì¢… CSVì˜ org_namesë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì—£ì§€ Top10ì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def _render_summary_unified(sel):
        """
        í•­ìƒ 'ì„ íƒëœ ì‹¤í–‰'ë§Œ ìš”ì•½í•©ë‹ˆë‹¤.
        ì ˆëŒ€ë¡œ ì—¬ëŸ¬ ì‹¤í–‰ì„ í•©ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """
        _render_summary_block(sel["csv"])


    # === ê³µí†µ ë‹¤ìš´ë¡œë“œ ë¸”ë¡(ì‹œê°í™” í¬í•¨) ===
    def _render_download_block(sel_csv: str, sel_html: str):
        csv_path = Path(sel_csv)
        file_size_mb = csv_path.stat().st_size / (1024 * 1024)

        col_csv, col_html = st.columns(2)
        with col_csv:
            if file_size_mb <= THRESHOLD_MB:
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ ({file_size_mb:.1f} MB)",
                    data=csv_path.read_bytes(),
                    file_name=csv_path.name,
                    mime="text/csv",
                    width="stretch",
                )
            else:
                buf = BytesIO()
                with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
                    zf.writestr(csv_path.name, csv_path.read_bytes())
                zip_bytes = buf.getvalue()
                st.download_button(
                    label=f"CSV ë‹¤ìš´ë¡œë“œ (ZIP, ì›ë³¸ {file_size_mb:.1f} MB)",
                    data=zip_bytes,
                    file_name=csv_path.stem + ".zip",
                    mime="application/zip",
                    width="stretch",
                )

        with col_html:
            # ë„¤ ê°€ì§€ ì¤‘ì‹¬ì„± ì§€í‘œë³„ ë„¤íŠ¸ì›Œí¬ HTMLì„ ê°ê° ë‹¤ìš´ë¡œë“œ
            items = [
                ("degree",      "Degree-based",      "_degree_network.html"),
                ("eigenvector", "Eigenvector-based", "_eigenvector_network.html"),
                ("betweenness", "Betweenness-based", "_betweenness_network.html"),
                ("closeness",   "Closeness-based",   "_closeness_network.html"),
            ]

            for metric, label, suffix in items:
                html_str = make_html_string_from_csv(str(csv_path), size_by=metric, color_by=metric)
                st.download_button(
                    label=f"Visualization ë‹¤ìš´ë¡œë“œ ({label})",
                    data=html_str.encode("utf-8"),
                    file_name=csv_path.stem + suffix,
                    mime="text/html",
                    width = "stretch",
                )


    # === ê³µí†µ Preview(ìƒìœ„ 1,000í–‰) ë¸”ë¡ ===
    def _render_preview_block(sel_csv: str):
        df = pd.read_csv(sel_csv)
        total_rows = len(df)
        st.caption(
            f"âš ï¸ ì•„ë˜ í‘œëŠ” ì´ {total_rows:,}í–‰ ì¤‘ "
            f"ìƒìœ„ {min(PREVIEW_N, total_rows):,}í–‰ë§Œ Previewì…ë‹ˆë‹¤. "
            "'ë‹¤ìš´ë¡œë“œ' íƒ­ì—ì„œ íŒŒì¼ì„ ë‚´ë ¤ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        st.dataframe(df.head(PREVIEW_N), width="stretch")

        # â–¼ ë„¤íŠ¸ì›Œí¬ Preview(ì„ë² ë“œ) â€” degree / eigenvector
        with st.expander("ë„¤íŠ¸ì›Œí¬ Preview (Degree / Eigenvector / Betweenness / Closeness)", expanded=False):
            st.caption("â€» ì•ˆë‚´: 'Summary' íƒ­ì˜ ê³µí†µ ìš”ì•½ í‘œì— í‘œì‹œë˜ëŠ” 'ë…¸ë“œ/ì—£ì§€ ìˆ˜'ëŠ” **ìµœì¢… CSV ì „ì²´**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. "
                        "ì•„ë˜ ë¯¸ë¦¬ë³´ê¸° ë„¤íŠ¸ì›Œí¬ëŠ” **Centrality ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë…¸ë“œë“¤ì„ ê°•ì¡°/í•„í„°ë§**í•˜ì—¬ "
                        "ìš”ì•½ì˜ ìˆ˜ì¹˜ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ëŠ” 'ë‹¤ìš´ë¡œë“œ' íƒ­ì—ì„œ HTMLë¡œ ë‹¤ìš´ë¡œë“œ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            tabs = st.tabs(["Degree ë„¤íŠ¸ì›Œí¬", "Eigenvector ë„¤íŠ¸ì›Œí¬", "Betweenness ë„¤íŠ¸ì›Œí¬", "Closeness ë„¤íŠ¸ì›Œí¬"])

            # HTML ë¬¸ìì—´ ìƒì„± (ê¸°ì¡´ í—¬í¼ ì¬ì‚¬ìš©)
            deg_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="degree",       color_by="degree")
            eig_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="eigenvector",  color_by="eigenvector")
            bet_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="betweenness",  color_by="betweenness")
            clo_html = make_html_string_from_csv(str(Path(sel_csv)), size_by="closeness",    color_by="closeness")


            with tabs[0]:
                components.html(deg_html, height=800, scrolling=True)
            with tabs[1]:
                components.html(eig_html, height=800, scrolling=True)
            with tabs[2]:
                components.html(bet_html, height=800, scrolling=True)
            with tabs[3]:
                components.html(clo_html, height=800, scrolling=True)

    # === ì‹¤í–‰ ì—¬ë¶€ì— ë”°ë¼ ê²°ê³¼ ë Œë” ===

    # A) ë²„íŠ¼ í´ë¦­ ì‹œ: ìƒíƒœë§Œ ì„¸íŒ…í•˜ê³  ì¦‰ì‹œ ì¬ì‹¤í–‰ â†’ í™”ë©´ì„ 'ì‹¤í–‰ ì¤‘' ìƒíƒœë¡œ ê³ ì •
    if run_btn:
        st.session_state._job = {
            "issns": issns,
            "year_start": int(year_start),
            "year_end": int(year_end),
            "email": email,
        }
        st.session_state._job_state = "running"
        
        #ë¬´í•œ ë¦¬ëŸ° ë°©ì§€
        if not st.session_state.get("_reran_once"):
            st.session_state["_reran_once"] = True
            st.rerun()

    # B) ì¬ì‹¤í–‰ í›„: ìƒíƒœë¥¼ ë³´ê³  ì‹¤ì œ ì‘ì—… ìˆ˜í–‰ (ì¤‘ê°„ì— ì•±ì´ ì¬ì‹¤í–‰ë¼ë„ ê³„ì† 'ì‹¤í–‰ ì¤‘' ìœ ì§€)
    if st.session_state._job_state == "running" and st.session_state._job:
        params = st.session_state._job
        start_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

        with st.spinner("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)"):
            progress_text.info("ì¤€ë¹„ ì¤‘..")
            final_csv, final_html = cached_run(
                params["issns"], params["year_start"], params["year_end"], params["email"],
                progress_bar=progress_bar, progress_text=progress_text
            )

        # ì¢…ë£Œ ì‹œê°„/ë¼ë²¨ ê¸°ë¡
        end_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))
        duration = end_time - start_time
        prefix = Path(final_csv).stem.replace(
            f"_{params['year_start']}_{params['year_end']}_ror_extract_name", ""
        )
        label = f"{prefix} | {params['year_start']}-{params['year_end']} | {start_time.strftime('%m/%d %H:%M')}"

        st.session_state.runs.append({
            "label": label, "csv": final_csv, "html": final_html,
            "start": start_time.strftime('%Y-%m-%d %H:%M:%S'),
            "end": end_time.strftime('%Y-%m-%d %H:%M:%S'),
            "duration": str(duration)
        })

        # ì‹¤í–‰ ì¢…ë£Œ í‘œì‹œ ë° ìƒíƒœ ì´ˆê¸°í™”
        st.session_state._job_state = "done"
        st.session_state._job = None

        # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ê°€ë“œ í•´ì œ
        st.session_state.pop("_reran_once", None)

        # ë°©ê¸ˆ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë Œë”(ê¸°ì¡´ UI íŒ¨í„´ ìœ ì§€)
        sel = st.session_state.runs[-1]
        st.success(f"ì‹¤í–‰ ì™„ë£Œ: {sel['label']}")

        st.sidebar.markdown("---")
        st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
        st.sidebar.write(f" ì‹œì‘ {sel['start']}")
        st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
        st.sidebar.write(f" ì†Œìš” {sel['duration']}")

        # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
        view = st.radio(
            "ë³´ê¸° ì„ íƒ",
            ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
            index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
            key="_view_radio",
            horizontal=True,
            on_change=_on_change_view,
        )
        view = st.session_state.view

        if view == "Preview":
            _render_preview_block(sel["csv"])
        if view == "ë‹¤ìš´ë¡œë“œ":
            _render_download_block(sel["csv"], sel["html"])
        if view == "Summary":
            _render_summary_unified(sel)

        # ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€(ì„ íƒ ìœ ì§€ìš©)
        st.session_state["last_csv"] = sel["csv"]


    else:
        # ì´ì „ ì‹¤í–‰ ì´ë ¥ ì„ íƒ/í‘œì‹œ
        if st.session_state.runs:
            st.markdown("### ì´ì „ ì‹¤í–‰ ê²°ê³¼ ë³´ê¸°")
            options = [r["label"] for r in st.session_state.runs]
            selected = st.selectbox("ì‹¤í–‰ ê²°ê³¼ ì„ íƒ", options, index=len(options)-1)
            sel = next(r for r in st.session_state.runs if r["label"] == selected)

            # ì‚¬ì´ë“œë°”ì— ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
            st.sidebar.markdown("---")
            st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
            st.sidebar.write(f" ì‹œì‘ {sel['start']}")
            st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
            st.sidebar.write(f" ì†Œìš” {sel['duration']}")

            # âœ… ë¼ë””ì˜¤ ë°•ìŠ¤ ë³µì›: Preview / ë‹¤ìš´ë¡œë“œ / Summary
            view = st.radio(
                "ë³´ê¸° ì„ íƒ",
                ["Preview", "ë‹¤ìš´ë¡œë“œ", "Summary"],
                index=["Preview","ë‹¤ìš´ë¡œë“œ","Summary"].index(st.session_state.view),
                key="_view_radio",
                horizontal=True,
                on_change=_on_change_view,
            )
            view = st.session_state.view

            if view == "Preview":
                _render_preview_block(sel["csv"])
            if view == "ë‹¤ìš´ë¡œë“œ":
                _render_download_block(sel["csv"], sel["html"])
            if view == "Summary":
                _render_summary_unified(sel)

            # ìµœê·¼ ì„ íƒ CSVë¥¼ ì„¸ì…˜ì— ë³´ê´€(ì„ íƒ ìœ ì§€ìš©)
            st.session_state["last_csv"] = sel["csv"]
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")



if __name__ == "__main__":
    app()
