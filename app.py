import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"

import streamlit as st
import nest_asyncio
import pandas as pd
import datetime
from pathlib import Path
from zoneinfo import ZoneInfo
from io import BytesIO
import zipfile
import json
import re

from new import run_pipeline, make_html_from_csv, run_pipeline_cached, make_html_string_from_csv

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
now_kst = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

# ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ë°©ì§€ (async ì‚¬ìš© ì‹œ í•„ìˆ˜)
nest_asyncio.apply()


st.set_page_config(page_title="OpenAlex Paper Metadata Pipeline", layout="wide")


# ìºì‹œ ë°ì½”ë ˆì´í„° ì œê±° (íŒŒì¼ ìƒì„±/ì™¸ë¶€ I/OëŠ” ìºì‹œ ë¹„ê¶Œì¥)
def cached_run(issns, year_start, year_end, email,
               progress_bar=None, progress_text=None):

    # storage í•˜ìœ„ì— ê²°ê³¼ ì €ì¥
    final_csv_path, _ = run_pipeline_cached(
        issns=issns, year_start=year_start, year_end=year_end, email=email,
        include_only_with_abstract=False, make_html=True,
        base_dir=Path("storage")
    )

    if progress_bar:
        progress_bar.progress(0.7)
    if progress_text:
        progress_text.info("ìµœì¢… CSV ìƒì„± ë° í™•ì¸ ì¤‘ ..")

    #ìµœì¢… CSV ê²½ë¡œ ë¬¸ìì—´ -> Path ê°ì²´
    out_path = Path(final_csv_path)

    final_html = make_html_from_csv(str(out_path))

    if progress_bar:
        progress_bar.progress(1.0)
    if progress_text:
        progress_text.success("ëª¨ë“  êµ¬ê°„ ì²˜ë¦¬ ë° ìµœì¢… íŒŒì¼ ìƒì„± ì™„ë£Œ")

    st.sidebar.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ Â· {now_kst:%H:%M:%S}")

    return str(out_path), final_html

def _summary_metrics_table(final_csv_path: str, centrality: str = "eigenvector"):
    import pandas as pd
    from pathlib import Path
    import json, re

    final_csv = Path(final_csv_path)                        # e.g., .../desalination_1992_1992_ror_extract_name.csv
    base_path = final_csv.with_suffix("")                   # .../desalination_1992_1992_ror_extract_name
    base = str(base_path)

    # ì‚°ì¶œë¬¼ ê²½ë¡œ (ìˆì„ ìˆ˜ë„/ì—†ì„ ìˆ˜ë„ ìˆìŒ: ë³‘í•© CSV ì„ íƒ ì‹œ json/csvê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
    json_merged = Path(f"{base}.json")                     # ìˆ˜ì§‘+ë³´ê°• í•©ë³¸ JSON(ë‹¨ì¼ ì¡°ê°ì¼ ë•Œ ì£¼ë¡œ ìˆìŒ)
    csv_file    = Path(f"{base}.csv")                      # ì¡°ê° CSV (ì—†ì„ ìˆ˜ ìˆìŒ; ë³‘í•© CSVë©´ ì—†ìŒ)
    ror_ex_csv  = Path(f"{base}_ror_extract.csv")          # ROR ì¶”ì¶œ CSV (ì—†ì„ ìˆ˜ ìˆìŒ; ë³‘í•© CSVë©´ ì—†ìŒ)

    # ë³‘í•© CSV ëŒ€ë¹„: ì—°ë„/ì €ë„ prefix ë¶„í•´
    year_m = re.search(r"_(\d{4})_(\d{4})$", base)
    prefixes = []
    y0 = y1 = None
    if year_m:
        y0, y1 = int(year_m.group(1)), int(year_m.group(2))
        prefix_all = Path(base).name[:-(len(year_m.group(0)))]  # e.g., "desalination" or "desalination-water_research"
        prefixes = prefix_all.split("-") if prefix_all else []

    # per-year metrics.json í•©ì‚° í—¬í¼
    def sum_metrics(keys):
        acc = {k: 0 for k in keys}
        found = False
        if y0 is None or y1 is None or not prefixes:
            return acc, found
        for y in range(y0, y1+1):
            for px in prefixes:
                mf = Path(f"{px}_{y}_{y}_metrics.json")
                if mf.exists():
                    try:
                        mm = json.loads(mf.read_text(encoding="utf-8")) or {}
                        for k in keys:
                            acc[k] += int(mm.get(k, 0)) if mm.get(k) is not None else 0
                        found = True
                    except Exception:
                        pass
        return acc, found

    # =========================
    # 1) ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜ & authorships==[] ì‚­ì œ ìˆ˜
    # =========================
    total_collected = None
    removed_authorships_empty = None
    if json_merged.exists():
        with open(json_merged, "r", encoding="utf-8") as f:
            works = json.load(f)
        total_collected = len(works)
        removed_authorships_empty = sum(
            1 for w in works
            if not isinstance(w.get("authorships"), list) or len(w.get("authorships") or []) == 0
        )
    else:
        # ë³‘í•© CSV ë“±: per-year metrics í•©ì‚°ìœ¼ë¡œ ëŒ€ì²´
        acc, found = sum_metrics(["json_rows", "authorships_removed_empty_list"])
        if found:
            total_collected = acc["json_rows"]
            removed_authorships_empty = acc["authorships_removed_empty_list"]

    # =========================
    # 2) DOI ê²°ì¸¡(ìµœì¢…) & DOI ë³´ê°• ìˆ˜/ë³´ê°•ë¥ 
    # =========================
    doi_missing_final = None
    if csv_file.exists():
        df_csv = pd.read_csv(csv_file, dtype={"doi": str})
        doi_missing_final = df_csv["doi"].isna().sum() + (df_csv["doi"].astype(str).str.strip() == "").sum()

    doi_missing_initial = None
    doi_enriched = None
    doi_enrich_rate = None
    # per-year metricsì˜ doi_missing_initial / doi_enriched_from_empty í•©ì‚°
    acc, found = sum_metrics(["doi_missing_initial", "doi_enriched_from_empty"])
    if found:
        doi_missing_initial = acc["doi_missing_initial"]
        doi_enriched = acc["doi_enriched_from_empty"]
        doi_enrich_rate = (doi_enriched / doi_missing_initial) if doi_missing_initial > 0 else None

    # =========================
    # 3) ROR ê²°ì¸¡/ë³´ê°• ìˆ˜/ë³´ê°•ë¥ 
    # =========================
    # ë§¤í•‘ ì „ ê²°ì¸¡(before): authorshipsì—ì„œ ROR URL ì •ê·œì‹ìœ¼ë¡œ ì¹´ìš´íŠ¸
    rx = re.compile(r'https?://ror\.org/[0-9a-z]+', re.IGNORECASE)

    def _extract_ror_list(s):
        if not isinstance(s, str):
            return []
        return list(dict.fromkeys(rx.findall(s)))

    ror_missing_before = None
    # ìš°ì„  í•©ë³¸ CSV ê¸°ì¤€
    if csv_file.exists():
        tmp = pd.read_csv(csv_file, dtype={"authorships": str})
        ror_before = tmp["authorships"].apply(_extract_ror_list)
        ror_missing_before = (ror_before.str.len() == 0).sum()
    else:
        # ë³‘í•© CSVì˜ ê²½ìš° per-year ì¡°ê° CSVì—ì„œ í•©ì‚°
        if y0 is not None and y1 is not None and prefixes:
            total_missing_before = 0
            found_any = False
            for y in range(y0, y1+1):
                for px in prefixes:
                    piece_csv = Path(f"{px}_{y}_{y}.csv")
                    if piece_csv.exists():
                        try:
                            tmp = pd.read_csv(piece_csv, dtype={"authorships": str})
                            ror_before = tmp["authorships"].apply(_extract_ror_list) if "authorships" in tmp.columns else pd.Series([], dtype=object)
                            total_missing_before += int((ror_before.str.len() == 0).sum()) if len(ror_before) else 0
                            found_any = True
                        except Exception:
                            pass
            if found_any:
                ror_missing_before = total_missing_before

    # ë§¤í•‘ í›„ ê²°ì¸¡(after): ror_extract.csv ìˆìœ¼ë©´ ì§ì ‘, ì—†ìœ¼ë©´ per-year metrics í•©ì‚°
    ror_missing_after = None
    if ror_ex_csv.exists():
        try:
            df_ror = pd.read_csv(ror_ex_csv)
            # real4ëŠ” 'ror' ì»¬ëŸ¼ì— ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ ì €ì¥
            get_len = lambda x: (len(eval(x)) if isinstance(x, str) and x.startswith('[') else (len(x) if isinstance(x, list) else 0))
            ror_missing_after = (df_ror["ror"].apply(get_len) == 0).sum() if "ror" in df_ror.columns else None
        except Exception:
            ror_missing_after = None
    else:
        acc, found = sum_metrics(["ror_missing_after_extract"])
        if found:
            ror_missing_after = acc["ror_missing_after_extract"]

    ror_augmented_rows = None
    ror_augment_rate = None
    if (ror_missing_before is not None) and (ror_missing_after is not None):
        ror_augmented_rows = max(0, ror_missing_before - ror_missing_after)
        ror_augment_rate = (ror_augmented_rows / ror_missing_before) if ror_missing_before > 0 else None

    # =========================
    # 4) ë…¸ë“œ/ì—£ì§€ (ì¤‘ì•™ì„± ê¸°ì¤€ â†’ íŒŒì¼ ì—†ìœ¼ë©´ í´ë°±)
    # =========================
    # ì¤‘ì•™ì„± CSV í›„ë³´: ë‹¤ì–‘í•œ ì´ë¦„ & workdir_tmp ë””ë ‰í„°ë¦¬ ì»¤ë²„
    name_candidates = [
        f"{base}_centrality_{centrality}_{centrality}.csv",
        f"{base}_centrality_{centrality}_degree.csv",
        f"{base}_centrality_degree_{centrality}.csv",
        f"{base}_centrality_degree_degree.csv",
        f"{base}_centrality_eigenvector_eigenvector.csv",
        f"{base}_centrality.csv",
        f"{base}_ror_extract_name_centrality.csv",
    ]
    dir_candidates = [final_csv.parent, final_csv.parent / "workdir_tmp"]

    cent_csv = None
    for d in dir_candidates:
        for n in name_candidates:
            cand = (Path(n) if Path(n).is_absolute() else (d / Path(n).name))
            if cand.exists():
                cent_csv = cand
                break
        if cent_csv is not None:
            break

    # org_namesì—ì„œ í˜‘ì—… pair ë§Œë“¤ê¸°
    def _to_pairs(names):
        if not isinstance(names, str):
            return []
        try:
            arr = json.loads(names)
            if not isinstance(arr, list):
                return []
        except Exception:
            return []
        arr = [str(a) for a in arr if a]
        pairs = []
        for i in range(len(arr)):
            for j in range(i+1, len(arr)):
                a, b = sorted((arr[i], arr[j]))
                pairs.append((a, b))
        return pairs

    node_count = edge_count = None
    # final_csv ìì²´ë¥¼ ì½ì–´ pairs ìƒì„± (ë³‘í•©/ë‹¨ì¼ ê³µí†µ)
    df_final = pd.read_csv(final_csv)
    all_pairs = []
    if "org_names" in df_final.columns:
        for pairs in df_final["org_names"].apply(_to_pairs):
            all_pairs.extend(pairs)

    if cent_csv is not None:
        try:
            cent = pd.read_csv(cent_csv)
            nodes_col = "node" if "node" in cent.columns else cent.columns[0]
            node_keep = set(cent[nodes_col].astype(str).tolist())
            node_count = len(node_keep)
            kept_edges = {(a, b) for (a, b) in all_pairs if a in node_keep and b in node_keep}
            edge_count = len(kept_edges)
        except Exception:
            nodes = {n for pair in all_pairs for n in pair}
            node_count = len(nodes)
            edge_count = len(set(all_pairs))
    else:
        nodes = {n for pair in all_pairs for n in pair}
        node_count = len(nodes)
        edge_count = len(set(all_pairs))

    # =========================
    # í‘œ ìƒì„± (1í–‰)
    # =========================
    data = [{
        "ì¤‘ì‹¬ì„± ì§€í‘œ": centrality,
        "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜": total_collected,
        "authorships==[] ì‚­ì œ ìˆ˜": removed_authorships_empty,
        "DOI ê²°ì¸¡ ìˆ˜(ìµœì¢…)": doi_missing_final,
        "DOI ë³´ê°• ìˆ˜(í•©ì‚°)": doi_enriched,
        "DOI ë³´ê°•ë¥ (ê²°ì¸¡ ëŒ€ë¹„)": (None if doi_enrich_rate is None else round(doi_enrich_rate*100, 2)),
        "ROR ê²°ì¸¡ ìˆ˜(ë§¤í•‘ ì „)": ror_missing_before,
        "ROR ê²°ì¸¡ ìˆ˜(ë§¤í•‘ í›„)": ror_missing_after,
        "ROR ë³´ê°• ìˆ˜(í–‰ ê¸°ì¤€)": ror_augmented_rows,
        "ROR ë³´ê°•ë¥ (ê²°ì¸¡ ëŒ€ë¹„)": (None if ror_augment_rate is None else round(ror_augment_rate*100, 2)),
        "ë…¸ë“œ ìˆ˜(ê¸°ê´€ ìˆ˜)": node_count,
        "ì—£ì§€ ìˆ˜(í˜‘ì—… ìˆ˜)": edge_count,
    }]
    df = pd.DataFrame(data)

    # ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥ (None â†’ "-", í¼ì„¼íŠ¸ í¬ë§·)
    percent_cols = ["DOI ë³´ê°•ë¥ (ê²°ì¸¡ ëŒ€ë¹„)", "ROR ë³´ê°•ë¥ (ê²°ì¸¡ ëŒ€ë¹„)"]
    for col in percent_cols:
        if col in df.columns:
            df[col] = df[col].apply(lambda v: "-" if v is None else f"{v:.2f}%")
    num_cols = [
        "ì „ì²´ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜","authorships==[] ì‚­ì œ ìˆ˜","DOI ê²°ì¸¡ ìˆ˜(ìµœì¢…)","DOI ë³´ê°• ìˆ˜(í•©ì‚°)",
        "ROR ê²°ì¸¡ ìˆ˜(ë§¤í•‘ ì „)","ROR ê²°ì¸¡ ìˆ˜(ë§¤í•‘ í›„)","ROR ë³´ê°• ìˆ˜(í–‰ ê¸°ì¤€)",
        "ë…¸ë“œ ìˆ˜(ê¸°ê´€ ìˆ˜)","ì—£ì§€ ìˆ˜(í˜‘ì—… ìˆ˜)"
    ]
    for col in num_cols:
        if col in df.columns:
            df[col] = df[col].apply(lambda v: 0 if v is None else v)
    df = df.fillna("-")
    return df


def sidebar_controls():
    st.sidebar.header("ì„¤ì •")
    issns_text = st.sidebar.text_area(
        "ISSN ë¦¬ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
        value="0043-1354\n0011-9164\n0733-9429",
        height=100,
    )
    issns = [s.strip() for s in issns_text.splitlines() if s.strip()]

    year_col1, year_col2 = st.sidebar.columns(2)
    year_start = year_col1.number_input("ì‹œì‘ ì—°ë„", value=2015, step=1)
    year_end = year_col2.number_input("ì¢…ë£Œ ì—°ë„", value=2024, step=1)

    email = st.sidebar.text_input("OpenAlex ì´ë©”ì¼", value="s0124kw@gmail.com")
    run_btn = st.sidebar.button("ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰", width="stretch")
    return issns, int(year_start), int(year_end), email, run_btn

def app():
    st.title("Paper Metadata Collection Pipeline")

    if "runs" not in st.session_state:
        st.session_state.runs = []

    issns, year_start, year_end, email, run_btn = sidebar_controls()

    # ì§„í–‰ë¥ /ë©”ì‹œì§€ ìŠ¬ë¡¯ -> ì§„í–‰ì¤‘ì¸ ìƒí™© ì¶œë ¥
    progress_bar = st.progress(0, text="ëŒ€ê¸° ì¤‘")
    progress_text = st.empty()

    # ë¯¸ë¦¬ë³´ê¸° í–‰ ìˆ˜(ê³ ì •)
    PREVIEW_N = 1000
    # CSV ë‹¤ìš´ë¡œë“œ ì„ê³„ê°’(MB)
    THRESHOLD_MB = 150

    if run_btn:

        # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
        start_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))

        with st.spinner("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)"):
            progress_text.info("ì¤€ë¹„ ì¤‘..")
            final_csv, final_html = cached_run(issns, year_start, year_end, email,
                                               progress_bar=progress_bar, progress_text=progress_text
                                               )
        
        # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        end_time = datetime.datetime.now(ZoneInfo("Asia/Seoul"))
        duration = end_time - start_time
        
        # ì„¸ì…˜ ì´ë ¥ì— ê¸°ë¡ (ë¼ë²¨: issn|ì—°ë„|ì‹œê°)
        prefix = Path(final_csv).stem.replace(f"_{year_start}_{year_end}_ror_extract_name", "")

        label = f"{prefix} | {year_start}-{year_end} | {start_time.strftime('%m/%d %H:%M')}"
        st.session_state.runs.append({"label": label, "csv": final_csv, "html": final_html,
                                      "start": start_time.strftime('%Y-%m-%d %H:%M:%S'),
                                      "end": end_time.strftime('%Y-%m-%d %H:%M:%S'),
                                      "duration": str(duration)
                                      })

        # âœ… ì´ë ¥ ë“œë¡­ë‹¤ìš´(ê¸°ë³¸ê°’=ë°©ê¸ˆ ì‹¤í–‰í•œ í•­ëª©)
        options = [r["label"] for r in st.session_state.runs]
        selected = st.selectbox("ì‹¤í–‰ ê²°ê³¼ ì„ íƒ", options, index=len(options)-1)
        sel = next(r for r in st.session_state.runs if r["label"] == selected)

        df = pd.read_csv(sel["csv"])
        st.success("íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì„ íƒí•œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì•„ë˜ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")

        # âœ… ìƒìœ„ 1000í–‰ë§Œ í™”ë©´ì— í”„ë¦¬ë·°
        total_rows = len(df)
        st.caption(
            f"âš ï¸ ì•„ë˜ í‘œëŠ” ì´ {total_rows:,}í–‰ ì¤‘ "
            f"ìƒìœ„ {min(PREVIEW_N, total_rows):,}í–‰ë§Œ ë¯¸ë¦¬ë³´ê¸°ì…ë‹ˆë‹¤. "
            "ì „ì²´ëŠ” ì•„ë˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
        )
        st.dataframe(df.head(PREVIEW_N), width="stretch")

        # ì‚¬ì´ë“œë°”ì— ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
        st.sidebar.markdown("---")
        st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
        st.sidebar.write(f" ì‹œì‘ {sel['start']}")
        st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
        st.sidebar.write(f" ì†Œìš” {sel['duration']}")

        # âœ… CSV ë‹¤ìš´ë¡œë“œ: ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ZIP
        csv_path = Path(sel["csv"])
        file_size_mb = csv_path.stat().st_size / (1024 * 1024)

        # html ê²½ë¡œ
        html_path = Path(sel["html"])

        col_csv, col_html = st.columns(2)
        with col_csv:
            if file_size_mb <= THRESHOLD_MB:
                st.download_button(
                    label=f"CSV Download ({file_size_mb:.1f} MB)",
                    data=csv_path.read_bytes(),
                    file_name=csv_path.name,
                    mime="text/csv",
                    width="stretch",
                )
            else:
                buf = BytesIO()
                with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
                    zf.writestr(csv_path.name, csv_path.read_bytes())
                zip_bytes = buf.getvalue()
                st.download_button(
                    label=f"CSV Download (ZIP, ì›ë³¸ {file_size_mb:.1f} MB)",
                    data=zip_bytes,
                    file_name=csv_path.stem + ".zip",
                    mime="application/zip",
                    width="stretch",
                )

        with col_html:
            # Degree ê¸°ë°˜ (í¬ê¸°=degree, ìƒ‰=eigenvector)
            deg_html = make_html_string_from_csv(str(csv_path), size_by="degree", color_by="degree")
            st.download_button(
                label="Visualization Download (Degree-based)",
                data=deg_html.encode("utf-8"),
                file_name=csv_path.stem + "_degree_network.html",
                mime="text/html",
                width="stretch",
            )

            # Eigenvector ê¸°ë°˜ (í¬ê¸°=eigenvector, ìƒ‰=degree)
            eig_html = make_html_string_from_csv(str(csv_path), size_by="eigenvector", color_by="eigenvector")
            st.download_button(
                label="Visualization Download (Eigenvector-based)",
                data=eig_html.encode("utf-8"),
                file_name=csv_path.stem + "_eigenvector_network.html",
                mime="text/html",
                width="stretch",
            )

        # ì‹¤í–‰ ëë‚˜ë©´ ìš”ì•½ ì§€í‘œ í…Œì´ë¸” ì¶œë ¥
        with st.expander("Summary", expanded=True):
            st.subheader("Degree ê¸°ë°˜")
            st.dataframe(_summary_metrics_table(sel["csv"], centrality="degree"), width="stretch")

            st.subheader("Eigenvector ê¸°ë°˜")
            st.dataframe(_summary_metrics_table(sel["csv"], centrality="eigenvector"), width="stretch")



    else:
        if st.session_state.runs:
            options = [r["label"] for r in st.session_state.runs]
            selected = st.selectbox("ì‹¤í–‰ ê²°ê³¼ ì„ íƒ", options, index=len(options)-1)
            sel = next(r for r in st.session_state.runs if r["label"] == selected)

            # âœ… ì„ íƒëœ ì‹¤í–‰ì˜ ì‹œê°„ ë¡œê·¸ -> ì‚¬ì´ë“œë°”ì— í‘œì‹œ (ë“œë¡­ë‹¤ìš´ ë³€ê²½ ì‹œì—ë„ ìœ ì§€)
            st.sidebar.markdown("---")
            st.sidebar.subheader("ì‹¤í–‰ ì‹œê°„")
            st.sidebar.write(f" ì‹œì‘ {sel['start']}")
            st.sidebar.write(f" ì¢…ë£Œ {sel['end']}")
            st.sidebar.write(f" ì†Œìš” {sel['duration']}")

            df = pd.read_csv(sel["csv"])

            # âœ… ìƒìœ„ 1000í–‰ë§Œ í™”ë©´ì— í”„ë¦¬ë·°
            total_rows = len(df)
            st.caption(
                f"âš ï¸ ì•„ë˜ í‘œëŠ” ì´ {total_rows:,}í–‰ ì¤‘ "
                f"ìƒìœ„ {min(PREVIEW_N, total_rows):,}í–‰ë§Œ ë¯¸ë¦¬ë³´ê¸°ì…ë‹ˆë‹¤. "
                "ì „ì²´ëŠ” ì•„ë˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
            )
            st.dataframe(df.head(PREVIEW_N), width="stretch")

            # âœ… CSV ë‹¤ìš´ë¡œë“œ: ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ZIP
            csv_path = Path(sel["csv"])
            file_size_mb = csv_path.stat().st_size / (1024 * 1024)
            html_path = Path(sel["html"])

            col_csv, col_html = st.columns(2)
            with col_csv:
                if file_size_mb <= THRESHOLD_MB:
                    st.download_button(
                        label=f"CSV Download ({file_size_mb:.1f} MB)",
                        data=csv_path.read_bytes(),
                        file_name=csv_path.name,
                        mime="text/csv",
                        width="stretch",
                    )
                else:
                    buf = BytesIO()
                    with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
                        zf.writestr(csv_path.name, csv_path.read_bytes())
                    zip_bytes = buf.getvalue()
                    st.download_button(
                        label=f"CSV Download (ZIP, ì›ë³¸ {file_size_mb:.1f} MB)",
                        data=zip_bytes,
                        file_name=csv_path.stem + ".zip",
                        mime="application/zip",
                        width="stretch",
                    )

            with col_html:
                #Degree ê¸°ë°˜
                deg_html = make_html_string_from_csv(str(csv_path), size_by="degree", color_by="eigenvector")
                st.download_button(
                    label="Visualization Download (Degree-based)",
                    data=deg_html.encode("utf-8"),
                    file_name=csv_path.stem + "_degree_network.html",
                    mime="text/html",
                    width="stretch",
                )

                #Eigenvector ê¸°ë°˜
                eig_html = make_html_string_from_csv(str(csv_path), size_by="eigenvector", color_by="degree")
                st.download_button(
                    label="Visualization Download (Eigenvector-based)",
                    data=eig_html.encode("utf-8"),
                    file_name=csv_path.stem + "_eigenvector_network.html",
                    mime="text/html",
                    width="stretch",
                )
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    app()
