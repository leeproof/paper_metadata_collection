import asyncio
import aiohttp
import json
import ast
import re
import pickle
import time  # ì‹œê°„ ì¸¡ì •ìš©
from pathlib import Path
from tqdm.asyncio import tqdm
import pandas as pd
from collections import defaultdict


# === ADD (ê³µí†µ í—¬í¼) ==========================================
from pathlib import Path
import csv
from numbers import Integral, Real

def _update_metrics(prefix, year_start, year_end, __anchor=None, **updates):
    """
    ë©”íŠ¸ë¦­ì„ CSV í•œ ì¥(key,value)ìœ¼ë¡œë§Œ ê¸°ë¡.
    __anchor í´ë”ê°€ ì—†ìœ¼ë©´ ì¡°ìš©íˆ ìŠ¤í‚µ(ê¸°ì¡´ ì •ì±… ìœ ì§€)
    """
    from pathlib import Path
    import csv
    from numbers import Integral, Real

    def _norm(v):
        try:
            import numpy as np
            if isinstance(v, (np.integer,)): return int(v)
            if isinstance(v, (np.floating,)): return float(v)
        except Exception:
            pass
        if isinstance(v, Integral): return int(v)
        if isinstance(v, Real):     return float(v)
        return v

    updates = {k: _norm(v) for k, v in updates.items()}
    if __anchor is None:  # ê¸°ë¡ ìœ„ì¹˜ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
        return updates

    p = Path(__anchor)
    out_dir = p if p.is_dir() else p.parent
    if not out_dir.exists():
        return updates

    out_csv = out_dir / f"{prefix}_{year_start}_{year_end}_metrics.csv"
    existing = {}
    if out_csv.exists():
        try:
            with out_csv.open("r", encoding="utf-8", newline="") as f:
                rdr = csv.reader(f)
                header = next(rdr, None)
                if header and header[:2] == ["key", "value"]:
                    for row in rdr:
                        if len(row) >= 2:
                            existing[row[0]] = row[1]
        except Exception:
            pass

    for k, v in updates.items():
        existing[k] = "" if v is None else str(v)

    with out_csv.open("w", encoding="utf-8", newline="") as f:
        wr = csv.writer(f)
        wr.writerow(["key", "value"])
        for k in sorted(existing.keys()):
            wr.writerow([k, existing[k]])

    return updates
# =============================================================



# ë°˜ë³µ ì»´íŒŒì¼ ë°©ì§€
SKIP_PAT = re.compile(
                r'\b(ORCID|orcid\.org|Scopus|ResearcherID|Publons|Web of Science|'
                r'Google Scholar|ResearchGate|email|E-mail)\b', re.I
            )

# ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ì •ì œ ì†ë„ ê°œì„ 
PAREN_REGEX = re.compile(r"\([^)]*\)")
PUNCT_REGEX = re.compile(r"[\"',.&:/\\-]")
WHITESPACE_REGEX = re.compile(r"\s+")

def clean_name(name: str) -> str:
    name = name.lower()
    name = PAREN_REGEX.sub("", name)
    name = PUNCT_REGEX.sub(" ", name)
    name = WHITESPACE_REGEX.sub(" ", name).strip()
    return name

async def fetch_ror(session: aiohttp.ClientSession, name: str, cache: dict, sem: asyncio.Semaphore) -> str:
    if name in cache:
        return cache[name]
    url = f"https://api.ror.org/v2/organizations?query={aiohttp.helpers.quote(name)}"
    retries, backoff = 3, 1
    async with sem:
        for attempt in range(1, retries + 1):
            try:
                response = await asyncio.wait_for(session.get(url), timeout=10)
                async with response as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        items = data.get('items', [])
                        if items:
                            best = max(items, key=lambda x: x.get('score', 0))
                            cache[name] = best.get('id', '')
                            return cache[name]
                        cache[name] = ''
                        return ''
                    if resp.status in {429, 500, 502, 503, 504} and attempt < retries:
                        await asyncio.sleep(backoff)
                        backoff *= 2
                    else:
                        cache[name] = ''
                        return ''
            except (asyncio.TimeoutError, aiohttp.ClientError):
                if attempt < retries:
                    await asyncio.sleep(backoff)
                    backoff *= 2
    return ''

# CSV ë‚´ 'authorships' í•„ë“œ JSON/ë¦¬í„°ëŸ´ íŒŒì‹±
def parse_affiliations(raw: str) -> list:
    if not raw or pd.isna(raw):
        return []
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        try:
            return ast.literal_eval(raw)
        except Exception:
            return []

async def process(input_csv, output_csv, cache_file, concurrency=20,
                  anchor_path: str | None = None):
    """
    ìµœì†Œ ë³€ê²½:
      - ê¸°ì¡´ raw_affiliation ê¸°ë°˜ í›„ë³´ ìˆ˜ì§‘/ì¡°íšŒ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€
      - ì—¬ê¸°ì— 'OpenAlex I-ID(https://openalex.org/Ixxxx)'ê°€ ìˆìœ¼ë©´
        OpenAlex Institution APIë¡œ rorë¥¼ 'ì§ì ‘' ê°€ì ¸ì˜¤ëŠ” ì–‡ì€ ê²½ë¡œë¥¼ ì¶”ê°€
    """
    import pandas as pd
    import pickle
    import time
    from pathlib import Path
    import aiohttp
    import asyncio
    from collections import defaultdict
    import json
    import ast
    import re
    from urllib.parse import urlencode
    from tqdm.asyncio import tqdm as atqdm

    start_time = time.perf_counter()
    df = pd.read_csv(input_csv)

    # ìºì‹œ ë¡œë“œ
    cache = {}
    if Path(cache_file).exists():
        try:
            cache = pickle.loads(Path(cache_file).read_bytes())
        except Exception:
            cache = {}

    # ---------------- í›„ë³´ ìˆ˜ì§‘ ----------------
    # (ì¦ì„¤) I-ID í›„ë³´ í…Œì´ë¸”
    iid_to_entities = defaultdict(list)      # inst_id -> [(idx, auth, inst_dict)]
    # (ê¸°ì¡´) raw affiliation í›„ë³´ í…Œì´ë¸”
    name_to_entities = defaultdict(list)     # cleanëœ í† ë§‰/ë¬¸ì¥ -> [(etype, idx, auth, obj)]

    for idx, row in df.iterrows():
        infos = parse_affiliations(row.get('authorships', ''))

        # A) institutions: ror ì—†ê³  I-ID ìˆìœ¼ë©´ ì§í†µ í›„ë³´ ë“±ë¡ (ì‹ ê·œ ì¶”ê°€)
        for auth in infos:
            for inst in auth.get('institutions', []) or []:
                if inst.get('ror'):
                    continue
                inst_id = inst.get('id') or inst.get('openalex') or ""
                if isinstance(inst_id, str) and inst_id.startswith("https://openalex.org/I"):
                    iid_to_entities[inst_id].append((idx, auth, inst))

        # B) affiliations: ê¸°ì¡´ raw ê¸°ë°˜ í›„ë³´ ìˆ˜ì§‘ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        for auth in infos:
            # 1) affiliations[].raw_affiliation_string  ğŸ‘‰ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ROR affiliation ì§ˆì˜ì— ì‚¬ìš©
            for aff in auth.get('affiliations', []):
                raw = aff.get('raw_affiliation_string')
                if raw and not aff.get('ror'):
                    q = str(raw).strip()
                    if len(q) > 3:
                        name_to_entities[q] = name_to_entities.get(q, [])
                        name_to_entities[q].append(('aff', idx, auth, aff))

            # 2) raw_affiliation_strings  ğŸ‘‰ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ(ê°„ë‹¨ ë…¸ì´ì¦ˆë§Œ í•„í„°)
            for raw in auth.get('raw_affiliation_strings', []):
                if not raw:
                    continue
                s = str(raw).strip()
                # URL/ORCID/ì§§ì€ í† ë§‰ ë“±ë§Œ ë°°ì œ(ë„ˆì˜ ê¸°ì¡´ SKIP_PAT ì„±í–¥ ìœ ì§€)
                if 'http' in s.lower() or 'orcid' in s.lower() or len(s) <= 3:
                    continue
                name_to_entities[s] = name_to_entities.get(s, [])
                name_to_entities[s].append(('raw', idx, auth, raw))


    augment_count = 0

    # ---------------- HTTP helpers ----------------
    async def fetch_ror_via_openalex_inst(session, inst_id: str, sem: asyncio.Semaphore) -> str:
        """OpenAlex Institution APIì—ì„œ ror ì§í†µ ì¡°íšŒ (I-ì•„ì´ë””)."""
        if inst_id in cache:
            return cache[inst_id]
        url = inst_id.replace("https://openalex.org/", "https://api.openalex.org/")
        retries, backoff = 3, 1
        async with sem:
            for attempt in range(1, retries + 1):
                try:
                    resp = await asyncio.wait_for(session.get(url), timeout=10)
                    async with resp:
                        if resp.status == 200:
                            data = await resp.json()
                            r = (data or {}).get("ror", "") or ""
                            cache[inst_id] = r
                            return r
                        if resp.status in {429, 500, 502, 503, 504} and attempt < retries:
                            await asyncio.sleep(backoff); backoff *= 2
                        else:
                            cache[inst_id] = ""; return ""
                except (asyncio.TimeoutError, aiohttp.ClientError):
                    if attempt < retries:
                        await asyncio.sleep(backoff); backoff *= 2
        return ""

    async def fetch_ror_via_affiliation(session, aff_text: str, sem: asyncio.Semaphore) -> str:
        """ROR affiliation ë§¤ì¹­ (raw affiliation ë¬¸ì¥)."""
        key = ("aff", aff_text)
        if key in cache:
            return cache[key]
        url = f"https://api.ror.org/organizations?{urlencode({'affiliation': aff_text})}"
        retries, backoff = 3, 1
        async with sem:
            for attempt in range(1, retries + 1):
                try:
                    resp = await asyncio.wait_for(session.get(url), timeout=10)
                    async with resp:
                        if resp.status == 200:
                            data = await resp.json()
                            items = (data or {}).get("items") or []
                            r = (items[0].get("id") if items else "") or ""
                            cache[key] = r
                            return r
                        if resp.status in {429, 500, 502, 503, 504} and attempt < retries:
                            await asyncio.sleep(backoff); backoff *= 2
                        else:
                            cache[key] = ""; return ""
                except (asyncio.TimeoutError, aiohttp.ClientError):
                    if attempt < retries:
                        await asyncio.sleep(backoff); backoff *= 2
        return ""

    # ---------------- ì‹¤í–‰(Aâ†’B) ----------------
    async def _run():
        nonlocal augment_count
        sem = asyncio.Semaphore(concurrency)
        timeout = aiohttp.ClientTimeout(total=None)
        conn = aiohttp.TCPConnector(limit_per_host=concurrency)
        async with aiohttp.ClientSession(timeout=timeout, connector=conn) as session:
            # A) I-ID ì§í†µ (ë¨¼ì €)
            if iid_to_entities:
                tasks_a = {asyncio.create_task(fetch_ror_via_openalex_inst(session, inst_id, sem)): inst_id
                           for inst_id in iid_to_entities}
                async for fut in atqdm(asyncio.as_completed(tasks_a), total=len(tasks_a),
                                       desc="Fetching ROR by OpenAlex I-ID", unit="task"):
                    inst_id = tasks_a[fut]
                    ror_id  = await fut
                    if ror_id:
                        for idx, auth, inst in iid_to_entities[inst_id]:
                            if not inst.get("ror"):
                                augment_count += 1
                            inst["ror"] = ror_id

            # B) affiliation ë¬¸ìì—´ (ê¸°ì¡´ ê²½ë¡œ ê·¸ëŒ€ë¡œ)
            if name_to_entities:
                tasks_b = {asyncio.create_task(fetch_ror_via_affiliation(session, name, sem)): name
                           for name in name_to_entities}
                async for fut in atqdm(asyncio.as_completed(tasks_b), total=len(tasks_b),
                                       desc="Fetching ROR by affiliation", unit="task"):
                    name   = tasks_b[fut]
                    ror_id = await fut
                    if ror_id:
                        for etype, idx, auth, obj in name_to_entities[name]:
                            if etype == 'aff':
                                if not obj.get('ror'):
                                    augment_count += 1
                                obj['ror'] = ror_id
                            elif etype == 'raw':
                                for aff in auth.get('affiliations', []) or []:
                                    if aff.get('raw_affiliation_string') == obj and not aff.get('ror'):
                                        augment_count += 1
                                        aff['ror'] = ror_id

    await _run()

    # ìºì‹œ ì €ì¥
    try:
        Path(cache_file).write_bytes(pickle.dumps(cache))
    except Exception:
        pass

    # DataFrame ë°˜ì˜ ë° ì €ì¥ (ì›ë˜ íë¦„ ìœ ì§€: ì¬íŒŒì‹±ë¡œì§ì´ ìˆì—ˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    for idx, row in df.iterrows():
        updated = False
        infos = parse_affiliations(row.get('authorships', ''))
        for auth in infos:
            for aff in auth.get('affiliations', []) or []:
                if aff.get('ror') is not None:
                    updated = True
        if updated:
            df.at[idx, 'authorships'] = json.dumps(infos, ensure_ascii=False)

    df.to_csv(output_csv, index=False)

    # (ë©”íŠ¸ë¦­ ê¸°ë¡ ë£¨í‹´ì´ ì›ë˜ ìˆì—ˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”)
    elapsed = time.perf_counter() - start_time
    print(f"[real3] ROR ë³´ê°• ì™„ë£Œ: +{augment_count}ê±´, ì†Œìš” {elapsed:.2f}s")



if __name__ == "__main__":
    INPUT_CSV = Path("0723_2015_2024_optimized.csv")      # ë¶ˆëŸ¬ì˜¬ CSV íŒŒì¼ ê²½ë¡œ
    OUTPUT_CSV = Path("0723_2015_2024_optimized_ror.csv")  # ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ
    CACHE_FILE = Path("ror_cache.pkl")                   # ìºì‹œ íŒŒì¼ ê²½ë¡œ
    CONCURRENCY = 20                                      # ë™ì‹œ ìš”ì²­ ìˆ˜
    asyncio.run(process(INPUT_CSV, OUTPUT_CSV, CACHE_FILE, CONCURRENCY))
