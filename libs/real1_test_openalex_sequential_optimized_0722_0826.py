import asyncio
import re
import time
import aiohttp
from pyalex import config
from typing import List, Optional
import platform
import json


def normalize_author_name(author_info):
    """ì €ìëª… ì •ê·œí™”"""
    if isinstance(author_info, dict):
        name = author_info.get('author', {}).get('display_name', '') if 'author' in author_info else author_info.get('display_name', '')
    else:
        name = str(author_info)
    return ' '.join(name.lower().split())

def validate_author_match(openalex_work, crossref_item):
    """ì €ì ìˆœì„œë³„ ë§¤ì¹­ ê²€ì¦"""
    openalex_authors = openalex_work.get('authorships', [])
    crossref_authors = crossref_item.get('author', [])
    
    if len(openalex_authors) < 1 or len(crossref_authors) < 1:
        return False
    
    # 1ì €ì ë§¤ì¹­
    openalex_first = normalize_author_name(openalex_authors[0])
    crossref_first = normalize_author_name({'display_name': ' '.join(filter(None, [crossref_authors[0].get('given'), crossref_authors[0].get('family')]))})
    
    if openalex_first != crossref_first:
        return False
    
    # 2ì €ì ë§¤ì¹­ (ìˆëŠ” ê²½ìš°ì—ë§Œ)
    if len(openalex_authors) > 1 and len(crossref_authors) > 1:
        openalex_second = normalize_author_name(openalex_authors[1])
        crossref_second = normalize_author_name({'display_name': ' '.join(filter(None, [crossref_authors[1].get('given'), crossref_authors[1].get('family')]))})
        return openalex_second == crossref_second
    elif len(openalex_authors) > 1 or len(crossref_authors) > 1:
        return False  # í•œìª½ë§Œ 2ì €ìê°€ ìˆìœ¼ë©´ ë§¤ì¹­ ì‹¤íŒ¨
    
    return True

# 1ï¸âƒ£ ì¡°íšŒí•  í•„ë“œ ì •ì˜ (OpenAlex APIì—ì„œ ê°€ì ¸ì˜¬ ë©”íƒ€ë°ì´í„° í•„ë“œ)
FIELDS = [
    'id',                     # ê³ ìœ  ì‹ë³„ì
    'doi',                    # DOI
    'primary_location',       # ì €ë„ëª…
    'display_name',           # ë…¼ë¬¸ ì œëª©
    'publication_date',       # ì¶œíŒì¼ì
    'authorships',            # ì €ì ì •ë³´
    # 'abstract_inverted_index', # ì´ˆë¡
]

def reconstruct_abstract(inverted_index: dict) -> str:
    
    # OpenAlexì˜ abstract_inverted_indexë¥¼ í‰ë¬¸ ë¬¸ìì—´ë¡œ ë³µì›
    
    if not isinstance(inverted_index, dict) or not inverted_index:
        return ""
    pairs = []
    for word, positions in inverted_index.items():
        if isinstance(positions, list):
            for pos in positions:
                if isinstance(pos, int):
                    pairs.append((pos, word))
    pairs.sort(key=lambda x: x[0])
    return " ".join(w for _, w in pairs)


# Windows í™˜ê²½ì—ì„œë§Œ ProactorEventLoop ì‚¬ìš©

if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# 3ï¸âƒ£ ì •ê·œì‹ íŒ¨í„´ ë¯¸ë¦¬ ì»´íŒŒì¼
_bracket_rx    = re.compile(r"[\[ã€].*?[ã€‘\]]")
_nonascii_rx   = re.compile(r"[^\x00-\x7F]+")
_multispace_rx = re.compile(r"\s+")


def sanitize_title(raw: str) -> str:
    """ì œëª©ì—ì„œ ëŒ€ê´„í˜¸, ë¹„-ASCII, ë‹¤ì¤‘ ê³µë°± ì œê±°"""
    if not raw:
        return ""
    t = _bracket_rx.sub("", raw)
    t = _nonascii_rx.sub(" ", t)
    return _multispace_rx.sub(" ", t).strip()

# 4ï¸âƒ£ authorships ë³€í™˜ í—¬í¼ í•¨ìˆ˜

def process_crossref_authors(authors):
    new_auths = []
    for au in authors:
        full = " ".join(filter(None, [au.get('given'), au.get('family')]))
        new_auths.append({
            "author": {"display_name": full, "orcid": au.get('ORCID')},
            "institutions": [],
            "countries": [],
            "is_corresponding": au.get('sequence') == 'first'
        })
    return new_auths


def process_datacite_authors(creators):
    new_auths = []
    for cr in creators:
        nm = cr.get('name', '')
        new_auths.append({
            "author": {"display_name": nm, "orcid": None},
            "institutions": [],
            "countries": [],
            "is_corresponding": False
        })
    return new_auths

# 5ï¸âƒ£ DOI íŒ¨í„´ í—¬í¼

def normalize_doi(doi: str) -> str:
    return doi.replace("https://doi.org/", "").strip() if doi else ""



# DOI prefix ì •ê·œì‹
_DOI_PREFIX_RX = re.compile(r"^10\.\d{4,9}/", re.I)

def _extract_prefix(doi: str) -> Optional[str]:
    if not doi:
        return None
    doi = doi.lower().replace("https://doi.org/", "").strip()
    m = _DOI_PREFIX_RX.match(doi)
    return m.group(0) if m else None


# ë‹¤ì¤‘ prefix ì¶”ì¶œìš©

def guess_doi_pattern_from_samples(works: List[dict], sample_size: int = 10) -> List[str]:
    dois = [normalize_doi(w.get('doi')) for w in works if w.get('doi')]
    dois = [d for d in dois if d and d.startswith('10.')]
    if not dois:
        return []

    # ê°„ë‹¨ ìƒ˜í”Œë§(ë¶„í¬ì„± í™•ë³´)
    n = len(dois)
    idxs = [0, int(0.25*(n-1)), int(0.5*(n-1)), int(0.75*(n-1)), n-1]
    sample = [dois[i] for i in idxs if 0 <= i < n]

    # prefix ì¶”ì¶œ â†’ ìƒìœ„ ë‹¤ìˆ˜ í›„ë³´ ì„ ì •
    prefixes = []
    for d in sample:
        p = _extract_prefix(d)
        if p:
            prefixes.append(p if p.endswith('/') else p + '/')

    # ì¤‘ë³µ ì œê±°
    prefixes = sorted(set(prefixes))
    return prefixes  # âš ï¸ í•˜ë‚˜ê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê°œ í—ˆìš©


def is_target_journal_doi(doi: str, patterns: List[str]) -> bool:
    d = normalize_doi(doi)
    if not d:
        return False
    if not patterns:
        return d.startswith('10.')
    return any(d.startswith(p) for p in patterns)

# API ì¹œí™” í—¤ë” + ë°±ì˜¤í”„ ìœ í‹¸
DEFAULT_HEADERS = {
    "User-Agent": "OpenAlex-DOI-Enricher/1.0 (+mailto:{})".format("s0124kw@gmail.com")
}

async def _backoff_sleep(attempt: int, base: float = 0.5, cap: float = 8.0):
    """ì§€ìˆ˜ ë°±ì˜¤í”„ + ì†ŒëŸ‰ ì§€í„°"""
    import random
    delay = min(cap, base * (2 ** (attempt - 1)))
    await asyncio.sleep(delay + random.random() * 0.2)

# 6ï¸âƒ£ ë¹„ë™ê¸° ë³´ê°• í•¨ìˆ˜ ì •ì˜
async def enrich_crossref_async(
    session: aiohttp.ClientSession,
    work: dict,
    patterns: List[str],
    max_rows: int = 3,
    total_timeout: float = 10.0,
    max_retries: int = 2
) -> Optional[dict]:
    """Crossref ìš°ì„  ì¡°íšŒ: prefix ì‚¬ì „ ì ìš© + ì €ì/ì œëª© ë§¤ì¹­ ì „ì— prefix ì»·."""
    if work.get('doi'):
        return None

    title_raw = work.get('display_name', '')
    title = sanitize_title(title_raw)  # [CHANGED] ë™ê¸° ì²˜ë¦¬(ê°€ë²¼ì›€)
    if not title:
        return None

    authorships = work.get('authorships', [])
    if len(authorships) < 1:
        return None
    first = authorships[0].get('author', {}).get('display_name', '')

    # prefix í›„ë³´ (ì—¬ëŸ¬ ê°œë©´ ìˆœì°¨ ì‹œë„)
    prefixes = patterns or [None]  # Noneì´ë©´ í•„í„° ì—†ì´ ì‹œë„

    base_params_list = [
        {'query.bibliographic': title, 'query.author': first, 'rows': max_rows},
        {'query.title': title, 'query.author': first, 'rows': max_rows},
    ]

    for prefix in prefixes:
        for params in base_params_list:
            # [ADDED] ì¡°íšŒ ì „ì— prefix í•„í„° ì‚¬ì „ ì ìš©
            call_params = dict(params)
            if prefix:
                call_params['filter'] = f'prefix:{prefix}'

            for attempt in range(1, max_retries + 1):
                try:
                    async with session.get(
                        'https://api.crossref.org/works',
                        params=call_params,
                        timeout=aiohttp.ClientTimeout(total=total_timeout)
                    ) as resp:
                        if resp.status == 200:
                            items = (await resp.json()).get('message', {}).get('items', [])
                            for item in items:
                                doi = item.get('DOI')
                                if not doi:
                                    continue
                                # [ADDED] ì œëª©/ì €ì ë§¤ì¹­ ì „ì— prefix 1ì°¨ ì»· (2ì¤‘ ë°©ì–´)
                                if prefix and not doi.lower().startswith(prefix.lower()):
                                    continue

                                if validate_author_match(work, item):
                                    return {
                                        'work_id': work['id'],
                                        'doi': doi,
                                        'authorships': process_crossref_authors(item.get('author', [])),
                                        'source': 'crossref'
                                    }
                            break  # 200ì´ì§€ë§Œ ìœ íš¨ í›„ë³´ ì—†ìŒ â†’ ë‹¤ìŒ params/prefix ì‹œë„
                        elif resp.status in {429, 500, 502, 503, 504} and attempt < max_retries:
                            await _backoff_sleep(attempt)
                            continue
                        else:
                            break  # ë¹„ì¬ì‹œë„ ì—ëŸ¬
                except (asyncio.TimeoutError, aiohttp.ClientError):
                    if attempt < max_retries:
                        await _backoff_sleep(attempt)
                        continue
                    else:
                        break
    return None


async def enrich_datacite_async(
    session: aiohttp.ClientSession,
    work: dict,
    total_timeout: float = 10.0,
    max_retries: int = 2
) -> Optional[dict]:
    """Crossref ì‹¤íŒ¨ ì‹œì—ë§Œ í˜¸ì¶œë˜ëŠ” í´ë°±."""
    if work.get('doi'):
        return None

    title = sanitize_title(work.get('display_name', ''))
    if not title:
        return None

    for attempt in range(1, max_retries + 1):
        try:
            async with session.get(
                'https://api.datacite.org/works',
                params={'query.title': title, 'page[size]': 1},
                timeout=aiohttp.ClientTimeout(total=total_timeout)
            ) as resp:
                if resp.status == 200:
                    items = (await resp.json()).get('data', [])
                    if items:
                        attr = items[0].get('attributes', {})
                        doi = attr.get('doi')
                        if doi:
                            return {
                                'work_id': work['id'],
                                'doi': doi,
                                'authorships': process_datacite_authors(attr.get('creators', [])),
                                'source': 'datacite'
                            }
                    break
                elif resp.status in {429, 500, 502, 503, 504} and attempt < max_retries:
                    await _backoff_sleep(attempt)
                    continue
                else:
                    break
        except (asyncio.TimeoutError, aiohttp.ClientError):
            if attempt < max_retries:
                await _backoff_sleep(attempt)
                continue
            else:
                break
    return None

# 7ï¸âƒ£ ì „ëŸ‰ì„ í•œêº¼ë²ˆì— ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜

async def enrich_works_all(
    works: List[dict],
    patterns: List[str],
    concurrency: int = 30
) -> None:
    """
    í•œ ë²ˆì˜ ì„¸ì…˜/ë£¨í”„ ì•ˆì—ì„œ:
      1) DOI ë¯¸ë³´ìœ  ì „ì²´ì— ëŒ€í•´ Crossref ë³‘ë ¬
      2) ë‚¨ì€ ê²ƒì— í•œí•´ DataCite í´ë°± ë³‘ë ¬
    """
    connector = aiohttp.TCPConnector(
        limit=concurrency * 2,
        limit_per_host=concurrency,
        force_close=False
    )
    timeout = aiohttp.ClientTimeout(total=30)

    async with aiohttp.ClientSession(
        connector=connector,
        timeout=timeout,
        headers=DEFAULT_HEADERS
    ) as session:
        sem = asyncio.Semaphore(concurrency)

        async def _run_crossref(w):
            async with sem:
                return await enrich_crossref_async(session, w, patterns)

        async def _run_datacite(w):
            async with sem:
                return await enrich_datacite_async(session, w)

        # 1) Crossref ì „ì²´ ë³‘ë ¬
        targets = [w for w in works if not w.get('doi')]
        crossref_results = await asyncio.gather(
            *[_run_crossref(w) for w in targets],
            return_exceptions=True
        )

        # ê²°ê³¼ ë°˜ì˜ (+ post-filter ì•ˆì „ë§)
        resolved_ids = set()
        for r in crossref_results:
            if isinstance(r, dict) and is_target_journal_doi(r.get('doi'), patterns):
                for w in works:
                    if w['id'] == r['work_id']:
                        w.update({'doi': r['doi'], 'authorships': r['authorships']})
                        resolved_ids.add(r['work_id'])

        # 2) ëª» ì±„ìš´ ê²ƒë§Œ DataCite í´ë°±
        remain = [w for w in works if not w.get('doi')]
        datacite_results = await asyncio.gather(
            *[_run_datacite(w) for w in remain],
            return_exceptions=True
        )

        # ê²°ê³¼ ë°˜ì˜ (+ post-filter ì•ˆì „ë§)
        for r in datacite_results:
            if isinstance(r, dict) and is_target_journal_doi(r.get('doi'), patterns):
                for w in works:
                    if w['id'] == r['work_id']:
                        w.update({'doi': r['doi'], 'authorships': r['authorships']})


def main(issns, year_start, year_end, email, prefix='',
         include_only_with_abstract: bool = False):
    from pyalex import Sources, Works
    # 0ï¸âƒ£ ì„¤ì • ì´ˆê¸°í™”
    config.email = email

    # 1ï¸âƒ£ ì €ë„ ID ì¡°íšŒ (ë‹¤ì¤‘ ISSN ì§€ì›)
    journal_ids = []
    for issn in issns:
        src_list = next(
            Sources().filter(issn=[issn])
                     .select(['id','display_name'])
                     .paginate(per_page=5)
        )
        if isinstance(src_list, list):
            journal_id   = src_list[0]['id']
            display_name = src_list[0]['display_name']
        else:
            journal_id   = src_list['id']
            display_name = src_list['display_name']
        journal_ids.append(journal_id)
        print(f"ğŸ’¡ ì„ íƒëœ ì €ë„: {display_name} (ID={journal_id})")

    # 2ï¸âƒ£ ë…¼ë¬¸ ìˆ˜ì§‘ (ì—°ë„ë³„ ë¶„í•  ì¡°íšŒ)
    works = []
    for journal_id in journal_ids:
        for year in range(year_start, year_end+1):
            from_date = f"{year}-01-01"
            to_date   = f"{year}-12-31"
            yearly = []

            works_q = Works().filter(
                journal=journal_id,
                from_publication_date=from_date,
                to_publication_date=to_date
            )
            if include_only_with_abstract:
                works_q = works_q.filter(has_abstract='true')

            for page in works_q.select(FIELDS).paginate(per_page=200):
                yearly.extend(page)

            print(f"âœ… {year}ë…„ [{journal_id}] ìˆ˜ì§‘: {len(yearly)}ê±´")
            works.extend(yearly)

    print(f"âœ… ì „ì²´ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜ (ì¤‘ë³µ ì „): {len(works)}ê±´")

    # 3ï¸âƒ£ ì¤‘ë³µ ì œê±°
    unique_works = list({w['id']: w for w in works}.values())
    print(f"âœ… ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜ (ì¤‘ë³µ ì œê±° í›„): {len(unique_works)}ê±´")

    # [REMOVED] abstract í‰ë¬¸í™” ë¸”ë¡ì€ ì£¼ì„ ì²˜ë¦¬(í•„ìš” ì‹œ ì‚¬ìš©)
    # for w in unique_works:
    #     if w.get('abstract'):
    #         continue
    #     aii = w.get('abstract_inverted_index', {})
    #     if isinstance(aii, dict) and aii:
    #         abs_text = reconstruct_abstract(aii)
    #         abs_text = re.sub(r'\s+', ' ', abs_text).strip()
    #         w['abstract'] = abs_text
    #     else:
    #         w['abstract'] = ""
    #     w.pop('abstract_inverted_index', None)

    # 4ï¸âƒ£ ì´ˆê¸° DOI ìƒíƒœ ë¶„ì„
    empty_ids = [w['id'] for w in unique_works if not w.get('doi')]
    print(f"  â€¢ DOI ë¯¸ë³´ìœ : {len(empty_ids)}ê±´")

    # 5ï¸âƒ£ DOI íŒ¨í„´ ì¶”ì • (patternsëŠ” ì‚¬ì „/ì‚¬í›„ ëª¨ë‘ì—ì„œ ì•ˆì „ë§ ì—­í• )
    patterns = guess_doi_pattern_from_samples(unique_works)

    # 6ï¸âƒ£ [ADDED] ì „ì²´ ë³‘ë ¬ ë³´ê°• (ë°°ì¹˜ X, ë‹¨ í•œ ë²ˆì˜ ë£¨í”„/ì„¸ì…˜)
    print("ğŸ”„ ì „ì²´ DOI ë³´ê°• ì‹œì‘â€¦ (Crossref â†’ DataCite í´ë°±)")
    asyncio.run(enrich_works_all(unique_works, patterns, concurrency=30))

    # 7ï¸âƒ£ ìµœì¢… í†µê³„
    final = sum(1 for w in unique_works if w.get('doi'))
    enriched_from_empty = sum(1 for w in unique_works if w['id'] in empty_ids and w.get('doi'))
    print(f"ğŸ“ˆ ìµœì¢… DOI ë³´ê°•: {final}/{len(unique_works)} ({(final/len(unique_works)*100 if unique_works else 0):.1f}%)")
    print(f"âœ… ì‹¤ì œë¡œ ë³´ê°• ì™„ë£Œí•œ ê±´ìˆ˜: {enriched_from_empty}ê±´")

    # 8ï¸âƒ£ ìµœì¢… í•©ë³¸ ì €ì¥ (ì›ë³¸ ë¡œì§ ìœ ì§€)
    max_per_file = 9500
    out_file = f'{prefix}_{year_start}_{year_end}.json'
    with open(out_file, 'w', encoding='utf-8') as fp:
        fp.write('[\n')
        total = len(unique_works)
        for start in range(0, total, max_per_file):
            for i, item in enumerate(unique_works[start:start+max_per_file]):
                json.dump(item, fp, ensure_ascii=False)
                if start + i + 1 < total:
                    fp.write(',\n')
        fp.write('\n]')
    print(f"âœ… ìµœì¢… í•©ë³¸ ì €ì¥ ì™„ë£Œ: {out_file} ({total}ê±´)")

if __name__ == '__main__':
    start = time.time()
    # ì‹¤í–‰ ì‹œ ì¸ì ì „ë‹¬ (ì›ë³¸ ê¸°ë³¸ê°’ ìœ ì§€)
    issns = ['0043-1354', '0011-9164', '0733-9429']
    year_start = 2015
    year_end = 2024
    email = 's0124kw@gmail.com'
    main(issns, year_start, year_end, email, '', include_only_with_abstract=True)
    print(f"â±ï¸ ì´ ì†Œìš”: {time.time() - start:.1f}s")
