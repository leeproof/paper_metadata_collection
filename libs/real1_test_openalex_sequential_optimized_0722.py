import asyncio
import re
import time
import aiohttp
from pyalex import config
from concurrent.futures import ProcessPoolExecutor
from typing import List, Optional
import pandas as pd
import platform
import json

# === ADD (ê³µí†µ í—¬í¼) ==========================================
from pathlib import Path

def _update_metrics(prefix, year_start, year_end, **updates):
    import os, json, math
    from numbers import Integral, Real
    try:
        import numpy as np
        _HAS_NP = True
    except Exception:
        _HAS_NP = False

    def _jsonable(v):
        if v is None: return None
        if isinstance(v, (bool, int, float, str)): return v
        if _HAS_NP:
            if isinstance(v, np.integer):  return int(v)
            if isinstance(v, np.floating): return None if (isinstance(v, float) and math.isnan(v)) else float(v)
            if isinstance(v, np.bool_):    return bool(v)
        if isinstance(v, Integral): return int(v)
        if isinstance(v, Real):     return float(v)
        if isinstance(v, (list, tuple)): return [_jsonable(x) for x in v]
        if isinstance(v, dict):          return {str(k): _jsonable(val) for k, val in v.items()}
        return str(v)

    # â˜… ì•µì»¤ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ì €ì¥
    anchor = updates.pop("__anchor", None)
    out_dir = Path(anchor).parent if anchor else Path(".")
    path = out_dir / f"{prefix}_{year_start}_{year_end}_metrics.json"

    data = {}
    if path.exists():
        try:
            data = json.loads(path.read_text(encoding="utf-8")) or {}
        except Exception:
            data = {}

    for k, v in updates.items():
        jv = _jsonable(v)
        if jv is not None:
            data[str(k)] = jv

    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")




# =============================================================



def normalize_author_name(author_info):
    """ì €ìëª… ì •ê·œí™”"""
    if isinstance(author_info, dict):
        name = author_info.get('author', {}).get('display_name', '') if 'author' in author_info else author_info.get('display_name', '')
    else:
        name = str(author_info)
    return ' '.join(name.lower().split())

def validate_author_match(openalex_work, crossref_item):
    """ì €ì ìˆœì„œë³„ ë§¤ì¹­ ê²€ì¦"""
    openalex_authors = openalex_work.get('authorships', [])
    crossref_authors = crossref_item.get('author', [])
    
    if len(openalex_authors) < 1 or len(crossref_authors) < 1:
        return False
    
    # 1ì €ì ë§¤ì¹­
    openalex_first = normalize_author_name(openalex_authors[0])
    crossref_first = normalize_author_name({'display_name': ' '.join(filter(None, [crossref_authors[0].get('given'), crossref_authors[0].get('family')]))})
    
    if openalex_first != crossref_first:
        return False
    
    # 2ì €ì ë§¤ì¹­ (ìˆëŠ” ê²½ìš°ì—ë§Œ)
    if len(openalex_authors) > 1 and len(crossref_authors) > 1:
        openalex_second = normalize_author_name(openalex_authors[1])
        crossref_second = normalize_author_name({'display_name': ' '.join(filter(None, [crossref_authors[1].get('given'), crossref_authors[1].get('family')]))})
        return openalex_second == crossref_second
    elif len(openalex_authors) > 1 or len(crossref_authors) > 1:
        return False  # í•œìª½ë§Œ 2ì €ìê°€ ìˆìœ¼ë©´ ë§¤ì¹­ ì‹¤íŒ¨
    
    return True

# 1ï¸âƒ£ ì¡°íšŒí•  í•„ë“œ ì •ì˜ (OpenAlex APIì—ì„œ ê°€ì ¸ì˜¬ ë©”íƒ€ë°ì´í„° í•„ë“œ)
FIELDS = [
    'id',                     # ê³ ìœ  ì‹ë³„ì
    'doi',                    # DOI
    'primary_location',       # ì €ë„ëª…
    'display_name',           # ë…¼ë¬¸ ì œëª©
    'publication_date',       # ì¶œíŒì¼ì
    'authorships',            # ì €ì ì •ë³´
    # 'abstract_inverted_index', # ì´ˆë¡
]

def reconstruct_abstract(inverted_index: dict) -> str:
    
    # OpenAlexì˜ abstract_inverted_indexë¥¼ í‰ë¬¸ ë¬¸ìì—´ë¡œ ë³µì›
    
    if not isinstance(inverted_index, dict) or not inverted_index:
        return ""
    pairs = []
    for word, positions in inverted_index.items():
        if isinstance(positions, list):
            for pos in positions:
                if isinstance(pos, int):
                    pairs.append((pos, word))
    pairs.sort(key=lambda x: x[0])
    return " ".join(w for _, w in pairs)


# Windows í™˜ê²½ì—ì„œë§Œ ProactorEventLoop ì‚¬ìš©

if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# 3ï¸âƒ£ ì •ê·œì‹ íŒ¨í„´ ë¯¸ë¦¬ ì»´íŒŒì¼
_bracket_rx    = re.compile(r"[\[ã€].*?[ã€‘\]]")
_nonascii_rx   = re.compile(r"[^\x00-\x7F]+")
_multispace_rx = re.compile(r"\s+")


def sanitize_title(raw: str) -> str:
    """ì œëª©ì—ì„œ ëŒ€ê´„í˜¸, ë¹„-ASCII, ë‹¤ì¤‘ ê³µë°± ì œê±°"""
    if not raw:
        return ""
    t = _bracket_rx.sub("", raw)
    t = _nonascii_rx.sub(" ", t)
    return _multispace_rx.sub(" ", t).strip()

# 4ï¸âƒ£ authorships ë³€í™˜ í—¬í¼ í•¨ìˆ˜

def process_crossref_authors(authors):
    new_auths = []
    for au in authors:
        full = " ".join(filter(None, [au.get('given'), au.get('family')]))
        new_auths.append({
            "author": {"display_name": full, "orcid": au.get('ORCID')},
            "institutions": [],
            "countries": [],
            "is_corresponding": au.get('sequence') == 'first'
        })
    return new_auths


def process_datacite_authors(creators):
    new_auths = []
    for cr in creators:
        nm = cr.get('name', '')
        new_auths.append({
            "author": {"display_name": nm, "orcid": None},
            "institutions": [],
            "countries": [],
            "is_corresponding": False
        })
    return new_auths

# 5ï¸âƒ£ DOI íŒ¨í„´ í—¬í¼

def normalize_doi(doi: str) -> str:
    return doi.replace("https://doi.org/", "").strip() if doi else ""

def longest_common_prefix(strs: List[str]) -> str:
    if not strs:
        return ""
    shortest = min(strs, key=len)
    for i, ch in enumerate(shortest):
        if any(other[i] != ch for other in strs):
            return shortest[:i]
    return shortest

def guess_doi_pattern_from_samples(works: List[dict], sample_size: int = 10) -> List[str]:
    # 1) DOI ì •ê·œí™”
    dois = [normalize_doi(w['doi']) for w in works if w.get('doi')]
    # 2) '10.'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìœ íš¨ DOIë§Œ í•„í„°
    dois = [d for d in dois if d.startswith('10.')]

    # 3) ìœ„ì¹˜ë³„ 12ê°œ ìƒ˜í”Œë§ (ì²˜ìŒÂ·20%Â·40%Â·60%Â·80%Â·ë§ˆì§€ë§‰ ê° 2ê°œì”©)
    n = len(dois)
    positions = [
        0,
        int(0.2 * (n - 1)),
        int(0.4 * (n - 1)),
        int(0.6 * (n - 1)),
        int(0.8 * (n - 1)),
        n - 1
    ]
    sample = []
    for pos in positions:
        for offset in (0, 1):
            idx = min(pos + offset, n - 1)
            sample.append(dois[idx])

    # 4) ê²°ê³¼ ì¶œë ¥ ë° ê³µí†µ prefix ê³„ì‚°
    print('ì •ê·œí™”ëœ DOI ìƒ˜í”Œ:', sample)
    pattern = longest_common_prefix(sample)
    print('ì¶”ì¶œëœ ê³µí†µ prefix:', pattern)

    return [pattern] if pattern else []


def is_target_journal_doi(doi: str, patterns: List[str]) -> bool:
    d = normalize_doi(doi)
    if not d:
        return False
    if not patterns:
        return d.startswith('10.')
    return any(d.startswith(p) for p in patterns)

# 6ï¸âƒ£ ë¹„ë™ê¸° ë³´ê°• í•¨ìˆ˜ ì •ì˜
async def enrich_crossref_async(session: aiohttp.ClientSession,
                                 work: dict,
                                 executor: ProcessPoolExecutor,
                                 patterns: List[str]) -> Optional[dict]:
    if work.get('doi'):
        return None
    loop = asyncio.get_running_loop()
    title = await loop.run_in_executor(executor, sanitize_title,
                                        work.get('display_name', ''))
    if not title:
        return None
    
    # 1ì €ì í™•ì¸
    authorships = work.get('authorships', [])
    if len(authorships) < 1:
        return None
    
    first = authorships[0].get('author', {}).get('display_name', '')
    
    for params in [
        {'query.bibliographic': title, 'query.author': first, 'rows':5},
        {'query.title': title, 'query.author': first, 'rows':5}
    ]:
        try:
            async with session.get(
                'https://api.crossref.org/works',
                params=params,
                timeout=aiohttp.ClientTimeout(total=15)
            ) as resp:
                if resp.status == 200:
                    items = (await resp.json()).get('message', {}).get('items', [])
                    
                    # ê° ê²°ê³¼ë¥¼ ì €ì ë§¤ì¹­ìœ¼ë¡œ ê²€ì¦
                    for item in items:
                        if validate_author_match(work, item):
                            doi = item.get('DOI')
                            if doi:
                                return {'work_id': work['id'],
                                        'doi': doi,
                                        'authorships': process_crossref_authors(item.get('author', [])),
                                        'source': 'crossref'}
        except Exception:
            continue
    return None


async def enrich_datacite_async(session: aiohttp.ClientSession,
                                 work: dict,
                                 executor: ProcessPoolExecutor,
                                 patterns: List[str]) -> Optional[dict]:
    if work.get('doi'):
        return None
    loop = asyncio.get_running_loop()
    title = await loop.run_in_executor(executor, sanitize_title,
                                        work.get('display_name', ''))
    if not title:
        return None
    try:
        async with session.get(
            'https://api.datacite.org/works',
            params={'query.title': title, 'page[size]':1},
            timeout=aiohttp.ClientTimeout(total=15)
        ) as resp:
            if resp.status == 200:
                items = (await resp.json()).get('data', [])
                if items:
                    attr = items[0].get('attributes', {})
                    doi = attr.get('doi')
                    if doi:
                        return {'work_id': work['id'],
                                'doi': doi,
                                'authorships': process_datacite_authors(
                                    attr.get('creators', [])),
                                'source': 'datacite'}
    except Exception:
        pass
    return None

# 7ï¸âƒ£ ë°°ì¹˜ ë‹¨ìœ„ ë³´ê°• ì‹¤í–‰
async def enrich_works_batch(batch: List[dict],
                              executor: ProcessPoolExecutor,
                              patterns: List[str]) -> None:
    connector = aiohttp.TCPConnector(limit=50, limit_per_host=10,
                                     force_close=False)
    timeout   = aiohttp.ClientTimeout(total=30)
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        tasks = []
        for w in batch:
            if not w.get('doi'):
                tasks.append(enrich_crossref_async(session, w, executor, patterns))
                tasks.append(enrich_datacite_async(session, w, executor, patterns))
        results = await asyncio.gather(*tasks, return_exceptions=True)
        # ìˆ˜ì •: íŒ¨í„´ì— ë§ì§€ ì•ŠëŠ” DOIëŠ” ìë™ ì œì™¸
        enhanced = {
            r['work_id']: r
            for r in results
            if isinstance(r, dict)
            and is_target_journal_doi(r['doi'], patterns)
        }
        for w in batch:
            if w['id'] in enhanced:
                r = enhanced[w['id']]
                w.update({'doi': r['doi'], 'authorships': r['authorships']})
                print(f"ğŸ”– ë³´ê°• ì™„ë£Œ: {w['id']} â†’ DOI={r['doi']} (ì†ŒìŠ¤: {r['source']})")

def main(issns, year_start, year_end, email, prefix='', 
         include_only_with_abstract: bool = False):
    import asyncio
    from pyalex import Sources, Works, config
    from concurrent.futures import ProcessPoolExecutor
    import json
    import re
    # 0ï¸âƒ£ ì„¤ì • ì´ˆê¸°í™”
    config.email = email
    BATCH_SIZE = 50

    # 1ï¸âƒ£ ProcessPoolExecutor ì‚¬ìš©
    executor = ProcessPoolExecutor()

    # 2ï¸âƒ£ ì €ë„ ID ì¡°íšŒ (ë‹¤ì¤‘ ISSN ì§€ì›)
    journal_ids = []
    for issn in issns:
        src_list = next(
            Sources().filter(issn=[issn])
                     .select(['id','display_name'])
                     .paginate(per_page=5)
        )
        if isinstance(src_list, list):
            journal_id   = src_list[0]['id']
            display_name = src_list[0]['display_name']
        else:
            journal_id   = src_list['id']
            display_name = src_list['display_name']
        journal_ids.append(journal_id)
        print(f"ğŸ’¡ ì„ íƒëœ ì €ë„: {display_name} (ID={journal_id})")

    # 3ï¸âƒ£ ë…¼ë¬¸ ìˆ˜ì§‘ (ì—°ë„ë³„ ë¶„í•  ì¡°íšŒ)
    works = []
    for journal_id in journal_ids:
        for year in range(year_start, year_end+1):
            from_date = f"{year}-01-01"
            to_date   = f"{year}-12-31"
            yearly = []

            works_q = Works().filter(
            journal=journal_id,
            from_publication_date=from_date,
            to_publication_date=to_date
        )
            if include_only_with_abstract:
                works_q=works_q.filter(has_abstract='true')

            for page in works_q.select(FIELDS).paginate(per_page=200):
                yearly.extend(page)

            print(f"âœ… {year}ë…„ [{journal_id}] ìˆ˜ì§‘: {len(yearly)}ê±´")
            works.extend(yearly)

    print(f"âœ… ì „ì²´ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜ (ì¤‘ë³µ ì „): {len(works)}ê±´")



    # 4ï¸âƒ£ ì¤‘ë³µ ì œê±°
    unique_works = list({w['id']: w for w in works}.values())
    print(f"âœ… ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜ (ì¤‘ë³µ ì œê±° í›„): {len(unique_works)}ê±´")

    # # ì—­ì¸ë±ìŠ¤ë¥¼ í‰ë¬¸ ì´ˆë¡ìœ¼ë¡œ ë³€í™˜ -> 'abstract' í•„ë“œì— ì €ì¥

    # for w in unique_works:
    #     # abstractê°€ ì´ë¯¸ í‰ë¬¸ìœ¼ë¡œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ 
    #     if w.get('abstract'):
    #         continue

    #     aii = w.get('abstract_inverted_index', {})
    #     if isinstance(aii, dict) and aii:
    #         abs_text = reconstruct_abstract(aii)
    #         # ì¤‘ë³µ ê³µë°± ì œê±° ë° ì–‘ìª½ ê³µë°± ì œê±°
    #         abs_text = re.sub(r'\s+', ' ', abs_text).strip()
    #         w['abstract'] = abs_text
    #     else:
    #         w['abstract'] = ""

    #     w.pop('abstract_inverted_index', None)  # ì›ë³¸ ì—­ì¸ë±ìŠ¤ ì‚­ì œ


    # 5ï¸âƒ£ ì´ˆê¸° DOI ìƒíƒœ ë¶„ì„
    empty = [w['id'] for w in unique_works if not w.get('doi')]
    print(f"  â€¢ DOI ë¯¸ë³´ìœ : {len(empty)}ê±´")

    # 6ï¸âƒ£ DOI íŒ¨í„´ ì¶”ì •
    patterns = guess_doi_pattern_from_samples(unique_works)

    # 7ï¸âƒ£ ë°°ì¹˜ ë‹¨ìœ„ ë³´ê°•
    total = len(unique_works)
    for i in range(0, total, BATCH_SIZE):
        batch = unique_works[i:i+BATCH_SIZE]
        print(f"ğŸ”„ ë°°ì¹˜ {i//BATCH_SIZE+1}/{(total+BATCH_SIZE-1)//BATCH_SIZE} ì²˜ë¦¬ ì¤‘â€¦")
        asyncio.run(enrich_works_batch(batch, executor, patterns))
        print(f"ğŸ“Š ì§„í–‰ë¥ : {min(i+BATCH_SIZE,total)}/{total} ({min(i+BATCH_SIZE,total)/total*100:.1f}%)")

    # 8ï¸âƒ£ executor ì •ë¦¬
    executor.shutdown(wait=True)


    # ìµœì¢… DOI ë³´ê°• í†µê³„
    final = sum(1 for w in unique_works if w.get('doi'))
    
    missing_doi_count = len(empty)
    enriched_count = sum(1 for w in unique_works if w['id'] in empty and w.get('doi'))
    enrich_rate = round(enriched_count / missing_doi_count * 100, 1 ) if missing_doi_count else 0.0
    print(f"ğŸ“ˆ ìµœì¢… DOI ë³´ê°•: {final}/{len(unique_works)} ({final/len(unique_works)*100:.1f}%)")
    print(f"âœ… ì‹¤ì œë¡œ ë³´ê°• ì™„ë£Œí•œ ê±´ìˆ˜: {enriched_count}ê±´")
    print(f"âœ… ê²°ì¸¡â†’ë³´ê°• ì„±ê³µë¥ : {enrich_rate}%")

    # âœ… metrics.json ì—…ë°ì´íŠ¸
    _update_metrics(
        prefix, year_start, year_end,
        total_collected=len(unique_works),
        doi_missing=missing_doi_count,
        doi_enriched=enriched_count,
        doi_enrich_rate=enrich_rate,
        __anchor=f"{prefix}_{year_start}_{year_end}.json",  # ë˜ëŠ” out_file ë³€ìˆ˜
    )

    # 9ï¸âƒ£ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìµœì¢… í•©ë³¸ë§Œ ì €ì¥

    # for w in unique_works:
    #     w.pop('abstract_inverted_index', None)  # ì—­ì¸ë±ìŠ¤ ì œê±°

    max_per_file = 9500
    out_file = f'{prefix}_{year_start}_{year_end}.json'
    with open(out_file, 'w', encoding='utf-8') as fp:
        fp.write('[\n')
        total = len(unique_works)
        for start in range(0, total, max_per_file):
            for i, item in enumerate(unique_works[start:start+max_per_file]):
                json.dump(item, fp, ensure_ascii=False)
                if start + i + 1 < total:
                    fp.write(',\n')
        fp.write('\n]')
    print(f"âœ… ìµœì¢… í•©ë³¸ ì €ì¥ ì™„ë£Œ: {out_file} ({total}ê±´)")



if __name__ == '__main__':

    start = time.time()
    # ì‹¤í–‰ ì‹œ ì¸ì ì „ë‹¬
    issns = ['0043-1354', '0011-9164', '0733-9429']
    year_start = 2015
    year_end = 2024
    email = 's0124kw@gmail.com'
    main(issns, year_start, year_end, email, '', include_only_with_abstract=True)
    print(f"â±ï¸ ì´ ì†Œìš”: {time.time() - start:.1f}s")
